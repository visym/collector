<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pycollector.model.face.faster_rcnn API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pycollector.model.face.faster_rcnn</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/pycollector/model/face/faster_rcnn.py#L0-L1297" class="git-link">Browse git</a>
</summary>
<pre><code class="python">import numpy as np
import os
import numpy as np
import imp
import torch 
import torchvision.ops
import torch.nn as nn
import torch.nn.functional
import PIL.Image

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math


import time
import os
import sys
from math import ceil
import torch
import numpy as np

import sys
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), &#39;..&#39;, &#39;models&#39;, &#39;detection&#39;))


class RpnLayers(nn.Module):
    def __init__(self, weight_file=None):
        super(RpnLayers, self).__init__()
        #global __rpn_layers_weights_dict  # Loaded by MMDNN
        #__rpn_layers_weights_dict = load_weights(weight_file)

        self.rpn_conv_3x3 = self.__conv(2, name=&#39;rpn_conv/3x3&#39;, in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)
        self.rpn_cls_score = self.__conv(2, name=&#39;rpn_cls_score&#39;, in_channels=512, out_channels=18, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)
        self.rpn_bbox_pred = self.__conv(2, name=&#39;rpn_bbox_pred&#39;, in_channels=512, out_channels=36, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)

    def forward(self, x):
        rpn_conv_3x3_pad = F.pad(x, (1, 1, 1, 1))
        rpn_conv_3x3    = self.rpn_conv_3x3(rpn_conv_3x3_pad)
        rpn_relu_3x3    = F.relu(rpn_conv_3x3)
        rpn_cls_score   = self.rpn_cls_score(rpn_relu_3x3)
        rpn_bbox_pred   = self.rpn_bbox_pred(rpn_relu_3x3)
        return rpn_cls_score, rpn_bbox_pred


    @staticmethod
    def __conv(dim, name, **kwargs):
        if   dim == 1:  layer = nn.Conv1d(**kwargs)
        elif dim == 2:  layer = nn.Conv2d(**kwargs)
        elif dim == 3:  layer = nn.Conv3d(**kwargs)
        else:           raise NotImplementedError()

        #layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__rpn_layers_weights_dict[name][&#39;weights&#39;]))
        #if &#39;bias&#39; in __rpn_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__rpn_layers_weights_dict[name][&#39;bias&#39;]))
        return layer




class BottomLayers(nn.Module):    
    def __init__(self, weight_file=None):
        super(BottomLayers, self).__init__()
        #global __bottom_layers_weights_dict  # loaded by MMDNN
        #__bottom_layers_weights_dict = load_weights(weight_file)

        self.conv1 = self.__conv(2, name=&#39;conv1&#39;, in_channels=3, out_channels=64, kernel_size=(7, 7), stride=(2, 2), groups=1, bias=False)
        self.bn_conv1 = self.__batch_normalization(2, &#39;bn_conv1&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2a_branch2a = self.__conv(2, name=&#39;res2a_branch2a&#39;, in_channels=64, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.res2a_branch1 = self.__conv(2, name=&#39;res2a_branch1&#39;, in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2a_branch2a = self.__batch_normalization(2, &#39;bn2a_branch2a&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.bn2a_branch1 = self.__batch_normalization(2, &#39;bn2a_branch1&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res2a_branch2b = self.__conv(2, name=&#39;res2a_branch2b&#39;, in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn2a_branch2b = self.__batch_normalization(2, &#39;bn2a_branch2b&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2a_branch2c = self.__conv(2, name=&#39;res2a_branch2c&#39;, in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2a_branch2c = self.__batch_normalization(2, &#39;bn2a_branch2c&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res2b_branch2a = self.__conv(2, name=&#39;res2b_branch2a&#39;, in_channels=256, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2b_branch2a = self.__batch_normalization(2, &#39;bn2b_branch2a&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2b_branch2b = self.__conv(2, name=&#39;res2b_branch2b&#39;, in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn2b_branch2b = self.__batch_normalization(2, &#39;bn2b_branch2b&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2b_branch2c = self.__conv(2, name=&#39;res2b_branch2c&#39;, in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2b_branch2c = self.__batch_normalization(2, &#39;bn2b_branch2c&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res2c_branch2a = self.__conv(2, name=&#39;res2c_branch2a&#39;, in_channels=256, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2c_branch2a = self.__batch_normalization(2, &#39;bn2c_branch2a&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2c_branch2b = self.__conv(2, name=&#39;res2c_branch2b&#39;, in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn2c_branch2b = self.__batch_normalization(2, &#39;bn2c_branch2b&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2c_branch2c = self.__conv(2, name=&#39;res2c_branch2c&#39;, in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2c_branch2c = self.__batch_normalization(2, &#39;bn2c_branch2c&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res3a_branch1 = self.__conv(2, name=&#39;res3a_branch1&#39;, in_channels=256, out_channels=512, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.res3a_branch2a = self.__conv(2, name=&#39;res3a_branch2a&#39;, in_channels=256, out_channels=128, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.bn3a_branch1 = self.__batch_normalization(2, &#39;bn3a_branch1&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.bn3a_branch2a = self.__batch_normalization(2, &#39;bn3a_branch2a&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3a_branch2b = self.__conv(2, name=&#39;res3a_branch2b&#39;, in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn3a_branch2b = self.__batch_normalization(2, &#39;bn3a_branch2b&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3a_branch2c = self.__conv(2, name=&#39;res3a_branch2c&#39;, in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3a_branch2c = self.__batch_normalization(2, &#39;bn3a_branch2c&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res3b1_branch2a = self.__conv(2, name=&#39;res3b1_branch2a&#39;, in_channels=512, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b1_branch2a = self.__batch_normalization(2, &#39;bn3b1_branch2a&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b1_branch2b = self.__conv(2, name=&#39;res3b1_branch2b&#39;, in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn3b1_branch2b = self.__batch_normalization(2, &#39;bn3b1_branch2b&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b1_branch2c = self.__conv(2, name=&#39;res3b1_branch2c&#39;, in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b1_branch2c = self.__batch_normalization(2, &#39;bn3b1_branch2c&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res3b2_branch2a = self.__conv(2, name=&#39;res3b2_branch2a&#39;, in_channels=512, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b2_branch2a = self.__batch_normalization(2, &#39;bn3b2_branch2a&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b2_branch2b = self.__conv(2, name=&#39;res3b2_branch2b&#39;, in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn3b2_branch2b = self.__batch_normalization(2, &#39;bn3b2_branch2b&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b2_branch2c = self.__conv(2, name=&#39;res3b2_branch2c&#39;, in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b2_branch2c = self.__batch_normalization(2, &#39;bn3b2_branch2c&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res3b3_branch2a = self.__conv(2, name=&#39;res3b3_branch2a&#39;, in_channels=512, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b3_branch2a = self.__batch_normalization(2, &#39;bn3b3_branch2a&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b3_branch2b = self.__conv(2, name=&#39;res3b3_branch2b&#39;, in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn3b3_branch2b = self.__batch_normalization(2, &#39;bn3b3_branch2b&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b3_branch2c = self.__conv(2, name=&#39;res3b3_branch2c&#39;, in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b3_branch2c = self.__batch_normalization(2, &#39;bn3b3_branch2c&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res4a_branch1 = self.__conv(2, name=&#39;res4a_branch1&#39;, in_channels=512, out_channels=1024, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.res4a_branch2a = self.__conv(2, name=&#39;res4a_branch2a&#39;, in_channels=512, out_channels=256, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.bn4a_branch1 = self.__batch_normalization(2, &#39;bn4a_branch1&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.bn4a_branch2a = self.__batch_normalization(2, &#39;bn4a_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4a_branch2b = self.__conv(2, name=&#39;res4a_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4a_branch2b = self.__batch_normalization(2, &#39;bn4a_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4a_branch2c = self.__conv(2, name=&#39;res4a_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4a_branch2c = self.__batch_normalization(2, &#39;bn4a_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b1_branch2a = self.__conv(2, name=&#39;res4b1_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b1_branch2a = self.__batch_normalization(2, &#39;bn4b1_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b1_branch2b = self.__conv(2, name=&#39;res4b1_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b1_branch2b = self.__batch_normalization(2, &#39;bn4b1_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b1_branch2c = self.__conv(2, name=&#39;res4b1_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b1_branch2c = self.__batch_normalization(2, &#39;bn4b1_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b2_branch2a = self.__conv(2, name=&#39;res4b2_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b2_branch2a = self.__batch_normalization(2, &#39;bn4b2_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b2_branch2b = self.__conv(2, name=&#39;res4b2_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b2_branch2b = self.__batch_normalization(2, &#39;bn4b2_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b2_branch2c = self.__conv(2, name=&#39;res4b2_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b2_branch2c = self.__batch_normalization(2, &#39;bn4b2_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b3_branch2a = self.__conv(2, name=&#39;res4b3_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b3_branch2a = self.__batch_normalization(2, &#39;bn4b3_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b3_branch2b = self.__conv(2, name=&#39;res4b3_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b3_branch2b = self.__batch_normalization(2, &#39;bn4b3_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b3_branch2c = self.__conv(2, name=&#39;res4b3_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b3_branch2c = self.__batch_normalization(2, &#39;bn4b3_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b4_branch2a = self.__conv(2, name=&#39;res4b4_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b4_branch2a = self.__batch_normalization(2, &#39;bn4b4_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b4_branch2b = self.__conv(2, name=&#39;res4b4_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b4_branch2b = self.__batch_normalization(2, &#39;bn4b4_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b4_branch2c = self.__conv(2, name=&#39;res4b4_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b4_branch2c = self.__batch_normalization(2, &#39;bn4b4_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b5_branch2a = self.__conv(2, name=&#39;res4b5_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b5_branch2a = self.__batch_normalization(2, &#39;bn4b5_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b5_branch2b = self.__conv(2, name=&#39;res4b5_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b5_branch2b = self.__batch_normalization(2, &#39;bn4b5_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b5_branch2c = self.__conv(2, name=&#39;res4b5_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b5_branch2c = self.__batch_normalization(2, &#39;bn4b5_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b6_branch2a = self.__conv(2, name=&#39;res4b6_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b6_branch2a = self.__batch_normalization(2, &#39;bn4b6_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b6_branch2b = self.__conv(2, name=&#39;res4b6_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b6_branch2b = self.__batch_normalization(2, &#39;bn4b6_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b6_branch2c = self.__conv(2, name=&#39;res4b6_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b6_branch2c = self.__batch_normalization(2, &#39;bn4b6_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b7_branch2a = self.__conv(2, name=&#39;res4b7_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b7_branch2a = self.__batch_normalization(2, &#39;bn4b7_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b7_branch2b = self.__conv(2, name=&#39;res4b7_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b7_branch2b = self.__batch_normalization(2, &#39;bn4b7_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b7_branch2c = self.__conv(2, name=&#39;res4b7_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b7_branch2c = self.__batch_normalization(2, &#39;bn4b7_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b8_branch2a = self.__conv(2, name=&#39;res4b8_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b8_branch2a = self.__batch_normalization(2, &#39;bn4b8_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b8_branch2b = self.__conv(2, name=&#39;res4b8_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b8_branch2b = self.__batch_normalization(2, &#39;bn4b8_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b8_branch2c = self.__conv(2, name=&#39;res4b8_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b8_branch2c = self.__batch_normalization(2, &#39;bn4b8_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b9_branch2a = self.__conv(2, name=&#39;res4b9_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b9_branch2a = self.__batch_normalization(2, &#39;bn4b9_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b9_branch2b = self.__conv(2, name=&#39;res4b9_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b9_branch2b = self.__batch_normalization(2, &#39;bn4b9_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b9_branch2c = self.__conv(2, name=&#39;res4b9_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b9_branch2c = self.__batch_normalization(2, &#39;bn4b9_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b10_branch2a = self.__conv(2, name=&#39;res4b10_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b10_branch2a = self.__batch_normalization(2, &#39;bn4b10_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b10_branch2b = self.__conv(2, name=&#39;res4b10_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b10_branch2b = self.__batch_normalization(2, &#39;bn4b10_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b10_branch2c = self.__conv(2, name=&#39;res4b10_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b10_branch2c = self.__batch_normalization(2, &#39;bn4b10_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b11_branch2a = self.__conv(2, name=&#39;res4b11_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b11_branch2a = self.__batch_normalization(2, &#39;bn4b11_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b11_branch2b = self.__conv(2, name=&#39;res4b11_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b11_branch2b = self.__batch_normalization(2, &#39;bn4b11_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b11_branch2c = self.__conv(2, name=&#39;res4b11_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b11_branch2c = self.__batch_normalization(2, &#39;bn4b11_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b12_branch2a = self.__conv(2, name=&#39;res4b12_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b12_branch2a = self.__batch_normalization(2, &#39;bn4b12_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b12_branch2b = self.__conv(2, name=&#39;res4b12_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b12_branch2b = self.__batch_normalization(2, &#39;bn4b12_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b12_branch2c = self.__conv(2, name=&#39;res4b12_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b12_branch2c = self.__batch_normalization(2, &#39;bn4b12_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b13_branch2a = self.__conv(2, name=&#39;res4b13_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b13_branch2a = self.__batch_normalization(2, &#39;bn4b13_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b13_branch2b = self.__conv(2, name=&#39;res4b13_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b13_branch2b = self.__batch_normalization(2, &#39;bn4b13_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b13_branch2c = self.__conv(2, name=&#39;res4b13_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b13_branch2c = self.__batch_normalization(2, &#39;bn4b13_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b14_branch2a = self.__conv(2, name=&#39;res4b14_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b14_branch2a = self.__batch_normalization(2, &#39;bn4b14_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b14_branch2b = self.__conv(2, name=&#39;res4b14_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b14_branch2b = self.__batch_normalization(2, &#39;bn4b14_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b14_branch2c = self.__conv(2, name=&#39;res4b14_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b14_branch2c = self.__batch_normalization(2, &#39;bn4b14_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b15_branch2a = self.__conv(2, name=&#39;res4b15_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b15_branch2a = self.__batch_normalization(2, &#39;bn4b15_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b15_branch2b = self.__conv(2, name=&#39;res4b15_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b15_branch2b = self.__batch_normalization(2, &#39;bn4b15_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b15_branch2c = self.__conv(2, name=&#39;res4b15_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b15_branch2c = self.__batch_normalization(2, &#39;bn4b15_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b16_branch2a = self.__conv(2, name=&#39;res4b16_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b16_branch2a = self.__batch_normalization(2, &#39;bn4b16_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b16_branch2b = self.__conv(2, name=&#39;res4b16_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b16_branch2b = self.__batch_normalization(2, &#39;bn4b16_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b16_branch2c = self.__conv(2, name=&#39;res4b16_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b16_branch2c = self.__batch_normalization(2, &#39;bn4b16_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b17_branch2a = self.__conv(2, name=&#39;res4b17_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b17_branch2a = self.__batch_normalization(2, &#39;bn4b17_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b17_branch2b = self.__conv(2, name=&#39;res4b17_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b17_branch2b = self.__batch_normalization(2, &#39;bn4b17_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b17_branch2c = self.__conv(2, name=&#39;res4b17_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b17_branch2c = self.__batch_normalization(2, &#39;bn4b17_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b18_branch2a = self.__conv(2, name=&#39;res4b18_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b18_branch2a = self.__batch_normalization(2, &#39;bn4b18_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b18_branch2b = self.__conv(2, name=&#39;res4b18_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b18_branch2b = self.__batch_normalization(2, &#39;bn4b18_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b18_branch2c = self.__conv(2, name=&#39;res4b18_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b18_branch2c = self.__batch_normalization(2, &#39;bn4b18_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b19_branch2a = self.__conv(2, name=&#39;res4b19_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b19_branch2a = self.__batch_normalization(2, &#39;bn4b19_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b19_branch2b = self.__conv(2, name=&#39;res4b19_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b19_branch2b = self.__batch_normalization(2, &#39;bn4b19_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b19_branch2c = self.__conv(2, name=&#39;res4b19_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b19_branch2c = self.__batch_normalization(2, &#39;bn4b19_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b20_branch2a = self.__conv(2, name=&#39;res4b20_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b20_branch2a = self.__batch_normalization(2, &#39;bn4b20_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b20_branch2b = self.__conv(2, name=&#39;res4b20_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b20_branch2b = self.__batch_normalization(2, &#39;bn4b20_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b20_branch2c = self.__conv(2, name=&#39;res4b20_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b20_branch2c = self.__batch_normalization(2, &#39;bn4b20_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b21_branch2a = self.__conv(2, name=&#39;res4b21_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b21_branch2a = self.__batch_normalization(2, &#39;bn4b21_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b21_branch2b = self.__conv(2, name=&#39;res4b21_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b21_branch2b = self.__batch_normalization(2, &#39;bn4b21_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b21_branch2c = self.__conv(2, name=&#39;res4b21_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b21_branch2c = self.__batch_normalization(2, &#39;bn4b21_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b22_branch2a = self.__conv(2, name=&#39;res4b22_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b22_branch2a = self.__batch_normalization(2, &#39;bn4b22_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b22_branch2b = self.__conv(2, name=&#39;res4b22_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b22_branch2b = self.__batch_normalization(2, &#39;bn4b22_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b22_branch2c = self.__conv(2, name=&#39;res4b22_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b22_branch2c = self.__batch_normalization(2, &#39;bn4b22_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)

    def forward(self, x):
        conv1_pad       = F.pad(x, (3, 3, 3, 3))
        conv1           = self.conv1(conv1_pad)
        bn_conv1        = self.bn_conv1(conv1)
        conv1_relu      = F.relu(bn_conv1)
        pool1_pad       = F.pad(conv1_relu, (0, 1, 0, 1), value=float(&#39;-inf&#39;))
        pool1           = F.max_pool2d(pool1_pad, kernel_size=(3, 3), stride=(2, 2), padding=0, ceil_mode=False)
        res2a_branch2a  = self.res2a_branch2a(pool1)
        res2a_branch1   = self.res2a_branch1(pool1)
        bn2a_branch2a   = self.bn2a_branch2a(res2a_branch2a)
        bn2a_branch1    = self.bn2a_branch1(res2a_branch1)
        res2a_branch2a_relu = F.relu(bn2a_branch2a)
        res2a_branch2b_pad = F.pad(res2a_branch2a_relu, (1, 1, 1, 1))
        res2a_branch2b  = self.res2a_branch2b(res2a_branch2b_pad)
        bn2a_branch2b   = self.bn2a_branch2b(res2a_branch2b)
        res2a_branch2b_relu = F.relu(bn2a_branch2b)
        res2a_branch2c  = self.res2a_branch2c(res2a_branch2b_relu)
        bn2a_branch2c   = self.bn2a_branch2c(res2a_branch2c)
        res2a           = bn2a_branch1 + bn2a_branch2c
        res2a_relu      = F.relu(res2a)
        res2b_branch2a  = self.res2b_branch2a(res2a_relu)
        bn2b_branch2a   = self.bn2b_branch2a(res2b_branch2a)
        res2b_branch2a_relu = F.relu(bn2b_branch2a)
        res2b_branch2b_pad = F.pad(res2b_branch2a_relu, (1, 1, 1, 1))
        res2b_branch2b  = self.res2b_branch2b(res2b_branch2b_pad)
        bn2b_branch2b   = self.bn2b_branch2b(res2b_branch2b)
        res2b_branch2b_relu = F.relu(bn2b_branch2b)
        res2b_branch2c  = self.res2b_branch2c(res2b_branch2b_relu)
        bn2b_branch2c   = self.bn2b_branch2c(res2b_branch2c)
        res2b           = res2a_relu + bn2b_branch2c
        res2b_relu      = F.relu(res2b)
        res2c_branch2a  = self.res2c_branch2a(res2b_relu)
        bn2c_branch2a   = self.bn2c_branch2a(res2c_branch2a)
        res2c_branch2a_relu = F.relu(bn2c_branch2a)
        res2c_branch2b_pad = F.pad(res2c_branch2a_relu, (1, 1, 1, 1))
        res2c_branch2b  = self.res2c_branch2b(res2c_branch2b_pad)
        bn2c_branch2b   = self.bn2c_branch2b(res2c_branch2b)
        res2c_branch2b_relu = F.relu(bn2c_branch2b)
        res2c_branch2c  = self.res2c_branch2c(res2c_branch2b_relu)
        bn2c_branch2c   = self.bn2c_branch2c(res2c_branch2c)
        res2c           = res2b_relu + bn2c_branch2c
        res2c_relu      = F.relu(res2c)
        res3a_branch1   = self.res3a_branch1(res2c_relu)
        res3a_branch2a  = self.res3a_branch2a(res2c_relu)
        bn3a_branch1    = self.bn3a_branch1(res3a_branch1)
        bn3a_branch2a   = self.bn3a_branch2a(res3a_branch2a)
        res3a_branch2a_relu = F.relu(bn3a_branch2a)
        res3a_branch2b_pad = F.pad(res3a_branch2a_relu, (1, 1, 1, 1))
        res3a_branch2b  = self.res3a_branch2b(res3a_branch2b_pad)
        bn3a_branch2b   = self.bn3a_branch2b(res3a_branch2b)
        res3a_branch2b_relu = F.relu(bn3a_branch2b)
        res3a_branch2c  = self.res3a_branch2c(res3a_branch2b_relu)
        bn3a_branch2c   = self.bn3a_branch2c(res3a_branch2c)
        res3a           = bn3a_branch1 + bn3a_branch2c
        res3a_relu      = F.relu(res3a)
        res3b1_branch2a = self.res3b1_branch2a(res3a_relu)
        bn3b1_branch2a  = self.bn3b1_branch2a(res3b1_branch2a)
        res3b1_branch2a_relu = F.relu(bn3b1_branch2a)
        res3b1_branch2b_pad = F.pad(res3b1_branch2a_relu, (1, 1, 1, 1))
        res3b1_branch2b = self.res3b1_branch2b(res3b1_branch2b_pad)
        bn3b1_branch2b  = self.bn3b1_branch2b(res3b1_branch2b)
        res3b1_branch2b_relu = F.relu(bn3b1_branch2b)
        res3b1_branch2c = self.res3b1_branch2c(res3b1_branch2b_relu)
        bn3b1_branch2c  = self.bn3b1_branch2c(res3b1_branch2c)
        res3b1          = res3a_relu + bn3b1_branch2c
        res3b1_relu     = F.relu(res3b1)
        res3b2_branch2a = self.res3b2_branch2a(res3b1_relu)
        bn3b2_branch2a  = self.bn3b2_branch2a(res3b2_branch2a)
        res3b2_branch2a_relu = F.relu(bn3b2_branch2a)
        res3b2_branch2b_pad = F.pad(res3b2_branch2a_relu, (1, 1, 1, 1))
        res3b2_branch2b = self.res3b2_branch2b(res3b2_branch2b_pad)
        bn3b2_branch2b  = self.bn3b2_branch2b(res3b2_branch2b)
        res3b2_branch2b_relu = F.relu(bn3b2_branch2b)
        res3b2_branch2c = self.res3b2_branch2c(res3b2_branch2b_relu)
        bn3b2_branch2c  = self.bn3b2_branch2c(res3b2_branch2c)
        res3b2          = res3b1_relu + bn3b2_branch2c
        res3b2_relu     = F.relu(res3b2)
        res3b3_branch2a = self.res3b3_branch2a(res3b2_relu)
        bn3b3_branch2a  = self.bn3b3_branch2a(res3b3_branch2a)
        res3b3_branch2a_relu = F.relu(bn3b3_branch2a)
        res3b3_branch2b_pad = F.pad(res3b3_branch2a_relu, (1, 1, 1, 1))
        res3b3_branch2b = self.res3b3_branch2b(res3b3_branch2b_pad)
        bn3b3_branch2b  = self.bn3b3_branch2b(res3b3_branch2b)
        res3b3_branch2b_relu = F.relu(bn3b3_branch2b)
        res3b3_branch2c = self.res3b3_branch2c(res3b3_branch2b_relu)
        bn3b3_branch2c  = self.bn3b3_branch2c(res3b3_branch2c)
        res3b3          = res3b2_relu + bn3b3_branch2c
        res3b3_relu     = F.relu(res3b3)
        res4a_branch1   = self.res4a_branch1(res3b3_relu)
        res4a_branch2a  = self.res4a_branch2a(res3b3_relu)
        bn4a_branch1    = self.bn4a_branch1(res4a_branch1)
        bn4a_branch2a   = self.bn4a_branch2a(res4a_branch2a)
        res4a_branch2a_relu = F.relu(bn4a_branch2a)
        res4a_branch2b_pad = F.pad(res4a_branch2a_relu, (1, 1, 1, 1))
        res4a_branch2b  = self.res4a_branch2b(res4a_branch2b_pad)
        bn4a_branch2b   = self.bn4a_branch2b(res4a_branch2b)
        res4a_branch2b_relu = F.relu(bn4a_branch2b)
        res4a_branch2c  = self.res4a_branch2c(res4a_branch2b_relu)
        bn4a_branch2c   = self.bn4a_branch2c(res4a_branch2c)
        res4a           = bn4a_branch1 + bn4a_branch2c
        res4a_relu      = F.relu(res4a)
        res4b1_branch2a = self.res4b1_branch2a(res4a_relu)
        bn4b1_branch2a  = self.bn4b1_branch2a(res4b1_branch2a)
        res4b1_branch2a_relu = F.relu(bn4b1_branch2a)
        res4b1_branch2b_pad = F.pad(res4b1_branch2a_relu, (1, 1, 1, 1))
        res4b1_branch2b = self.res4b1_branch2b(res4b1_branch2b_pad)
        bn4b1_branch2b  = self.bn4b1_branch2b(res4b1_branch2b)
        res4b1_branch2b_relu = F.relu(bn4b1_branch2b)
        res4b1_branch2c = self.res4b1_branch2c(res4b1_branch2b_relu)
        bn4b1_branch2c  = self.bn4b1_branch2c(res4b1_branch2c)
        res4b1          = res4a_relu + bn4b1_branch2c
        res4b1_relu     = F.relu(res4b1)
        res4b2_branch2a = self.res4b2_branch2a(res4b1_relu)
        bn4b2_branch2a  = self.bn4b2_branch2a(res4b2_branch2a)
        res4b2_branch2a_relu = F.relu(bn4b2_branch2a)
        res4b2_branch2b_pad = F.pad(res4b2_branch2a_relu, (1, 1, 1, 1))
        res4b2_branch2b = self.res4b2_branch2b(res4b2_branch2b_pad)
        bn4b2_branch2b  = self.bn4b2_branch2b(res4b2_branch2b)
        res4b2_branch2b_relu = F.relu(bn4b2_branch2b)
        res4b2_branch2c = self.res4b2_branch2c(res4b2_branch2b_relu)
        bn4b2_branch2c  = self.bn4b2_branch2c(res4b2_branch2c)
        res4b2          = res4b1_relu + bn4b2_branch2c
        res4b2_relu     = F.relu(res4b2)
        res4b3_branch2a = self.res4b3_branch2a(res4b2_relu)
        bn4b3_branch2a  = self.bn4b3_branch2a(res4b3_branch2a)
        res4b3_branch2a_relu = F.relu(bn4b3_branch2a)
        res4b3_branch2b_pad = F.pad(res4b3_branch2a_relu, (1, 1, 1, 1))
        res4b3_branch2b = self.res4b3_branch2b(res4b3_branch2b_pad)
        bn4b3_branch2b  = self.bn4b3_branch2b(res4b3_branch2b)
        res4b3_branch2b_relu = F.relu(bn4b3_branch2b)
        res4b3_branch2c = self.res4b3_branch2c(res4b3_branch2b_relu)
        bn4b3_branch2c  = self.bn4b3_branch2c(res4b3_branch2c)
        res4b3          = res4b2_relu + bn4b3_branch2c
        res4b3_relu     = F.relu(res4b3)
        res4b4_branch2a = self.res4b4_branch2a(res4b3_relu)
        bn4b4_branch2a  = self.bn4b4_branch2a(res4b4_branch2a)
        res4b4_branch2a_relu = F.relu(bn4b4_branch2a)
        res4b4_branch2b_pad = F.pad(res4b4_branch2a_relu, (1, 1, 1, 1))
        res4b4_branch2b = self.res4b4_branch2b(res4b4_branch2b_pad)
        bn4b4_branch2b  = self.bn4b4_branch2b(res4b4_branch2b)
        res4b4_branch2b_relu = F.relu(bn4b4_branch2b)
        res4b4_branch2c = self.res4b4_branch2c(res4b4_branch2b_relu)
        bn4b4_branch2c  = self.bn4b4_branch2c(res4b4_branch2c)
        res4b4          = res4b3_relu + bn4b4_branch2c
        res4b4_relu     = F.relu(res4b4)
        res4b5_branch2a = self.res4b5_branch2a(res4b4_relu)
        bn4b5_branch2a  = self.bn4b5_branch2a(res4b5_branch2a)
        res4b5_branch2a_relu = F.relu(bn4b5_branch2a)
        res4b5_branch2b_pad = F.pad(res4b5_branch2a_relu, (1, 1, 1, 1))
        res4b5_branch2b = self.res4b5_branch2b(res4b5_branch2b_pad)
        bn4b5_branch2b  = self.bn4b5_branch2b(res4b5_branch2b)
        res4b5_branch2b_relu = F.relu(bn4b5_branch2b)
        res4b5_branch2c = self.res4b5_branch2c(res4b5_branch2b_relu)
        bn4b5_branch2c  = self.bn4b5_branch2c(res4b5_branch2c)
        res4b5          = res4b4_relu + bn4b5_branch2c
        res4b5_relu     = F.relu(res4b5)
        res4b6_branch2a = self.res4b6_branch2a(res4b5_relu)
        bn4b6_branch2a  = self.bn4b6_branch2a(res4b6_branch2a)
        res4b6_branch2a_relu = F.relu(bn4b6_branch2a)
        res4b6_branch2b_pad = F.pad(res4b6_branch2a_relu, (1, 1, 1, 1))
        res4b6_branch2b = self.res4b6_branch2b(res4b6_branch2b_pad)
        bn4b6_branch2b  = self.bn4b6_branch2b(res4b6_branch2b)
        res4b6_branch2b_relu = F.relu(bn4b6_branch2b)
        res4b6_branch2c = self.res4b6_branch2c(res4b6_branch2b_relu)
        bn4b6_branch2c  = self.bn4b6_branch2c(res4b6_branch2c)
        res4b6          = res4b5_relu + bn4b6_branch2c
        res4b6_relu     = F.relu(res4b6)
        res4b7_branch2a = self.res4b7_branch2a(res4b6_relu)
        bn4b7_branch2a  = self.bn4b7_branch2a(res4b7_branch2a)
        res4b7_branch2a_relu = F.relu(bn4b7_branch2a)
        res4b7_branch2b_pad = F.pad(res4b7_branch2a_relu, (1, 1, 1, 1))
        res4b7_branch2b = self.res4b7_branch2b(res4b7_branch2b_pad)
        bn4b7_branch2b  = self.bn4b7_branch2b(res4b7_branch2b)
        res4b7_branch2b_relu = F.relu(bn4b7_branch2b)
        res4b7_branch2c = self.res4b7_branch2c(res4b7_branch2b_relu)
        bn4b7_branch2c  = self.bn4b7_branch2c(res4b7_branch2c)
        res4b7          = res4b6_relu + bn4b7_branch2c
        res4b7_relu     = F.relu(res4b7)
        res4b8_branch2a = self.res4b8_branch2a(res4b7_relu)
        bn4b8_branch2a  = self.bn4b8_branch2a(res4b8_branch2a)
        res4b8_branch2a_relu = F.relu(bn4b8_branch2a)
        res4b8_branch2b_pad = F.pad(res4b8_branch2a_relu, (1, 1, 1, 1))
        res4b8_branch2b = self.res4b8_branch2b(res4b8_branch2b_pad)
        bn4b8_branch2b  = self.bn4b8_branch2b(res4b8_branch2b)
        res4b8_branch2b_relu = F.relu(bn4b8_branch2b)
        res4b8_branch2c = self.res4b8_branch2c(res4b8_branch2b_relu)
        bn4b8_branch2c  = self.bn4b8_branch2c(res4b8_branch2c)
        res4b8          = res4b7_relu + bn4b8_branch2c
        res4b8_relu     = F.relu(res4b8)
        res4b9_branch2a = self.res4b9_branch2a(res4b8_relu)
        bn4b9_branch2a  = self.bn4b9_branch2a(res4b9_branch2a)
        res4b9_branch2a_relu = F.relu(bn4b9_branch2a)
        res4b9_branch2b_pad = F.pad(res4b9_branch2a_relu, (1, 1, 1, 1))
        res4b9_branch2b = self.res4b9_branch2b(res4b9_branch2b_pad)
        bn4b9_branch2b  = self.bn4b9_branch2b(res4b9_branch2b)
        res4b9_branch2b_relu = F.relu(bn4b9_branch2b)
        res4b9_branch2c = self.res4b9_branch2c(res4b9_branch2b_relu)
        bn4b9_branch2c  = self.bn4b9_branch2c(res4b9_branch2c)
        res4b9          = res4b8_relu + bn4b9_branch2c
        res4b9_relu     = F.relu(res4b9)
        res4b10_branch2a = self.res4b10_branch2a(res4b9_relu)
        bn4b10_branch2a = self.bn4b10_branch2a(res4b10_branch2a)
        res4b10_branch2a_relu = F.relu(bn4b10_branch2a)
        res4b10_branch2b_pad = F.pad(res4b10_branch2a_relu, (1, 1, 1, 1))
        res4b10_branch2b = self.res4b10_branch2b(res4b10_branch2b_pad)
        bn4b10_branch2b = self.bn4b10_branch2b(res4b10_branch2b)
        res4b10_branch2b_relu = F.relu(bn4b10_branch2b)
        res4b10_branch2c = self.res4b10_branch2c(res4b10_branch2b_relu)
        bn4b10_branch2c = self.bn4b10_branch2c(res4b10_branch2c)
        res4b10         = res4b9_relu + bn4b10_branch2c
        res4b10_relu    = F.relu(res4b10)
        res4b11_branch2a = self.res4b11_branch2a(res4b10_relu)
        bn4b11_branch2a = self.bn4b11_branch2a(res4b11_branch2a)
        res4b11_branch2a_relu = F.relu(bn4b11_branch2a)
        res4b11_branch2b_pad = F.pad(res4b11_branch2a_relu, (1, 1, 1, 1))
        res4b11_branch2b = self.res4b11_branch2b(res4b11_branch2b_pad)
        bn4b11_branch2b = self.bn4b11_branch2b(res4b11_branch2b)
        res4b11_branch2b_relu = F.relu(bn4b11_branch2b)
        res4b11_branch2c = self.res4b11_branch2c(res4b11_branch2b_relu)
        bn4b11_branch2c = self.bn4b11_branch2c(res4b11_branch2c)
        res4b11         = res4b10_relu + bn4b11_branch2c
        res4b11_relu    = F.relu(res4b11)
        res4b12_branch2a = self.res4b12_branch2a(res4b11_relu)
        bn4b12_branch2a = self.bn4b12_branch2a(res4b12_branch2a)
        res4b12_branch2a_relu = F.relu(bn4b12_branch2a)
        res4b12_branch2b_pad = F.pad(res4b12_branch2a_relu, (1, 1, 1, 1))
        res4b12_branch2b = self.res4b12_branch2b(res4b12_branch2b_pad)
        bn4b12_branch2b = self.bn4b12_branch2b(res4b12_branch2b)
        res4b12_branch2b_relu = F.relu(bn4b12_branch2b)
        res4b12_branch2c = self.res4b12_branch2c(res4b12_branch2b_relu)
        bn4b12_branch2c = self.bn4b12_branch2c(res4b12_branch2c)
        res4b12         = res4b11_relu + bn4b12_branch2c
        res4b12_relu    = F.relu(res4b12)
        res4b13_branch2a = self.res4b13_branch2a(res4b12_relu)
        bn4b13_branch2a = self.bn4b13_branch2a(res4b13_branch2a)
        res4b13_branch2a_relu = F.relu(bn4b13_branch2a)
        res4b13_branch2b_pad = F.pad(res4b13_branch2a_relu, (1, 1, 1, 1))
        res4b13_branch2b = self.res4b13_branch2b(res4b13_branch2b_pad)
        bn4b13_branch2b = self.bn4b13_branch2b(res4b13_branch2b)
        res4b13_branch2b_relu = F.relu(bn4b13_branch2b)
        res4b13_branch2c = self.res4b13_branch2c(res4b13_branch2b_relu)
        bn4b13_branch2c = self.bn4b13_branch2c(res4b13_branch2c)
        res4b13         = res4b12_relu + bn4b13_branch2c
        res4b13_relu    = F.relu(res4b13)
        res4b14_branch2a = self.res4b14_branch2a(res4b13_relu)
        bn4b14_branch2a = self.bn4b14_branch2a(res4b14_branch2a)
        res4b14_branch2a_relu = F.relu(bn4b14_branch2a)
        res4b14_branch2b_pad = F.pad(res4b14_branch2a_relu, (1, 1, 1, 1))
        res4b14_branch2b = self.res4b14_branch2b(res4b14_branch2b_pad)
        bn4b14_branch2b = self.bn4b14_branch2b(res4b14_branch2b)
        res4b14_branch2b_relu = F.relu(bn4b14_branch2b)
        res4b14_branch2c = self.res4b14_branch2c(res4b14_branch2b_relu)
        bn4b14_branch2c = self.bn4b14_branch2c(res4b14_branch2c)
        res4b14         = res4b13_relu + bn4b14_branch2c
        res4b14_relu    = F.relu(res4b14)
        res4b15_branch2a = self.res4b15_branch2a(res4b14_relu)
        bn4b15_branch2a = self.bn4b15_branch2a(res4b15_branch2a)
        res4b15_branch2a_relu = F.relu(bn4b15_branch2a)
        res4b15_branch2b_pad = F.pad(res4b15_branch2a_relu, (1, 1, 1, 1))
        res4b15_branch2b = self.res4b15_branch2b(res4b15_branch2b_pad)
        bn4b15_branch2b = self.bn4b15_branch2b(res4b15_branch2b)
        res4b15_branch2b_relu = F.relu(bn4b15_branch2b)
        res4b15_branch2c = self.res4b15_branch2c(res4b15_branch2b_relu)
        bn4b15_branch2c = self.bn4b15_branch2c(res4b15_branch2c)
        res4b15         = res4b14_relu + bn4b15_branch2c
        res4b15_relu    = F.relu(res4b15)
        res4b16_branch2a = self.res4b16_branch2a(res4b15_relu)
        bn4b16_branch2a = self.bn4b16_branch2a(res4b16_branch2a)
        res4b16_branch2a_relu = F.relu(bn4b16_branch2a)
        res4b16_branch2b_pad = F.pad(res4b16_branch2a_relu, (1, 1, 1, 1))
        res4b16_branch2b = self.res4b16_branch2b(res4b16_branch2b_pad)
        bn4b16_branch2b = self.bn4b16_branch2b(res4b16_branch2b)
        res4b16_branch2b_relu = F.relu(bn4b16_branch2b)
        res4b16_branch2c = self.res4b16_branch2c(res4b16_branch2b_relu)
        bn4b16_branch2c = self.bn4b16_branch2c(res4b16_branch2c)
        res4b16         = res4b15_relu + bn4b16_branch2c
        res4b16_relu    = F.relu(res4b16)
        res4b17_branch2a = self.res4b17_branch2a(res4b16_relu)
        bn4b17_branch2a = self.bn4b17_branch2a(res4b17_branch2a)
        res4b17_branch2a_relu = F.relu(bn4b17_branch2a)
        res4b17_branch2b_pad = F.pad(res4b17_branch2a_relu, (1, 1, 1, 1))
        res4b17_branch2b = self.res4b17_branch2b(res4b17_branch2b_pad)
        bn4b17_branch2b = self.bn4b17_branch2b(res4b17_branch2b)
        res4b17_branch2b_relu = F.relu(bn4b17_branch2b)
        res4b17_branch2c = self.res4b17_branch2c(res4b17_branch2b_relu)
        bn4b17_branch2c = self.bn4b17_branch2c(res4b17_branch2c)
        res4b17         = res4b16_relu + bn4b17_branch2c
        res4b17_relu    = F.relu(res4b17)
        res4b18_branch2a = self.res4b18_branch2a(res4b17_relu)
        bn4b18_branch2a = self.bn4b18_branch2a(res4b18_branch2a)
        res4b18_branch2a_relu = F.relu(bn4b18_branch2a)
        res4b18_branch2b_pad = F.pad(res4b18_branch2a_relu, (1, 1, 1, 1))
        res4b18_branch2b = self.res4b18_branch2b(res4b18_branch2b_pad)
        bn4b18_branch2b = self.bn4b18_branch2b(res4b18_branch2b)
        res4b18_branch2b_relu = F.relu(bn4b18_branch2b)
        res4b18_branch2c = self.res4b18_branch2c(res4b18_branch2b_relu)
        bn4b18_branch2c = self.bn4b18_branch2c(res4b18_branch2c)
        res4b18         = res4b17_relu + bn4b18_branch2c
        res4b18_relu    = F.relu(res4b18)
        res4b19_branch2a = self.res4b19_branch2a(res4b18_relu)
        bn4b19_branch2a = self.bn4b19_branch2a(res4b19_branch2a)
        res4b19_branch2a_relu = F.relu(bn4b19_branch2a)
        res4b19_branch2b_pad = F.pad(res4b19_branch2a_relu, (1, 1, 1, 1))
        res4b19_branch2b = self.res4b19_branch2b(res4b19_branch2b_pad)
        bn4b19_branch2b = self.bn4b19_branch2b(res4b19_branch2b)
        res4b19_branch2b_relu = F.relu(bn4b19_branch2b)
        res4b19_branch2c = self.res4b19_branch2c(res4b19_branch2b_relu)
        bn4b19_branch2c = self.bn4b19_branch2c(res4b19_branch2c)
        res4b19         = res4b18_relu + bn4b19_branch2c
        res4b19_relu    = F.relu(res4b19)
        res4b20_branch2a = self.res4b20_branch2a(res4b19_relu)
        bn4b20_branch2a = self.bn4b20_branch2a(res4b20_branch2a)
        res4b20_branch2a_relu = F.relu(bn4b20_branch2a)
        res4b20_branch2b_pad = F.pad(res4b20_branch2a_relu, (1, 1, 1, 1))
        res4b20_branch2b = self.res4b20_branch2b(res4b20_branch2b_pad)
        bn4b20_branch2b = self.bn4b20_branch2b(res4b20_branch2b)
        res4b20_branch2b_relu = F.relu(bn4b20_branch2b)
        res4b20_branch2c = self.res4b20_branch2c(res4b20_branch2b_relu)
        bn4b20_branch2c = self.bn4b20_branch2c(res4b20_branch2c)
        res4b20         = res4b19_relu + bn4b20_branch2c
        res4b20_relu    = F.relu(res4b20)
        res4b21_branch2a = self.res4b21_branch2a(res4b20_relu)
        bn4b21_branch2a = self.bn4b21_branch2a(res4b21_branch2a)
        res4b21_branch2a_relu = F.relu(bn4b21_branch2a)
        res4b21_branch2b_pad = F.pad(res4b21_branch2a_relu, (1, 1, 1, 1))
        res4b21_branch2b = self.res4b21_branch2b(res4b21_branch2b_pad)
        bn4b21_branch2b = self.bn4b21_branch2b(res4b21_branch2b)
        res4b21_branch2b_relu = F.relu(bn4b21_branch2b)
        res4b21_branch2c = self.res4b21_branch2c(res4b21_branch2b_relu)
        bn4b21_branch2c = self.bn4b21_branch2c(res4b21_branch2c)
        res4b21         = res4b20_relu + bn4b21_branch2c
        res4b21_relu    = F.relu(res4b21)
        res4b22_branch2a = self.res4b22_branch2a(res4b21_relu)
        bn4b22_branch2a = self.bn4b22_branch2a(res4b22_branch2a)
        res4b22_branch2a_relu = F.relu(bn4b22_branch2a)
        res4b22_branch2b_pad = F.pad(res4b22_branch2a_relu, (1, 1, 1, 1))
        res4b22_branch2b = self.res4b22_branch2b(res4b22_branch2b_pad)
        bn4b22_branch2b = self.bn4b22_branch2b(res4b22_branch2b)
        res4b22_branch2b_relu = F.relu(bn4b22_branch2b)
        res4b22_branch2c = self.res4b22_branch2c(res4b22_branch2b_relu)
        bn4b22_branch2c = self.bn4b22_branch2c(res4b22_branch2c)
        res4b22         = res4b21_relu + bn4b22_branch2c
        res4b22_relu    = F.relu(res4b22)
        return res4b22_relu


    @staticmethod
    def __batch_normalization(dim, name, **kwargs):
        if   dim == 0 or dim == 1:  layer = nn.BatchNorm1d(**kwargs)
        elif dim == 2:  layer = nn.BatchNorm2d(**kwargs)
        elif dim == 3:  layer = nn.BatchNorm3d(**kwargs)
        else:           raise NotImplementedError()

        #if &#39;scale&#39; in __bottom_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;scale&#39;]))
        #else:
        #    layer.weight.data.fill_(1)

        #if &#39;bias&#39; in __bottom_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;bias&#39;]))
        #else:
        #    layer.bias.data.fill_(0)

        #layer.state_dict()[&#39;running_mean&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;mean&#39;]))
        #layer.state_dict()[&#39;running_var&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;var&#39;]))
        return layer

    @staticmethod
    def __conv(dim, name, **kwargs):
        if   dim == 1:  layer = nn.Conv1d(**kwargs)
        elif dim == 2:  layer = nn.Conv2d(**kwargs)
        elif dim == 3:  layer = nn.Conv3d(**kwargs)
        else:           raise NotImplementedError()

        #layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;weights&#39;]))
        #if &#39;bias&#39; in __bottom_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;bias&#39;]))
        return layer





class TopLayers(nn.Module):
    def __init__(self, weight_file=None):
        super(TopLayers, self).__init__()
        #global __top_layers_weights_dict  # Loaded by MMDNN
        #__top_layers_weights_dict = load_weights(weight_file)

        self.res5a_branch1 = self.__conv(2, name=&#39;res5a_branch1&#39;, in_channels=1024, out_channels=2048, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.res5a_branch2a = self.__conv(2, name=&#39;res5a_branch2a&#39;, in_channels=1024, out_channels=512, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.bn5a_branch1 = self.__batch_normalization(2, &#39;bn5a_branch1&#39;, num_features=2048, eps=9.99999974738e-06, momentum=0.0)
        self.bn5a_branch2a = self.__batch_normalization(2, &#39;bn5a_branch2a&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5a_branch2b = self.__conv(2, name=&#39;res5a_branch2b&#39;, in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn5a_branch2b = self.__batch_normalization(2, &#39;bn5a_branch2b&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5a_branch2c = self.__conv(2, name=&#39;res5a_branch2c&#39;, in_channels=512, out_channels=2048, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn5a_branch2c = self.__batch_normalization(2, &#39;bn5a_branch2c&#39;, num_features=2048, eps=9.99999974738e-06, momentum=0.0)
        self.res5b_branch2a = self.__conv(2, name=&#39;res5b_branch2a&#39;, in_channels=2048, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn5b_branch2a = self.__batch_normalization(2, &#39;bn5b_branch2a&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5b_branch2b = self.__conv(2, name=&#39;res5b_branch2b&#39;, in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn5b_branch2b = self.__batch_normalization(2, &#39;bn5b_branch2b&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5b_branch2c = self.__conv(2, name=&#39;res5b_branch2c&#39;, in_channels=512, out_channels=2048, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn5b_branch2c = self.__batch_normalization(2, &#39;bn5b_branch2c&#39;, num_features=2048, eps=9.99999974738e-06, momentum=0.0)
        self.res5c_branch2a = self.__conv(2, name=&#39;res5c_branch2a&#39;, in_channels=2048, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn5c_branch2a = self.__batch_normalization(2, &#39;bn5c_branch2a&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5c_branch2b = self.__conv(2, name=&#39;res5c_branch2b&#39;, in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn5c_branch2b = self.__batch_normalization(2, &#39;bn5c_branch2b&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5c_branch2c = self.__conv(2, name=&#39;res5c_branch2c&#39;, in_channels=512, out_channels=2048, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn5c_branch2c = self.__batch_normalization(2, &#39;bn5c_branch2c&#39;, num_features=2048, eps=9.99999974738e-06, momentum=0.0)
        self.bbox_pred_1 = self.__dense(name = &#39;bbox_pred_1&#39;, in_features = 2048, out_features = 8, bias = True)
        self.cls_score_1 = self.__dense(name = &#39;cls_score_1&#39;, in_features = 2048, out_features = 2, bias = True)

    def forward(self, x):
        res5a_branch1   = self.res5a_branch1(x)
        res5a_branch2a  = self.res5a_branch2a(x)
        bn5a_branch1    = self.bn5a_branch1(res5a_branch1)
        bn5a_branch2a   = self.bn5a_branch2a(res5a_branch2a)
        res5a_branch2a_relu = F.relu(bn5a_branch2a)

        # Fix MMDNN dilated convolution bug
        #res5a_branch2b_pad = F.pad(res5a_branch2a_relu, (0, 0, 0, 0))
        #res5a_branch2b  = self.res5a_branch2b(res5a_branch2b_pad)
        # Fix broken dilated convolutions on MMDNN conversion, and roll in padding
        res5a_branch2b = F.conv2d(res5a_branch2a_relu, weight=self.res5a_branch2b.weight, bias=self.res5a_branch2b.bias, 
                                  stride=self.res5a_branch2b.stride, padding=(2,2), dilation=2, groups=self.res5a_branch2b.groups)

        bn5a_branch2b   = self.bn5a_branch2b(res5a_branch2b)
        res5a_branch2b_relu = F.relu(bn5a_branch2b)
        res5a_branch2c  = self.res5a_branch2c(res5a_branch2b_relu)
        bn5a_branch2c   = self.bn5a_branch2c(res5a_branch2c)
        res5a           = bn5a_branch1 + bn5a_branch2c
        res5a_relu      = F.relu(res5a)
        res5b_branch2a  = self.res5b_branch2a(res5a_relu)
        bn5b_branch2a   = self.bn5b_branch2a(res5b_branch2a)
        res5b_branch2a_relu = F.relu(bn5b_branch2a)

        # Fix MMDNN dilated convolution bug
        #res5b_branch2b_pad = F.pad(res5b_branch2a_relu, (0, 0, 0, 0))
        #res5b_branch2b  = self.res5b_branch2b(res5b_branch2b_pad)
        res5b_branch2b = F.conv2d(res5b_branch2a_relu, weight=self.res5b_branch2b.weight, bias=self.res5b_branch2b.bias, 
                                  stride=self.res5b_branch2b.stride, padding=(2,2), dilation=2, groups=self.res5b_branch2b.groups)

        bn5b_branch2b   = self.bn5b_branch2b(res5b_branch2b)
        res5b_branch2b_relu = F.relu(bn5b_branch2b)
        res5b_branch2c  = self.res5b_branch2c(res5b_branch2b_relu)
        bn5b_branch2c   = self.bn5b_branch2c(res5b_branch2c)
        res5b           = res5a_relu + bn5b_branch2c
        res5b_relu      = F.relu(res5b)
        res5c_branch2a  = self.res5c_branch2a(res5b_relu)
        bn5c_branch2a   = self.bn5c_branch2a(res5c_branch2a)
        res5c_branch2a_relu = F.relu(bn5c_branch2a)

        # Fix MMDNN dilated convolution bug
        #res5c_branch2b_pad = F.pad(res5c_branch2a_relu, (1, 1, 1, 1))
        #res5c_branch2b  = self.res5c_branch2b(res5c_branch2b_pad)
        res5c_branch2b = F.conv2d(res5c_branch2a_relu, weight=self.res5c_branch2b.weight, bias=self.res5c_branch2b.bias, 
                                  stride=self.res5c_branch2b.stride, padding=(2,2), dilation=2, groups=self.res5c_branch2b.groups)
        bn5c_branch2b   = self.bn5c_branch2b(res5c_branch2b)
        res5c_branch2b_relu = F.relu(bn5c_branch2b)
        res5c_branch2c  = self.res5c_branch2c(res5c_branch2b_relu)
        bn5c_branch2c   = self.bn5c_branch2c(res5c_branch2c)
        res5c           = res5b_relu + bn5c_branch2c
        res5c_relu      = F.relu(res5c)
        pool5           = F.avg_pool2d(res5c_relu, kernel_size=(7, 7), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)
        bbox_pred_0     = pool5.view(pool5.size(0), -1)
        cls_score_0     = pool5.view(pool5.size(0), -1)
        bbox_pred_1     = self.bbox_pred_1(bbox_pred_0)
        cls_score_1     = self.cls_score_1(cls_score_0)
        # import pdb; pdb.set_trace()
        cls_prob        = F.softmax(cls_score_1, dim=1)
        # Returning pre-softmax score to be consistent with Caffe implementation
        return bbox_pred_1, cls_prob, cls_score_1

    @staticmethod
    def __batch_normalization(dim, name, **kwargs):
        if   dim == 0 or dim == 1:  layer = nn.BatchNorm1d(**kwargs)
        elif dim == 2:  layer = nn.BatchNorm2d(**kwargs)
        elif dim == 3:  layer = nn.BatchNorm3d(**kwargs)
        else:           raise NotImplementedError()

        #if &#39;scale&#39; in __top_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;scale&#39;]))
        #else:
        #    layer.weight.data.fill_(1)

        #if &#39;bias&#39; in __top_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;bias&#39;]))
        #else:
        #    layer.bias.data.fill_(0)

        #layer.state_dict()[&#39;running_mean&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;mean&#39;]))
        #layer.state_dict()[&#39;running_var&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;var&#39;]))
        return layer

    @staticmethod
    def __conv(dim, name, **kwargs):
        if   dim == 1:  
            layer = nn.Conv1d(**kwargs)
        elif dim == 2:  
            layer = nn.Conv2d(**kwargs)
        elif dim == 3:  
            layer = nn.Conv3d(**kwargs)
        else:           
            raise NotImplementedError()

        #layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;weights&#39;]))
        #if &#39;bias&#39; in __top_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;bias&#39;]))
        return layer

    @staticmethod
    def __dense(name, **kwargs):
        layer = nn.Linear(**kwargs)
        #layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;weights&#39;]))
        #if &#39;bias&#39; in __top_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;bias&#39;]))
        return layer



class FasterRCNN(nn.Module):
    &#34;&#34;&#34;PyTorch-1.3 model conversion of ResNet-101_faster_rcnn_ohem_iter_20000.caffemodel, leveraging MMDNN conversion tools&#34;&#34;&#34;
    def __init__(self, device):
        super(FasterRCNN, self).__init__()

        self.device = device
        torch.device(device)

        self.top = TopLayers()
        self.top.eval()
        self.top = self.top.to(device)

        self.bottom = BottomLayers()
        self.bottom.eval()
        self.bottom = self.bottom.to(device)

        self.rpn = RpnLayers()
        self.rpn.eval()
        self.rpn = self.rpn.to(device)

        # Proposal layer:  manually imported from janus/src/rpn
        self._feat_stride = 16
        #self._anchors = rpn.generate_anchors.generate_anchors(scales=np.array( (8,16,32) ))    
        self._anchors = np.array([[ -84.,  -40.,   99.,   55.],
                                  [-176.,  -88.,  191.,  103.],
                                  [-360., -184.,  375.,  199.],
                                  [ -56.,  -56.,   71.,   71.],
                                  [-120., -120.,  135.,  135.],
                                  [-248., -248.,  263.,  263.],
                                  [ -36.,  -80.,   51.,   95.],
                                  [ -80., -168.,   95.,  183.],
                                  [-168., -344.,  183.,  359.]])
        self._num_anchors = self._anchors.shape[0]
        
    def __call__(self, im, im_info):
        # im is a tensor, N x 3 x H x W; im_info is another,
        # N x 3 (H, W, scale)
        with torch.no_grad():
            im = im.to(self.device)
            
            res4b22 = self.bottom(im)
            
            (rpn_cls_score, rpn_bbox_pred) = self.rpn(res4b22)
            
            (N,C,W,H) = rpn_cls_score.shape
            rpn_cls_score_reshape = torch.reshape(rpn_cls_score, (N, 2, -1, H))
            del rpn_cls_score
            rpn_cls_prob = torch.nn.functional.softmax(rpn_cls_score_reshape, dim=1)  # FIXME: is this dim right?
            rpn_cls_prob_reshape = torch.reshape(rpn_cls_prob, (N, 18, -1, H))
            del rpn_cls_prob
            
            # TODO: Make this handle multiple images, instead of horrible flaming death.
            rois = self._proposal_layer(rpn_cls_prob_reshape.cpu(), rpn_bbox_pred.cpu(), im_info)
            del rpn_bbox_pred
            # import pdb; pdb.set_trace()
            rois_gpu = rois.to(self.device)
            roi_pool5 = torchvision.ops.roi_pool(res4b22, rois_gpu, (14,14), 0.0625)
            del res4b22
            del rois_gpu
            (bbox_pred_1, cls_prob, cls_score) = self.top(roi_pool5)
            rois_cpu = rois.cpu()
            del rois
            bbox_pred_1_cpu = bbox_pred_1.cpu()
            del bbox_pred_1
            cls_prob_cpu = cls_prob.cpu()
            del cls_prob
            cls_score_cpu = cls_score.cpu()
            del cls_score
            del im
        return (rois_cpu, bbox_pred_1_cpu, cls_prob_cpu, cls_score_cpu)

    def _proposal_layer(self, rpn_cls_prob_reshape, rpn_bbox_pred, im_info):
        &#34;&#34;&#34;rpn.proposal_layer&#34;&#34;&#34;
        # &#39;Only single item batches are supported&#39;
        assert(rpn_cls_prob_reshape.shape[0] == 1) 

        # TODO: Defaults from caffe: make me configurable 
        #   {&#39;PROPOSAL_METHOD&#39;: &#39;selective_search&#39;, &#39;SVM&#39;: False, &#39;NMS&#39;: 0.3, &#39;RPN_NMS_THRESH&#39;: 0.7, &#39;SCALES&#39;: [800], 
        #   &#39;RPN_POST_NMS_TOP_N&#39;: 300, &#39;HAS_RPN&#39;: False, &#39;RPN_PRE_NMS_TOP_N&#39;: 6000, &#39;BBOX_REG&#39;: True, &#39;RPN_MIN_SIZE&#39;: 3, &#39;MAX_SIZE&#39;: 1333}
        cfg_key = &#39;TEST&#39;      # either &#39;TRAIN&#39; or &#39;TEST&#39;
        pre_nms_topN = 6000   # RPN_PRE_NMS_TOP_N
        post_nms_topN = 300   # RPN_POST_NMS_TOP_N
        nms_thresh= 0.7       # RPN_NMS_THRESH
        min_size = 3          # RPN_MIN_SIZE                

        # the first set of _num_anchors channels are bg probs
        # the second set are the fg probs, which we want
        scores = rpn_cls_prob_reshape.detach().numpy()[:, self._num_anchors:, :, :]
        bbox_deltas = rpn_bbox_pred.detach().numpy()
        (im_height, im_width, im_scale) = im_info[0]  # H, W, scale

        # 1. Generate proposals from bbox deltas and shifted anchors
        height, width = scores.shape[-2:]

        # Enumerate all shifts
        shift_x = np.arange(0, width) * self._feat_stride
        shift_y = np.arange(0, height) * self._feat_stride
        shift_x, shift_y = np.meshgrid(shift_x, shift_y)
        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),
                            shift_x.ravel(), shift_y.ravel())).transpose()

        # Enumerate all shifted anchors:
        #
        # add A anchors (1, A, 4) to
        # cell K shifts (K, 1, 4) to get
        # shift anchors (K, A, 4)
        # reshape to (K*A, 4) shifted anchors
        A = self._num_anchors
        K = shifts.shape[0]
        anchors = self._anchors.reshape((1, A, 4)) + \
                  shifts.reshape((1, K, 4)).transpose((1, 0, 2))
        anchors = anchors.reshape((K * A, 4))

        # Transpose and reshape predicted bbox transformations to get them
        # into the same order as the anchors:
        #
        # bbox deltas will be (1, 4 * A, H, W) format
        # transpose to (1, H, W, 4 * A)
        # reshape to (1 * H * W * A, 4) where rows are ordered by (h, w, a)
        # in slowest to fastest order
        bbox_deltas = bbox_deltas.transpose((0, 2, 3, 1)).reshape((-1, 4))

        # Same story for the scores:
        #
        # scores are (1, A, H, W) format
        # transpose to (1, H, W, A)
        # reshape to (1 * H * W * A, 1) where rows are ordered by (h, w, a)
        scores = scores.transpose((0, 2, 3, 1)).reshape((-1, 1))

        # Convert anchors into proposals via bbox transformations
        proposals = self._bbox_transform_inv(anchors, bbox_deltas)

        # 2. clip predicted boxes to image
        proposals = self._clip_boxes(proposals, (im_height.item(), im_width.item()))

        # 3. remove predicted boxes with either height or width &lt; threshold
        # (NOTE: convert min_size to input image scale stored in im_info[2])
        keep = self._filter_boxes(proposals, min_size * im_scale.item())
        proposals = proposals[keep, :]
        scores = scores[keep]

        # 4. sort all (proposal, score) pairs by score from highest to lowest
        # 5. take top pre_nms_topN (e.g. 6000)
        order = scores.ravel().argsort()[::-1]
        if pre_nms_topN &gt; 0:
            order = order[:pre_nms_topN]
        proposals = proposals[order, :]
        scores = scores[order]

        # 6. apply nms (e.g. threshold = 0.7)
        # 7. take after_nms_topN (e.g. 300)
        # 8. return the top proposals (-&gt; RoIs top)
        keep = self._nms(np.hstack((proposals, scores)), nms_thresh)
        if post_nms_topN &gt; 0:
            keep = keep[:post_nms_topN]
        proposals = proposals[keep, :]
        scores = scores[keep]

        # Output rois blob
        # Our RPN implementation only supports a single input image, so all
        # batch inds are 0
        batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)
        blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))
        return torch.tensor(blob)


    def _bbox_transform_inv(self, boxes, deltas):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/fast_rcnn.bbox_transform.bbox_transform_inv&#34;&#34;&#34;
        if boxes.shape[0] == 0:
            return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)

        boxes = boxes.astype(deltas.dtype, copy=False)

        widths = boxes[:, 2] - boxes[:, 0] + 1.0
        heights = boxes[:, 3] - boxes[:, 1] + 1.0
        ctr_x = boxes[:, 0] + 0.5 * widths
        ctr_y = boxes[:, 1] + 0.5 * heights
        
        dx = deltas[:, 0::4]
        dy = deltas[:, 1::4]
        dw = deltas[:, 2::4]
        dh = deltas[:, 3::4]
        
        pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]
        pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]
        pred_w = np.exp(dw) * widths[:, np.newaxis]
        pred_h = np.exp(dh) * heights[:, np.newaxis]

        pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)
        # x1
        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w
        # y1
        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h
        # x2
        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w
        # y2
        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h
        
        return pred_boxes

    def _clip_boxes(self, boxes, im_shape):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/fast_rcnn.bbox_transform.clip_boxes&#34;&#34;&#34;
        # x1 &gt;= 0
        boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)
        # y1 &gt;= 0
        boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)
        # x2 &lt; im_shape[1]
        boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)
        # y2 &lt; im_shape[0]
        boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)
        return boxes


    def _filter_boxes(self, boxes, min_size):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/rpn.proposal_layer._filter_boxes&#34;&#34;&#34;
        ws = boxes[:, 2] - boxes[:, 0] + 1
        hs = boxes[:, 3] - boxes[:, 1] + 1
        keep = np.where((ws &gt;= min_size) &amp; (hs &gt;= min_size))[0]
        return keep


    def _nms(self, dets, thresh):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/nms/py_cpu_nms.py&#34;&#34;&#34;
        &#34;&#34;&#34;FIXME: GPU acceleration needed?&#34;&#34;&#34;
        x1 = dets[:, 0]
        y1 = dets[:, 1]
        x2 = dets[:, 2]
        y2 = dets[:, 3]
        scores = dets[:, 4]
        
        areas = (x2 - x1 + 1) * (y2 - y1 + 1)
        order = scores.argsort()[::-1]
        
        keep = []
        while order.size &gt; 0:
            i = order[0]
            keep.append(i)
            xx1 = np.maximum(x1[i], x1[order[1:]])
            yy1 = np.maximum(y1[i], y1[order[1:]])
            xx2 = np.minimum(x2[i], x2[order[1:]])
            yy2 = np.minimum(y2[i], y2[order[1:]])
            
            w = np.maximum(0.0, xx2 - xx1 + 1)
            h = np.maximum(0.0, yy2 - yy1 + 1)
            inter = w * h
            ovr = inter / (areas[i] + areas[order[1:]] - inter)

            inds = np.where(ovr &lt;= thresh)[0]
            order = order[inds + 1]
        
        return keep




class FasterRCNN_MMDNN(nn.Module):
    &#34;&#34;&#34;PyTorch-1.3 model conversion of ResNet-101_faster_rcnn_ohem_iter_20000.caffemodel, leveraging MMDNN conversion tools&#34;&#34;&#34;
    def __init__(self, model_dir, device):
        super(FasterRCNN_MMDNN, self).__init__()

        self.device = device
        torch.device(device)
        # Converted using convert_caffe_to_pytorch.convert_top()
        dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), &#39;..&#39;, &#39;models&#39;, &#39;detection&#39;)
        MainModel = imp.load_source(&#39;MainModel&#39;, os.path.join(dir, &#34;top_layers.py&#34;))
        self.top = torch.load(os.path.join(model_dir, &#34;top_layers.pth&#34;), map_location=device)
        self.top.eval()

        # Converted using convert_caffe_to_pytorch.convert_bottom()
        MainModel = imp.load_source(&#39;MainModel&#39;, os.path.join(dir, &#34;bottom_layers.py&#34;))
        self.bottom = torch.load(os.path.join(model_dir, &#34;bottom_layers.pth&#34;), map_location=device)
        self.bottom.eval()
        self.bottom = self.bottom.to(device)

        # Converted using convert_caffe_to_pytorch.convert_rpn()
        MainModel = imp.load_source(&#39;MainModel&#39;, os.path.join(dir, &#34;rpn_layers.py&#34;))
        self.rpn = torch.load(os.path.join(model_dir, &#34;rpn_layers.pth&#34;), map_location=device)
        self.rpn.eval()
        self.rpn = self.rpn.to(device)


        # Proposal layer:  manually imported from janus/src/rpn
        self._feat_stride = 16
        #self._anchors = rpn.generate_anchors.generate_anchors(scales=np.array( (8,16,32) ))    
        self._anchors = np.array([[ -84.,  -40.,   99.,   55.],
                                  [-176.,  -88.,  191.,  103.],
                                  [-360., -184.,  375.,  199.],
                                  [ -56.,  -56.,   71.,   71.],
                                  [-120., -120.,  135.,  135.],
                                  [-248., -248.,  263.,  263.],
                                  [ -36.,  -80.,   51.,   95.],
                                  [ -80., -168.,   95.,  183.],
                                  [-168., -344.,  183.,  359.]])
        self._num_anchors = self._anchors.shape[0]
        
    def __call__(self, im, im_info):
        # im is a tensor, N x 3 x H x W; im_info is another,
        # N x 3 (H, W, scale)
        # import pdb; pdb.set_trace()
        with torch.no_grad():
            im = im.to(self.device)
            
            res4b22 = self.bottom(im)
            
            (rpn_cls_score, rpn_bbox_pred) = self.rpn(res4b22)
            
            (N,C,W,H) = rpn_cls_score.shape
            rpn_cls_score_reshape = torch.reshape(rpn_cls_score, (N, 2, -1, H))
            del rpn_cls_score
            rpn_cls_prob = torch.nn.functional.softmax(rpn_cls_score_reshape, dim=1)  # FIXME: is this dim right?
            rpn_cls_prob_reshape = torch.reshape(rpn_cls_prob, (N, 18, -1, H))
            del rpn_cls_prob
            
            # TODO: Make this handle multiple images, instead of horrible flaming death.
            rois = self._proposal_layer(rpn_cls_prob_reshape.cpu(), rpn_bbox_pred.cpu(), im_info)
            del rpn_bbox_pred
            # import pdb; pdb.set_trace()
            rois_gpu = rois.to(self.device)
            roi_pool5 = torchvision.ops.roi_pool(res4b22, rois_gpu, (14,14), 0.0625)
            del res4b22
            del rois_gpu
            (bbox_pred_1, cls_prob, cls_score) = self.top(roi_pool5)
            rois_cpu = rois.cpu()
            del rois
            bbox_pred_1_cpu = bbox_pred_1.cpu()
            del bbox_pred_1
            cls_prob_cpu = cls_prob.cpu()
            del cls_prob
            cls_score_cpu = cls_score.cpu()   
            del cls_score
            del im
        return (rois_cpu, bbox_pred_1_cpu, cls_prob_cpu, cls_score_cpu)

    def _proposal_layer(self, rpn_cls_prob_reshape, rpn_bbox_pred, im_info):
        &#34;&#34;&#34;rpn.proposal_layer&#34;&#34;&#34;
        # &#39;Only single item batches are supported&#39;
        assert(rpn_cls_prob_reshape.shape[0] == 1) 

        # TODO: Defaults from caffe: make me configurable 
        #   {&#39;PROPOSAL_METHOD&#39;: &#39;selective_search&#39;, &#39;SVM&#39;: False, &#39;NMS&#39;: 0.3, &#39;RPN_NMS_THRESH&#39;: 0.7, &#39;SCALES&#39;: [800], 
        #   &#39;RPN_POST_NMS_TOP_N&#39;: 300, &#39;HAS_RPN&#39;: False, &#39;RPN_PRE_NMS_TOP_N&#39;: 6000, &#39;BBOX_REG&#39;: True, &#39;RPN_MIN_SIZE&#39;: 3, &#39;MAX_SIZE&#39;: 1333}
        cfg_key = &#39;TEST&#39;      # either &#39;TRAIN&#39; or &#39;TEST&#39;
        pre_nms_topN = 6000   # RPN_PRE_NMS_TOP_N
        post_nms_topN = 300   # RPN_POST_NMS_TOP_N
        nms_thresh= 0.7       # RPN_NMS_THRESH
        min_size = 3          # RPN_MIN_SIZE                

        # the first set of _num_anchors channels are bg probs
        # the second set are the fg probs, which we want
        scores = rpn_cls_prob_reshape.detach().numpy()[:, self._num_anchors:, :, :]
        bbox_deltas = rpn_bbox_pred.detach().numpy()
        (im_height, im_width, im_scale) = im_info[0]  # H, W, scale

        # 1. Generate proposals from bbox deltas and shifted anchors
        height, width = scores.shape[-2:]

        # Enumerate all shifts
        shift_x = np.arange(0, width) * self._feat_stride
        shift_y = np.arange(0, height) * self._feat_stride
        shift_x, shift_y = np.meshgrid(shift_x, shift_y)
        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),
                            shift_x.ravel(), shift_y.ravel())).transpose()

        # Enumerate all shifted anchors:
        #
        # add A anchors (1, A, 4) to
        # cell K shifts (K, 1, 4) to get
        # shift anchors (K, A, 4)
        # reshape to (K*A, 4) shifted anchors
        A = self._num_anchors
        K = shifts.shape[0]
        anchors = self._anchors.reshape((1, A, 4)) + \
                  shifts.reshape((1, K, 4)).transpose((1, 0, 2))
        anchors = anchors.reshape((K * A, 4))

        # Transpose and reshape predicted bbox transformations to get them
        # into the same order as the anchors:
        #
        # bbox deltas will be (1, 4 * A, H, W) format
        # transpose to (1, H, W, 4 * A)
        # reshape to (1 * H * W * A, 4) where rows are ordered by (h, w, a)
        # in slowest to fastest order
        bbox_deltas = bbox_deltas.transpose((0, 2, 3, 1)).reshape((-1, 4))

        # Same story for the scores:
        #
        # scores are (1, A, H, W) format
        # transpose to (1, H, W, A)
        # reshape to (1 * H * W * A, 1) where rows are ordered by (h, w, a)
        scores = scores.transpose((0, 2, 3, 1)).reshape((-1, 1))

        # Convert anchors into proposals via bbox transformations
        proposals = self._bbox_transform_inv(anchors, bbox_deltas)

        # 2. clip predicted boxes to image
        proposals = self._clip_boxes(proposals, (im_height.item(), im_width.item()))

        # 3. remove predicted boxes with either height or width &lt; threshold
        # (NOTE: convert min_size to input image scale stored in im_info[2])
        keep = self._filter_boxes(proposals, min_size * im_scale.item())
        proposals = proposals[keep, :]
        scores = scores[keep]

        # 4. sort all (proposal, score) pairs by score from highest to lowest
        # 5. take top pre_nms_topN (e.g. 6000)
        order = scores.ravel().argsort()[::-1]
        if pre_nms_topN &gt; 0:
            order = order[:pre_nms_topN]
        proposals = proposals[order, :]
        scores = scores[order]

        # 6. apply nms (e.g. threshold = 0.7)
        # 7. take after_nms_topN (e.g. 300)
        # 8. return the top proposals (-&gt; RoIs top)
        keep = self._nms(np.hstack((proposals, scores)), nms_thresh)
        if post_nms_topN &gt; 0:
            keep = keep[:post_nms_topN]
        proposals = proposals[keep, :]
        scores = scores[keep]

        # Output rois blob
        # Our RPN implementation only supports a single input image, so all
        # batch inds are 0
        batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)
        blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))
        return torch.tensor(blob)


    def _bbox_transform_inv(self, boxes, deltas):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/fast_rcnn.bbox_transform.bbox_transform_inv&#34;&#34;&#34;
        if boxes.shape[0] == 0:
            return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)

        boxes = boxes.astype(deltas.dtype, copy=False)

        widths = boxes[:, 2] - boxes[:, 0] + 1.0
        heights = boxes[:, 3] - boxes[:, 1] + 1.0
        ctr_x = boxes[:, 0] + 0.5 * widths
        ctr_y = boxes[:, 1] + 0.5 * heights
        
        dx = deltas[:, 0::4]
        dy = deltas[:, 1::4]
        dw = deltas[:, 2::4]
        dh = deltas[:, 3::4]
        
        pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]
        pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]
        pred_w = np.exp(dw) * widths[:, np.newaxis]
        pred_h = np.exp(dh) * heights[:, np.newaxis]

        pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)
        # x1
        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w
        # y1
        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h
        # x2
        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w
        # y2
        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h
        
        return pred_boxes

    def _clip_boxes(self, boxes, im_shape):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/fast_rcnn.bbox_transform.clip_boxes&#34;&#34;&#34;
        # x1 &gt;= 0
        boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)
        # y1 &gt;= 0
        boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)
        # x2 &lt; im_shape[1]
        boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)
        # y2 &lt; im_shape[0]
        boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)
        return boxes


    def _filter_boxes(self, boxes, min_size):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/rpn.proposal_layer._filter_boxes&#34;&#34;&#34;
        ws = boxes[:, 2] - boxes[:, 0] + 1
        hs = boxes[:, 3] - boxes[:, 1] + 1
        keep = np.where((ws &gt;= min_size) &amp; (hs &gt;= min_size))[0]
        return keep


    def _nms(self, dets, thresh):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/nms/py_cpu_nms.py&#34;&#34;&#34;
        &#34;&#34;&#34;FIXME: GPU acceleration needed?&#34;&#34;&#34;
        x1 = dets[:, 0]
        y1 = dets[:, 1]
        x2 = dets[:, 2]
        y2 = dets[:, 3]
        scores = dets[:, 4]
        
        areas = (x2 - x1 + 1) * (y2 - y1 + 1)
        order = scores.argsort()[::-1]
        
        keep = []
        while order.size &gt; 0:
            i = order[0]
            keep.append(i)
            xx1 = np.maximum(x1[i], x1[order[1:]])
            yy1 = np.maximum(y1[i], y1[order[1:]])
            xx2 = np.minimum(x2[i], x2[order[1:]])
            yy2 = np.minimum(y2[i], y2[order[1:]])
            
            w = np.maximum(0.0, xx2 - xx1 + 1)
            h = np.maximum(0.0, yy2 - yy1 + 1)
            inter = w * h
            ovr = inter / (areas[i] + areas[order[1:]] - inter)

            inds = np.where(ovr &lt;= thresh)[0]
            order = order[inds + 1]
        
        return keep


def conversion():
    mmdnn = FasterRCNN_MMDNN(&#39;./models/detection&#39;, &#39;cpu&#39;); 
    net = FasterRCNN(&#39;cpu&#39;)
    net.load_state_dict(mmdnn.state_dict())
    torch.save(net.state_dict(), &#39;./models/detection/faster_rcnn.pth&#39;)
    return net</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pycollector.model.face.faster_rcnn.conversion"><code class="name flex">
<span>def <span class="ident">conversion</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/pycollector/model/face/faster_rcnn.py#L1293-L1298" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def conversion():
    mmdnn = FasterRCNN_MMDNN(&#39;./models/detection&#39;, &#39;cpu&#39;); 
    net = FasterRCNN(&#39;cpu&#39;)
    net.load_state_dict(mmdnn.state_dict())
    torch.save(net.state_dict(), &#39;./models/detection/faster_rcnn.pth&#39;)
    return net</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pycollector.model.face.faster_rcnn.BottomLayers"><code class="flex name class">
<span>class <span class="ident">BottomLayers</span></span>
<span>(</span><span>weight_file=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/pycollector/model/face/faster_rcnn.py#L63-L635" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class BottomLayers(nn.Module):    
    def __init__(self, weight_file=None):
        super(BottomLayers, self).__init__()
        #global __bottom_layers_weights_dict  # loaded by MMDNN
        #__bottom_layers_weights_dict = load_weights(weight_file)

        self.conv1 = self.__conv(2, name=&#39;conv1&#39;, in_channels=3, out_channels=64, kernel_size=(7, 7), stride=(2, 2), groups=1, bias=False)
        self.bn_conv1 = self.__batch_normalization(2, &#39;bn_conv1&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2a_branch2a = self.__conv(2, name=&#39;res2a_branch2a&#39;, in_channels=64, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.res2a_branch1 = self.__conv(2, name=&#39;res2a_branch1&#39;, in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2a_branch2a = self.__batch_normalization(2, &#39;bn2a_branch2a&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.bn2a_branch1 = self.__batch_normalization(2, &#39;bn2a_branch1&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res2a_branch2b = self.__conv(2, name=&#39;res2a_branch2b&#39;, in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn2a_branch2b = self.__batch_normalization(2, &#39;bn2a_branch2b&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2a_branch2c = self.__conv(2, name=&#39;res2a_branch2c&#39;, in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2a_branch2c = self.__batch_normalization(2, &#39;bn2a_branch2c&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res2b_branch2a = self.__conv(2, name=&#39;res2b_branch2a&#39;, in_channels=256, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2b_branch2a = self.__batch_normalization(2, &#39;bn2b_branch2a&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2b_branch2b = self.__conv(2, name=&#39;res2b_branch2b&#39;, in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn2b_branch2b = self.__batch_normalization(2, &#39;bn2b_branch2b&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2b_branch2c = self.__conv(2, name=&#39;res2b_branch2c&#39;, in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2b_branch2c = self.__batch_normalization(2, &#39;bn2b_branch2c&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res2c_branch2a = self.__conv(2, name=&#39;res2c_branch2a&#39;, in_channels=256, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2c_branch2a = self.__batch_normalization(2, &#39;bn2c_branch2a&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2c_branch2b = self.__conv(2, name=&#39;res2c_branch2b&#39;, in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn2c_branch2b = self.__batch_normalization(2, &#39;bn2c_branch2b&#39;, num_features=64, eps=9.99999974738e-06, momentum=0.0)
        self.res2c_branch2c = self.__conv(2, name=&#39;res2c_branch2c&#39;, in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn2c_branch2c = self.__batch_normalization(2, &#39;bn2c_branch2c&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res3a_branch1 = self.__conv(2, name=&#39;res3a_branch1&#39;, in_channels=256, out_channels=512, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.res3a_branch2a = self.__conv(2, name=&#39;res3a_branch2a&#39;, in_channels=256, out_channels=128, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.bn3a_branch1 = self.__batch_normalization(2, &#39;bn3a_branch1&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.bn3a_branch2a = self.__batch_normalization(2, &#39;bn3a_branch2a&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3a_branch2b = self.__conv(2, name=&#39;res3a_branch2b&#39;, in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn3a_branch2b = self.__batch_normalization(2, &#39;bn3a_branch2b&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3a_branch2c = self.__conv(2, name=&#39;res3a_branch2c&#39;, in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3a_branch2c = self.__batch_normalization(2, &#39;bn3a_branch2c&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res3b1_branch2a = self.__conv(2, name=&#39;res3b1_branch2a&#39;, in_channels=512, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b1_branch2a = self.__batch_normalization(2, &#39;bn3b1_branch2a&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b1_branch2b = self.__conv(2, name=&#39;res3b1_branch2b&#39;, in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn3b1_branch2b = self.__batch_normalization(2, &#39;bn3b1_branch2b&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b1_branch2c = self.__conv(2, name=&#39;res3b1_branch2c&#39;, in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b1_branch2c = self.__batch_normalization(2, &#39;bn3b1_branch2c&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res3b2_branch2a = self.__conv(2, name=&#39;res3b2_branch2a&#39;, in_channels=512, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b2_branch2a = self.__batch_normalization(2, &#39;bn3b2_branch2a&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b2_branch2b = self.__conv(2, name=&#39;res3b2_branch2b&#39;, in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn3b2_branch2b = self.__batch_normalization(2, &#39;bn3b2_branch2b&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b2_branch2c = self.__conv(2, name=&#39;res3b2_branch2c&#39;, in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b2_branch2c = self.__batch_normalization(2, &#39;bn3b2_branch2c&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res3b3_branch2a = self.__conv(2, name=&#39;res3b3_branch2a&#39;, in_channels=512, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b3_branch2a = self.__batch_normalization(2, &#39;bn3b3_branch2a&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b3_branch2b = self.__conv(2, name=&#39;res3b3_branch2b&#39;, in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn3b3_branch2b = self.__batch_normalization(2, &#39;bn3b3_branch2b&#39;, num_features=128, eps=9.99999974738e-06, momentum=0.0)
        self.res3b3_branch2c = self.__conv(2, name=&#39;res3b3_branch2c&#39;, in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn3b3_branch2c = self.__batch_normalization(2, &#39;bn3b3_branch2c&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res4a_branch1 = self.__conv(2, name=&#39;res4a_branch1&#39;, in_channels=512, out_channels=1024, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.res4a_branch2a = self.__conv(2, name=&#39;res4a_branch2a&#39;, in_channels=512, out_channels=256, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.bn4a_branch1 = self.__batch_normalization(2, &#39;bn4a_branch1&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.bn4a_branch2a = self.__batch_normalization(2, &#39;bn4a_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4a_branch2b = self.__conv(2, name=&#39;res4a_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4a_branch2b = self.__batch_normalization(2, &#39;bn4a_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4a_branch2c = self.__conv(2, name=&#39;res4a_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4a_branch2c = self.__batch_normalization(2, &#39;bn4a_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b1_branch2a = self.__conv(2, name=&#39;res4b1_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b1_branch2a = self.__batch_normalization(2, &#39;bn4b1_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b1_branch2b = self.__conv(2, name=&#39;res4b1_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b1_branch2b = self.__batch_normalization(2, &#39;bn4b1_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b1_branch2c = self.__conv(2, name=&#39;res4b1_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b1_branch2c = self.__batch_normalization(2, &#39;bn4b1_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b2_branch2a = self.__conv(2, name=&#39;res4b2_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b2_branch2a = self.__batch_normalization(2, &#39;bn4b2_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b2_branch2b = self.__conv(2, name=&#39;res4b2_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b2_branch2b = self.__batch_normalization(2, &#39;bn4b2_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b2_branch2c = self.__conv(2, name=&#39;res4b2_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b2_branch2c = self.__batch_normalization(2, &#39;bn4b2_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b3_branch2a = self.__conv(2, name=&#39;res4b3_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b3_branch2a = self.__batch_normalization(2, &#39;bn4b3_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b3_branch2b = self.__conv(2, name=&#39;res4b3_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b3_branch2b = self.__batch_normalization(2, &#39;bn4b3_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b3_branch2c = self.__conv(2, name=&#39;res4b3_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b3_branch2c = self.__batch_normalization(2, &#39;bn4b3_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b4_branch2a = self.__conv(2, name=&#39;res4b4_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b4_branch2a = self.__batch_normalization(2, &#39;bn4b4_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b4_branch2b = self.__conv(2, name=&#39;res4b4_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b4_branch2b = self.__batch_normalization(2, &#39;bn4b4_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b4_branch2c = self.__conv(2, name=&#39;res4b4_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b4_branch2c = self.__batch_normalization(2, &#39;bn4b4_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b5_branch2a = self.__conv(2, name=&#39;res4b5_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b5_branch2a = self.__batch_normalization(2, &#39;bn4b5_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b5_branch2b = self.__conv(2, name=&#39;res4b5_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b5_branch2b = self.__batch_normalization(2, &#39;bn4b5_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b5_branch2c = self.__conv(2, name=&#39;res4b5_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b5_branch2c = self.__batch_normalization(2, &#39;bn4b5_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b6_branch2a = self.__conv(2, name=&#39;res4b6_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b6_branch2a = self.__batch_normalization(2, &#39;bn4b6_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b6_branch2b = self.__conv(2, name=&#39;res4b6_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b6_branch2b = self.__batch_normalization(2, &#39;bn4b6_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b6_branch2c = self.__conv(2, name=&#39;res4b6_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b6_branch2c = self.__batch_normalization(2, &#39;bn4b6_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b7_branch2a = self.__conv(2, name=&#39;res4b7_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b7_branch2a = self.__batch_normalization(2, &#39;bn4b7_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b7_branch2b = self.__conv(2, name=&#39;res4b7_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b7_branch2b = self.__batch_normalization(2, &#39;bn4b7_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b7_branch2c = self.__conv(2, name=&#39;res4b7_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b7_branch2c = self.__batch_normalization(2, &#39;bn4b7_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b8_branch2a = self.__conv(2, name=&#39;res4b8_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b8_branch2a = self.__batch_normalization(2, &#39;bn4b8_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b8_branch2b = self.__conv(2, name=&#39;res4b8_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b8_branch2b = self.__batch_normalization(2, &#39;bn4b8_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b8_branch2c = self.__conv(2, name=&#39;res4b8_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b8_branch2c = self.__batch_normalization(2, &#39;bn4b8_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b9_branch2a = self.__conv(2, name=&#39;res4b9_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b9_branch2a = self.__batch_normalization(2, &#39;bn4b9_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b9_branch2b = self.__conv(2, name=&#39;res4b9_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b9_branch2b = self.__batch_normalization(2, &#39;bn4b9_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b9_branch2c = self.__conv(2, name=&#39;res4b9_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b9_branch2c = self.__batch_normalization(2, &#39;bn4b9_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b10_branch2a = self.__conv(2, name=&#39;res4b10_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b10_branch2a = self.__batch_normalization(2, &#39;bn4b10_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b10_branch2b = self.__conv(2, name=&#39;res4b10_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b10_branch2b = self.__batch_normalization(2, &#39;bn4b10_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b10_branch2c = self.__conv(2, name=&#39;res4b10_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b10_branch2c = self.__batch_normalization(2, &#39;bn4b10_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b11_branch2a = self.__conv(2, name=&#39;res4b11_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b11_branch2a = self.__batch_normalization(2, &#39;bn4b11_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b11_branch2b = self.__conv(2, name=&#39;res4b11_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b11_branch2b = self.__batch_normalization(2, &#39;bn4b11_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b11_branch2c = self.__conv(2, name=&#39;res4b11_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b11_branch2c = self.__batch_normalization(2, &#39;bn4b11_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b12_branch2a = self.__conv(2, name=&#39;res4b12_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b12_branch2a = self.__batch_normalization(2, &#39;bn4b12_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b12_branch2b = self.__conv(2, name=&#39;res4b12_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b12_branch2b = self.__batch_normalization(2, &#39;bn4b12_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b12_branch2c = self.__conv(2, name=&#39;res4b12_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b12_branch2c = self.__batch_normalization(2, &#39;bn4b12_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b13_branch2a = self.__conv(2, name=&#39;res4b13_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b13_branch2a = self.__batch_normalization(2, &#39;bn4b13_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b13_branch2b = self.__conv(2, name=&#39;res4b13_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b13_branch2b = self.__batch_normalization(2, &#39;bn4b13_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b13_branch2c = self.__conv(2, name=&#39;res4b13_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b13_branch2c = self.__batch_normalization(2, &#39;bn4b13_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b14_branch2a = self.__conv(2, name=&#39;res4b14_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b14_branch2a = self.__batch_normalization(2, &#39;bn4b14_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b14_branch2b = self.__conv(2, name=&#39;res4b14_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b14_branch2b = self.__batch_normalization(2, &#39;bn4b14_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b14_branch2c = self.__conv(2, name=&#39;res4b14_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b14_branch2c = self.__batch_normalization(2, &#39;bn4b14_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b15_branch2a = self.__conv(2, name=&#39;res4b15_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b15_branch2a = self.__batch_normalization(2, &#39;bn4b15_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b15_branch2b = self.__conv(2, name=&#39;res4b15_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b15_branch2b = self.__batch_normalization(2, &#39;bn4b15_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b15_branch2c = self.__conv(2, name=&#39;res4b15_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b15_branch2c = self.__batch_normalization(2, &#39;bn4b15_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b16_branch2a = self.__conv(2, name=&#39;res4b16_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b16_branch2a = self.__batch_normalization(2, &#39;bn4b16_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b16_branch2b = self.__conv(2, name=&#39;res4b16_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b16_branch2b = self.__batch_normalization(2, &#39;bn4b16_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b16_branch2c = self.__conv(2, name=&#39;res4b16_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b16_branch2c = self.__batch_normalization(2, &#39;bn4b16_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b17_branch2a = self.__conv(2, name=&#39;res4b17_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b17_branch2a = self.__batch_normalization(2, &#39;bn4b17_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b17_branch2b = self.__conv(2, name=&#39;res4b17_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b17_branch2b = self.__batch_normalization(2, &#39;bn4b17_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b17_branch2c = self.__conv(2, name=&#39;res4b17_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b17_branch2c = self.__batch_normalization(2, &#39;bn4b17_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b18_branch2a = self.__conv(2, name=&#39;res4b18_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b18_branch2a = self.__batch_normalization(2, &#39;bn4b18_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b18_branch2b = self.__conv(2, name=&#39;res4b18_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b18_branch2b = self.__batch_normalization(2, &#39;bn4b18_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b18_branch2c = self.__conv(2, name=&#39;res4b18_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b18_branch2c = self.__batch_normalization(2, &#39;bn4b18_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b19_branch2a = self.__conv(2, name=&#39;res4b19_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b19_branch2a = self.__batch_normalization(2, &#39;bn4b19_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b19_branch2b = self.__conv(2, name=&#39;res4b19_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b19_branch2b = self.__batch_normalization(2, &#39;bn4b19_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b19_branch2c = self.__conv(2, name=&#39;res4b19_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b19_branch2c = self.__batch_normalization(2, &#39;bn4b19_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b20_branch2a = self.__conv(2, name=&#39;res4b20_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b20_branch2a = self.__batch_normalization(2, &#39;bn4b20_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b20_branch2b = self.__conv(2, name=&#39;res4b20_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b20_branch2b = self.__batch_normalization(2, &#39;bn4b20_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b20_branch2c = self.__conv(2, name=&#39;res4b20_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b20_branch2c = self.__batch_normalization(2, &#39;bn4b20_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b21_branch2a = self.__conv(2, name=&#39;res4b21_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b21_branch2a = self.__batch_normalization(2, &#39;bn4b21_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b21_branch2b = self.__conv(2, name=&#39;res4b21_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b21_branch2b = self.__batch_normalization(2, &#39;bn4b21_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b21_branch2c = self.__conv(2, name=&#39;res4b21_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b21_branch2c = self.__batch_normalization(2, &#39;bn4b21_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)
        self.res4b22_branch2a = self.__conv(2, name=&#39;res4b22_branch2a&#39;, in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b22_branch2a = self.__batch_normalization(2, &#39;bn4b22_branch2a&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b22_branch2b = self.__conv(2, name=&#39;res4b22_branch2b&#39;, in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn4b22_branch2b = self.__batch_normalization(2, &#39;bn4b22_branch2b&#39;, num_features=256, eps=9.99999974738e-06, momentum=0.0)
        self.res4b22_branch2c = self.__conv(2, name=&#39;res4b22_branch2c&#39;, in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn4b22_branch2c = self.__batch_normalization(2, &#39;bn4b22_branch2c&#39;, num_features=1024, eps=9.99999974738e-06, momentum=0.0)

    def forward(self, x):
        conv1_pad       = F.pad(x, (3, 3, 3, 3))
        conv1           = self.conv1(conv1_pad)
        bn_conv1        = self.bn_conv1(conv1)
        conv1_relu      = F.relu(bn_conv1)
        pool1_pad       = F.pad(conv1_relu, (0, 1, 0, 1), value=float(&#39;-inf&#39;))
        pool1           = F.max_pool2d(pool1_pad, kernel_size=(3, 3), stride=(2, 2), padding=0, ceil_mode=False)
        res2a_branch2a  = self.res2a_branch2a(pool1)
        res2a_branch1   = self.res2a_branch1(pool1)
        bn2a_branch2a   = self.bn2a_branch2a(res2a_branch2a)
        bn2a_branch1    = self.bn2a_branch1(res2a_branch1)
        res2a_branch2a_relu = F.relu(bn2a_branch2a)
        res2a_branch2b_pad = F.pad(res2a_branch2a_relu, (1, 1, 1, 1))
        res2a_branch2b  = self.res2a_branch2b(res2a_branch2b_pad)
        bn2a_branch2b   = self.bn2a_branch2b(res2a_branch2b)
        res2a_branch2b_relu = F.relu(bn2a_branch2b)
        res2a_branch2c  = self.res2a_branch2c(res2a_branch2b_relu)
        bn2a_branch2c   = self.bn2a_branch2c(res2a_branch2c)
        res2a           = bn2a_branch1 + bn2a_branch2c
        res2a_relu      = F.relu(res2a)
        res2b_branch2a  = self.res2b_branch2a(res2a_relu)
        bn2b_branch2a   = self.bn2b_branch2a(res2b_branch2a)
        res2b_branch2a_relu = F.relu(bn2b_branch2a)
        res2b_branch2b_pad = F.pad(res2b_branch2a_relu, (1, 1, 1, 1))
        res2b_branch2b  = self.res2b_branch2b(res2b_branch2b_pad)
        bn2b_branch2b   = self.bn2b_branch2b(res2b_branch2b)
        res2b_branch2b_relu = F.relu(bn2b_branch2b)
        res2b_branch2c  = self.res2b_branch2c(res2b_branch2b_relu)
        bn2b_branch2c   = self.bn2b_branch2c(res2b_branch2c)
        res2b           = res2a_relu + bn2b_branch2c
        res2b_relu      = F.relu(res2b)
        res2c_branch2a  = self.res2c_branch2a(res2b_relu)
        bn2c_branch2a   = self.bn2c_branch2a(res2c_branch2a)
        res2c_branch2a_relu = F.relu(bn2c_branch2a)
        res2c_branch2b_pad = F.pad(res2c_branch2a_relu, (1, 1, 1, 1))
        res2c_branch2b  = self.res2c_branch2b(res2c_branch2b_pad)
        bn2c_branch2b   = self.bn2c_branch2b(res2c_branch2b)
        res2c_branch2b_relu = F.relu(bn2c_branch2b)
        res2c_branch2c  = self.res2c_branch2c(res2c_branch2b_relu)
        bn2c_branch2c   = self.bn2c_branch2c(res2c_branch2c)
        res2c           = res2b_relu + bn2c_branch2c
        res2c_relu      = F.relu(res2c)
        res3a_branch1   = self.res3a_branch1(res2c_relu)
        res3a_branch2a  = self.res3a_branch2a(res2c_relu)
        bn3a_branch1    = self.bn3a_branch1(res3a_branch1)
        bn3a_branch2a   = self.bn3a_branch2a(res3a_branch2a)
        res3a_branch2a_relu = F.relu(bn3a_branch2a)
        res3a_branch2b_pad = F.pad(res3a_branch2a_relu, (1, 1, 1, 1))
        res3a_branch2b  = self.res3a_branch2b(res3a_branch2b_pad)
        bn3a_branch2b   = self.bn3a_branch2b(res3a_branch2b)
        res3a_branch2b_relu = F.relu(bn3a_branch2b)
        res3a_branch2c  = self.res3a_branch2c(res3a_branch2b_relu)
        bn3a_branch2c   = self.bn3a_branch2c(res3a_branch2c)
        res3a           = bn3a_branch1 + bn3a_branch2c
        res3a_relu      = F.relu(res3a)
        res3b1_branch2a = self.res3b1_branch2a(res3a_relu)
        bn3b1_branch2a  = self.bn3b1_branch2a(res3b1_branch2a)
        res3b1_branch2a_relu = F.relu(bn3b1_branch2a)
        res3b1_branch2b_pad = F.pad(res3b1_branch2a_relu, (1, 1, 1, 1))
        res3b1_branch2b = self.res3b1_branch2b(res3b1_branch2b_pad)
        bn3b1_branch2b  = self.bn3b1_branch2b(res3b1_branch2b)
        res3b1_branch2b_relu = F.relu(bn3b1_branch2b)
        res3b1_branch2c = self.res3b1_branch2c(res3b1_branch2b_relu)
        bn3b1_branch2c  = self.bn3b1_branch2c(res3b1_branch2c)
        res3b1          = res3a_relu + bn3b1_branch2c
        res3b1_relu     = F.relu(res3b1)
        res3b2_branch2a = self.res3b2_branch2a(res3b1_relu)
        bn3b2_branch2a  = self.bn3b2_branch2a(res3b2_branch2a)
        res3b2_branch2a_relu = F.relu(bn3b2_branch2a)
        res3b2_branch2b_pad = F.pad(res3b2_branch2a_relu, (1, 1, 1, 1))
        res3b2_branch2b = self.res3b2_branch2b(res3b2_branch2b_pad)
        bn3b2_branch2b  = self.bn3b2_branch2b(res3b2_branch2b)
        res3b2_branch2b_relu = F.relu(bn3b2_branch2b)
        res3b2_branch2c = self.res3b2_branch2c(res3b2_branch2b_relu)
        bn3b2_branch2c  = self.bn3b2_branch2c(res3b2_branch2c)
        res3b2          = res3b1_relu + bn3b2_branch2c
        res3b2_relu     = F.relu(res3b2)
        res3b3_branch2a = self.res3b3_branch2a(res3b2_relu)
        bn3b3_branch2a  = self.bn3b3_branch2a(res3b3_branch2a)
        res3b3_branch2a_relu = F.relu(bn3b3_branch2a)
        res3b3_branch2b_pad = F.pad(res3b3_branch2a_relu, (1, 1, 1, 1))
        res3b3_branch2b = self.res3b3_branch2b(res3b3_branch2b_pad)
        bn3b3_branch2b  = self.bn3b3_branch2b(res3b3_branch2b)
        res3b3_branch2b_relu = F.relu(bn3b3_branch2b)
        res3b3_branch2c = self.res3b3_branch2c(res3b3_branch2b_relu)
        bn3b3_branch2c  = self.bn3b3_branch2c(res3b3_branch2c)
        res3b3          = res3b2_relu + bn3b3_branch2c
        res3b3_relu     = F.relu(res3b3)
        res4a_branch1   = self.res4a_branch1(res3b3_relu)
        res4a_branch2a  = self.res4a_branch2a(res3b3_relu)
        bn4a_branch1    = self.bn4a_branch1(res4a_branch1)
        bn4a_branch2a   = self.bn4a_branch2a(res4a_branch2a)
        res4a_branch2a_relu = F.relu(bn4a_branch2a)
        res4a_branch2b_pad = F.pad(res4a_branch2a_relu, (1, 1, 1, 1))
        res4a_branch2b  = self.res4a_branch2b(res4a_branch2b_pad)
        bn4a_branch2b   = self.bn4a_branch2b(res4a_branch2b)
        res4a_branch2b_relu = F.relu(bn4a_branch2b)
        res4a_branch2c  = self.res4a_branch2c(res4a_branch2b_relu)
        bn4a_branch2c   = self.bn4a_branch2c(res4a_branch2c)
        res4a           = bn4a_branch1 + bn4a_branch2c
        res4a_relu      = F.relu(res4a)
        res4b1_branch2a = self.res4b1_branch2a(res4a_relu)
        bn4b1_branch2a  = self.bn4b1_branch2a(res4b1_branch2a)
        res4b1_branch2a_relu = F.relu(bn4b1_branch2a)
        res4b1_branch2b_pad = F.pad(res4b1_branch2a_relu, (1, 1, 1, 1))
        res4b1_branch2b = self.res4b1_branch2b(res4b1_branch2b_pad)
        bn4b1_branch2b  = self.bn4b1_branch2b(res4b1_branch2b)
        res4b1_branch2b_relu = F.relu(bn4b1_branch2b)
        res4b1_branch2c = self.res4b1_branch2c(res4b1_branch2b_relu)
        bn4b1_branch2c  = self.bn4b1_branch2c(res4b1_branch2c)
        res4b1          = res4a_relu + bn4b1_branch2c
        res4b1_relu     = F.relu(res4b1)
        res4b2_branch2a = self.res4b2_branch2a(res4b1_relu)
        bn4b2_branch2a  = self.bn4b2_branch2a(res4b2_branch2a)
        res4b2_branch2a_relu = F.relu(bn4b2_branch2a)
        res4b2_branch2b_pad = F.pad(res4b2_branch2a_relu, (1, 1, 1, 1))
        res4b2_branch2b = self.res4b2_branch2b(res4b2_branch2b_pad)
        bn4b2_branch2b  = self.bn4b2_branch2b(res4b2_branch2b)
        res4b2_branch2b_relu = F.relu(bn4b2_branch2b)
        res4b2_branch2c = self.res4b2_branch2c(res4b2_branch2b_relu)
        bn4b2_branch2c  = self.bn4b2_branch2c(res4b2_branch2c)
        res4b2          = res4b1_relu + bn4b2_branch2c
        res4b2_relu     = F.relu(res4b2)
        res4b3_branch2a = self.res4b3_branch2a(res4b2_relu)
        bn4b3_branch2a  = self.bn4b3_branch2a(res4b3_branch2a)
        res4b3_branch2a_relu = F.relu(bn4b3_branch2a)
        res4b3_branch2b_pad = F.pad(res4b3_branch2a_relu, (1, 1, 1, 1))
        res4b3_branch2b = self.res4b3_branch2b(res4b3_branch2b_pad)
        bn4b3_branch2b  = self.bn4b3_branch2b(res4b3_branch2b)
        res4b3_branch2b_relu = F.relu(bn4b3_branch2b)
        res4b3_branch2c = self.res4b3_branch2c(res4b3_branch2b_relu)
        bn4b3_branch2c  = self.bn4b3_branch2c(res4b3_branch2c)
        res4b3          = res4b2_relu + bn4b3_branch2c
        res4b3_relu     = F.relu(res4b3)
        res4b4_branch2a = self.res4b4_branch2a(res4b3_relu)
        bn4b4_branch2a  = self.bn4b4_branch2a(res4b4_branch2a)
        res4b4_branch2a_relu = F.relu(bn4b4_branch2a)
        res4b4_branch2b_pad = F.pad(res4b4_branch2a_relu, (1, 1, 1, 1))
        res4b4_branch2b = self.res4b4_branch2b(res4b4_branch2b_pad)
        bn4b4_branch2b  = self.bn4b4_branch2b(res4b4_branch2b)
        res4b4_branch2b_relu = F.relu(bn4b4_branch2b)
        res4b4_branch2c = self.res4b4_branch2c(res4b4_branch2b_relu)
        bn4b4_branch2c  = self.bn4b4_branch2c(res4b4_branch2c)
        res4b4          = res4b3_relu + bn4b4_branch2c
        res4b4_relu     = F.relu(res4b4)
        res4b5_branch2a = self.res4b5_branch2a(res4b4_relu)
        bn4b5_branch2a  = self.bn4b5_branch2a(res4b5_branch2a)
        res4b5_branch2a_relu = F.relu(bn4b5_branch2a)
        res4b5_branch2b_pad = F.pad(res4b5_branch2a_relu, (1, 1, 1, 1))
        res4b5_branch2b = self.res4b5_branch2b(res4b5_branch2b_pad)
        bn4b5_branch2b  = self.bn4b5_branch2b(res4b5_branch2b)
        res4b5_branch2b_relu = F.relu(bn4b5_branch2b)
        res4b5_branch2c = self.res4b5_branch2c(res4b5_branch2b_relu)
        bn4b5_branch2c  = self.bn4b5_branch2c(res4b5_branch2c)
        res4b5          = res4b4_relu + bn4b5_branch2c
        res4b5_relu     = F.relu(res4b5)
        res4b6_branch2a = self.res4b6_branch2a(res4b5_relu)
        bn4b6_branch2a  = self.bn4b6_branch2a(res4b6_branch2a)
        res4b6_branch2a_relu = F.relu(bn4b6_branch2a)
        res4b6_branch2b_pad = F.pad(res4b6_branch2a_relu, (1, 1, 1, 1))
        res4b6_branch2b = self.res4b6_branch2b(res4b6_branch2b_pad)
        bn4b6_branch2b  = self.bn4b6_branch2b(res4b6_branch2b)
        res4b6_branch2b_relu = F.relu(bn4b6_branch2b)
        res4b6_branch2c = self.res4b6_branch2c(res4b6_branch2b_relu)
        bn4b6_branch2c  = self.bn4b6_branch2c(res4b6_branch2c)
        res4b6          = res4b5_relu + bn4b6_branch2c
        res4b6_relu     = F.relu(res4b6)
        res4b7_branch2a = self.res4b7_branch2a(res4b6_relu)
        bn4b7_branch2a  = self.bn4b7_branch2a(res4b7_branch2a)
        res4b7_branch2a_relu = F.relu(bn4b7_branch2a)
        res4b7_branch2b_pad = F.pad(res4b7_branch2a_relu, (1, 1, 1, 1))
        res4b7_branch2b = self.res4b7_branch2b(res4b7_branch2b_pad)
        bn4b7_branch2b  = self.bn4b7_branch2b(res4b7_branch2b)
        res4b7_branch2b_relu = F.relu(bn4b7_branch2b)
        res4b7_branch2c = self.res4b7_branch2c(res4b7_branch2b_relu)
        bn4b7_branch2c  = self.bn4b7_branch2c(res4b7_branch2c)
        res4b7          = res4b6_relu + bn4b7_branch2c
        res4b7_relu     = F.relu(res4b7)
        res4b8_branch2a = self.res4b8_branch2a(res4b7_relu)
        bn4b8_branch2a  = self.bn4b8_branch2a(res4b8_branch2a)
        res4b8_branch2a_relu = F.relu(bn4b8_branch2a)
        res4b8_branch2b_pad = F.pad(res4b8_branch2a_relu, (1, 1, 1, 1))
        res4b8_branch2b = self.res4b8_branch2b(res4b8_branch2b_pad)
        bn4b8_branch2b  = self.bn4b8_branch2b(res4b8_branch2b)
        res4b8_branch2b_relu = F.relu(bn4b8_branch2b)
        res4b8_branch2c = self.res4b8_branch2c(res4b8_branch2b_relu)
        bn4b8_branch2c  = self.bn4b8_branch2c(res4b8_branch2c)
        res4b8          = res4b7_relu + bn4b8_branch2c
        res4b8_relu     = F.relu(res4b8)
        res4b9_branch2a = self.res4b9_branch2a(res4b8_relu)
        bn4b9_branch2a  = self.bn4b9_branch2a(res4b9_branch2a)
        res4b9_branch2a_relu = F.relu(bn4b9_branch2a)
        res4b9_branch2b_pad = F.pad(res4b9_branch2a_relu, (1, 1, 1, 1))
        res4b9_branch2b = self.res4b9_branch2b(res4b9_branch2b_pad)
        bn4b9_branch2b  = self.bn4b9_branch2b(res4b9_branch2b)
        res4b9_branch2b_relu = F.relu(bn4b9_branch2b)
        res4b9_branch2c = self.res4b9_branch2c(res4b9_branch2b_relu)
        bn4b9_branch2c  = self.bn4b9_branch2c(res4b9_branch2c)
        res4b9          = res4b8_relu + bn4b9_branch2c
        res4b9_relu     = F.relu(res4b9)
        res4b10_branch2a = self.res4b10_branch2a(res4b9_relu)
        bn4b10_branch2a = self.bn4b10_branch2a(res4b10_branch2a)
        res4b10_branch2a_relu = F.relu(bn4b10_branch2a)
        res4b10_branch2b_pad = F.pad(res4b10_branch2a_relu, (1, 1, 1, 1))
        res4b10_branch2b = self.res4b10_branch2b(res4b10_branch2b_pad)
        bn4b10_branch2b = self.bn4b10_branch2b(res4b10_branch2b)
        res4b10_branch2b_relu = F.relu(bn4b10_branch2b)
        res4b10_branch2c = self.res4b10_branch2c(res4b10_branch2b_relu)
        bn4b10_branch2c = self.bn4b10_branch2c(res4b10_branch2c)
        res4b10         = res4b9_relu + bn4b10_branch2c
        res4b10_relu    = F.relu(res4b10)
        res4b11_branch2a = self.res4b11_branch2a(res4b10_relu)
        bn4b11_branch2a = self.bn4b11_branch2a(res4b11_branch2a)
        res4b11_branch2a_relu = F.relu(bn4b11_branch2a)
        res4b11_branch2b_pad = F.pad(res4b11_branch2a_relu, (1, 1, 1, 1))
        res4b11_branch2b = self.res4b11_branch2b(res4b11_branch2b_pad)
        bn4b11_branch2b = self.bn4b11_branch2b(res4b11_branch2b)
        res4b11_branch2b_relu = F.relu(bn4b11_branch2b)
        res4b11_branch2c = self.res4b11_branch2c(res4b11_branch2b_relu)
        bn4b11_branch2c = self.bn4b11_branch2c(res4b11_branch2c)
        res4b11         = res4b10_relu + bn4b11_branch2c
        res4b11_relu    = F.relu(res4b11)
        res4b12_branch2a = self.res4b12_branch2a(res4b11_relu)
        bn4b12_branch2a = self.bn4b12_branch2a(res4b12_branch2a)
        res4b12_branch2a_relu = F.relu(bn4b12_branch2a)
        res4b12_branch2b_pad = F.pad(res4b12_branch2a_relu, (1, 1, 1, 1))
        res4b12_branch2b = self.res4b12_branch2b(res4b12_branch2b_pad)
        bn4b12_branch2b = self.bn4b12_branch2b(res4b12_branch2b)
        res4b12_branch2b_relu = F.relu(bn4b12_branch2b)
        res4b12_branch2c = self.res4b12_branch2c(res4b12_branch2b_relu)
        bn4b12_branch2c = self.bn4b12_branch2c(res4b12_branch2c)
        res4b12         = res4b11_relu + bn4b12_branch2c
        res4b12_relu    = F.relu(res4b12)
        res4b13_branch2a = self.res4b13_branch2a(res4b12_relu)
        bn4b13_branch2a = self.bn4b13_branch2a(res4b13_branch2a)
        res4b13_branch2a_relu = F.relu(bn4b13_branch2a)
        res4b13_branch2b_pad = F.pad(res4b13_branch2a_relu, (1, 1, 1, 1))
        res4b13_branch2b = self.res4b13_branch2b(res4b13_branch2b_pad)
        bn4b13_branch2b = self.bn4b13_branch2b(res4b13_branch2b)
        res4b13_branch2b_relu = F.relu(bn4b13_branch2b)
        res4b13_branch2c = self.res4b13_branch2c(res4b13_branch2b_relu)
        bn4b13_branch2c = self.bn4b13_branch2c(res4b13_branch2c)
        res4b13         = res4b12_relu + bn4b13_branch2c
        res4b13_relu    = F.relu(res4b13)
        res4b14_branch2a = self.res4b14_branch2a(res4b13_relu)
        bn4b14_branch2a = self.bn4b14_branch2a(res4b14_branch2a)
        res4b14_branch2a_relu = F.relu(bn4b14_branch2a)
        res4b14_branch2b_pad = F.pad(res4b14_branch2a_relu, (1, 1, 1, 1))
        res4b14_branch2b = self.res4b14_branch2b(res4b14_branch2b_pad)
        bn4b14_branch2b = self.bn4b14_branch2b(res4b14_branch2b)
        res4b14_branch2b_relu = F.relu(bn4b14_branch2b)
        res4b14_branch2c = self.res4b14_branch2c(res4b14_branch2b_relu)
        bn4b14_branch2c = self.bn4b14_branch2c(res4b14_branch2c)
        res4b14         = res4b13_relu + bn4b14_branch2c
        res4b14_relu    = F.relu(res4b14)
        res4b15_branch2a = self.res4b15_branch2a(res4b14_relu)
        bn4b15_branch2a = self.bn4b15_branch2a(res4b15_branch2a)
        res4b15_branch2a_relu = F.relu(bn4b15_branch2a)
        res4b15_branch2b_pad = F.pad(res4b15_branch2a_relu, (1, 1, 1, 1))
        res4b15_branch2b = self.res4b15_branch2b(res4b15_branch2b_pad)
        bn4b15_branch2b = self.bn4b15_branch2b(res4b15_branch2b)
        res4b15_branch2b_relu = F.relu(bn4b15_branch2b)
        res4b15_branch2c = self.res4b15_branch2c(res4b15_branch2b_relu)
        bn4b15_branch2c = self.bn4b15_branch2c(res4b15_branch2c)
        res4b15         = res4b14_relu + bn4b15_branch2c
        res4b15_relu    = F.relu(res4b15)
        res4b16_branch2a = self.res4b16_branch2a(res4b15_relu)
        bn4b16_branch2a = self.bn4b16_branch2a(res4b16_branch2a)
        res4b16_branch2a_relu = F.relu(bn4b16_branch2a)
        res4b16_branch2b_pad = F.pad(res4b16_branch2a_relu, (1, 1, 1, 1))
        res4b16_branch2b = self.res4b16_branch2b(res4b16_branch2b_pad)
        bn4b16_branch2b = self.bn4b16_branch2b(res4b16_branch2b)
        res4b16_branch2b_relu = F.relu(bn4b16_branch2b)
        res4b16_branch2c = self.res4b16_branch2c(res4b16_branch2b_relu)
        bn4b16_branch2c = self.bn4b16_branch2c(res4b16_branch2c)
        res4b16         = res4b15_relu + bn4b16_branch2c
        res4b16_relu    = F.relu(res4b16)
        res4b17_branch2a = self.res4b17_branch2a(res4b16_relu)
        bn4b17_branch2a = self.bn4b17_branch2a(res4b17_branch2a)
        res4b17_branch2a_relu = F.relu(bn4b17_branch2a)
        res4b17_branch2b_pad = F.pad(res4b17_branch2a_relu, (1, 1, 1, 1))
        res4b17_branch2b = self.res4b17_branch2b(res4b17_branch2b_pad)
        bn4b17_branch2b = self.bn4b17_branch2b(res4b17_branch2b)
        res4b17_branch2b_relu = F.relu(bn4b17_branch2b)
        res4b17_branch2c = self.res4b17_branch2c(res4b17_branch2b_relu)
        bn4b17_branch2c = self.bn4b17_branch2c(res4b17_branch2c)
        res4b17         = res4b16_relu + bn4b17_branch2c
        res4b17_relu    = F.relu(res4b17)
        res4b18_branch2a = self.res4b18_branch2a(res4b17_relu)
        bn4b18_branch2a = self.bn4b18_branch2a(res4b18_branch2a)
        res4b18_branch2a_relu = F.relu(bn4b18_branch2a)
        res4b18_branch2b_pad = F.pad(res4b18_branch2a_relu, (1, 1, 1, 1))
        res4b18_branch2b = self.res4b18_branch2b(res4b18_branch2b_pad)
        bn4b18_branch2b = self.bn4b18_branch2b(res4b18_branch2b)
        res4b18_branch2b_relu = F.relu(bn4b18_branch2b)
        res4b18_branch2c = self.res4b18_branch2c(res4b18_branch2b_relu)
        bn4b18_branch2c = self.bn4b18_branch2c(res4b18_branch2c)
        res4b18         = res4b17_relu + bn4b18_branch2c
        res4b18_relu    = F.relu(res4b18)
        res4b19_branch2a = self.res4b19_branch2a(res4b18_relu)
        bn4b19_branch2a = self.bn4b19_branch2a(res4b19_branch2a)
        res4b19_branch2a_relu = F.relu(bn4b19_branch2a)
        res4b19_branch2b_pad = F.pad(res4b19_branch2a_relu, (1, 1, 1, 1))
        res4b19_branch2b = self.res4b19_branch2b(res4b19_branch2b_pad)
        bn4b19_branch2b = self.bn4b19_branch2b(res4b19_branch2b)
        res4b19_branch2b_relu = F.relu(bn4b19_branch2b)
        res4b19_branch2c = self.res4b19_branch2c(res4b19_branch2b_relu)
        bn4b19_branch2c = self.bn4b19_branch2c(res4b19_branch2c)
        res4b19         = res4b18_relu + bn4b19_branch2c
        res4b19_relu    = F.relu(res4b19)
        res4b20_branch2a = self.res4b20_branch2a(res4b19_relu)
        bn4b20_branch2a = self.bn4b20_branch2a(res4b20_branch2a)
        res4b20_branch2a_relu = F.relu(bn4b20_branch2a)
        res4b20_branch2b_pad = F.pad(res4b20_branch2a_relu, (1, 1, 1, 1))
        res4b20_branch2b = self.res4b20_branch2b(res4b20_branch2b_pad)
        bn4b20_branch2b = self.bn4b20_branch2b(res4b20_branch2b)
        res4b20_branch2b_relu = F.relu(bn4b20_branch2b)
        res4b20_branch2c = self.res4b20_branch2c(res4b20_branch2b_relu)
        bn4b20_branch2c = self.bn4b20_branch2c(res4b20_branch2c)
        res4b20         = res4b19_relu + bn4b20_branch2c
        res4b20_relu    = F.relu(res4b20)
        res4b21_branch2a = self.res4b21_branch2a(res4b20_relu)
        bn4b21_branch2a = self.bn4b21_branch2a(res4b21_branch2a)
        res4b21_branch2a_relu = F.relu(bn4b21_branch2a)
        res4b21_branch2b_pad = F.pad(res4b21_branch2a_relu, (1, 1, 1, 1))
        res4b21_branch2b = self.res4b21_branch2b(res4b21_branch2b_pad)
        bn4b21_branch2b = self.bn4b21_branch2b(res4b21_branch2b)
        res4b21_branch2b_relu = F.relu(bn4b21_branch2b)
        res4b21_branch2c = self.res4b21_branch2c(res4b21_branch2b_relu)
        bn4b21_branch2c = self.bn4b21_branch2c(res4b21_branch2c)
        res4b21         = res4b20_relu + bn4b21_branch2c
        res4b21_relu    = F.relu(res4b21)
        res4b22_branch2a = self.res4b22_branch2a(res4b21_relu)
        bn4b22_branch2a = self.bn4b22_branch2a(res4b22_branch2a)
        res4b22_branch2a_relu = F.relu(bn4b22_branch2a)
        res4b22_branch2b_pad = F.pad(res4b22_branch2a_relu, (1, 1, 1, 1))
        res4b22_branch2b = self.res4b22_branch2b(res4b22_branch2b_pad)
        bn4b22_branch2b = self.bn4b22_branch2b(res4b22_branch2b)
        res4b22_branch2b_relu = F.relu(bn4b22_branch2b)
        res4b22_branch2c = self.res4b22_branch2c(res4b22_branch2b_relu)
        bn4b22_branch2c = self.bn4b22_branch2c(res4b22_branch2c)
        res4b22         = res4b21_relu + bn4b22_branch2c
        res4b22_relu    = F.relu(res4b22)
        return res4b22_relu


    @staticmethod
    def __batch_normalization(dim, name, **kwargs):
        if   dim == 0 or dim == 1:  layer = nn.BatchNorm1d(**kwargs)
        elif dim == 2:  layer = nn.BatchNorm2d(**kwargs)
        elif dim == 3:  layer = nn.BatchNorm3d(**kwargs)
        else:           raise NotImplementedError()

        #if &#39;scale&#39; in __bottom_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;scale&#39;]))
        #else:
        #    layer.weight.data.fill_(1)

        #if &#39;bias&#39; in __bottom_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;bias&#39;]))
        #else:
        #    layer.bias.data.fill_(0)

        #layer.state_dict()[&#39;running_mean&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;mean&#39;]))
        #layer.state_dict()[&#39;running_var&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;var&#39;]))
        return layer

    @staticmethod
    def __conv(dim, name, **kwargs):
        if   dim == 1:  layer = nn.Conv1d(**kwargs)
        elif dim == 2:  layer = nn.Conv2d(**kwargs)
        elif dim == 3:  layer = nn.Conv3d(**kwargs)
        else:           raise NotImplementedError()

        #layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;weights&#39;]))
        #if &#39;bias&#39; in __bottom_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__bottom_layers_weights_dict[name][&#39;bias&#39;]))
        return layer</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pycollector.model.face.faster_rcnn.BottomLayers.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pycollector.model.face.faster_rcnn.BottomLayers.training"><code class="name">var <span class="ident">training</span> :bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pycollector.model.face.faster_rcnn.BottomLayers.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) >Callable[...,Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/pycollector/model/face/faster_rcnn.py#L258-L601" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def forward(self, x):
    conv1_pad       = F.pad(x, (3, 3, 3, 3))
    conv1           = self.conv1(conv1_pad)
    bn_conv1        = self.bn_conv1(conv1)
    conv1_relu      = F.relu(bn_conv1)
    pool1_pad       = F.pad(conv1_relu, (0, 1, 0, 1), value=float(&#39;-inf&#39;))
    pool1           = F.max_pool2d(pool1_pad, kernel_size=(3, 3), stride=(2, 2), padding=0, ceil_mode=False)
    res2a_branch2a  = self.res2a_branch2a(pool1)
    res2a_branch1   = self.res2a_branch1(pool1)
    bn2a_branch2a   = self.bn2a_branch2a(res2a_branch2a)
    bn2a_branch1    = self.bn2a_branch1(res2a_branch1)
    res2a_branch2a_relu = F.relu(bn2a_branch2a)
    res2a_branch2b_pad = F.pad(res2a_branch2a_relu, (1, 1, 1, 1))
    res2a_branch2b  = self.res2a_branch2b(res2a_branch2b_pad)
    bn2a_branch2b   = self.bn2a_branch2b(res2a_branch2b)
    res2a_branch2b_relu = F.relu(bn2a_branch2b)
    res2a_branch2c  = self.res2a_branch2c(res2a_branch2b_relu)
    bn2a_branch2c   = self.bn2a_branch2c(res2a_branch2c)
    res2a           = bn2a_branch1 + bn2a_branch2c
    res2a_relu      = F.relu(res2a)
    res2b_branch2a  = self.res2b_branch2a(res2a_relu)
    bn2b_branch2a   = self.bn2b_branch2a(res2b_branch2a)
    res2b_branch2a_relu = F.relu(bn2b_branch2a)
    res2b_branch2b_pad = F.pad(res2b_branch2a_relu, (1, 1, 1, 1))
    res2b_branch2b  = self.res2b_branch2b(res2b_branch2b_pad)
    bn2b_branch2b   = self.bn2b_branch2b(res2b_branch2b)
    res2b_branch2b_relu = F.relu(bn2b_branch2b)
    res2b_branch2c  = self.res2b_branch2c(res2b_branch2b_relu)
    bn2b_branch2c   = self.bn2b_branch2c(res2b_branch2c)
    res2b           = res2a_relu + bn2b_branch2c
    res2b_relu      = F.relu(res2b)
    res2c_branch2a  = self.res2c_branch2a(res2b_relu)
    bn2c_branch2a   = self.bn2c_branch2a(res2c_branch2a)
    res2c_branch2a_relu = F.relu(bn2c_branch2a)
    res2c_branch2b_pad = F.pad(res2c_branch2a_relu, (1, 1, 1, 1))
    res2c_branch2b  = self.res2c_branch2b(res2c_branch2b_pad)
    bn2c_branch2b   = self.bn2c_branch2b(res2c_branch2b)
    res2c_branch2b_relu = F.relu(bn2c_branch2b)
    res2c_branch2c  = self.res2c_branch2c(res2c_branch2b_relu)
    bn2c_branch2c   = self.bn2c_branch2c(res2c_branch2c)
    res2c           = res2b_relu + bn2c_branch2c
    res2c_relu      = F.relu(res2c)
    res3a_branch1   = self.res3a_branch1(res2c_relu)
    res3a_branch2a  = self.res3a_branch2a(res2c_relu)
    bn3a_branch1    = self.bn3a_branch1(res3a_branch1)
    bn3a_branch2a   = self.bn3a_branch2a(res3a_branch2a)
    res3a_branch2a_relu = F.relu(bn3a_branch2a)
    res3a_branch2b_pad = F.pad(res3a_branch2a_relu, (1, 1, 1, 1))
    res3a_branch2b  = self.res3a_branch2b(res3a_branch2b_pad)
    bn3a_branch2b   = self.bn3a_branch2b(res3a_branch2b)
    res3a_branch2b_relu = F.relu(bn3a_branch2b)
    res3a_branch2c  = self.res3a_branch2c(res3a_branch2b_relu)
    bn3a_branch2c   = self.bn3a_branch2c(res3a_branch2c)
    res3a           = bn3a_branch1 + bn3a_branch2c
    res3a_relu      = F.relu(res3a)
    res3b1_branch2a = self.res3b1_branch2a(res3a_relu)
    bn3b1_branch2a  = self.bn3b1_branch2a(res3b1_branch2a)
    res3b1_branch2a_relu = F.relu(bn3b1_branch2a)
    res3b1_branch2b_pad = F.pad(res3b1_branch2a_relu, (1, 1, 1, 1))
    res3b1_branch2b = self.res3b1_branch2b(res3b1_branch2b_pad)
    bn3b1_branch2b  = self.bn3b1_branch2b(res3b1_branch2b)
    res3b1_branch2b_relu = F.relu(bn3b1_branch2b)
    res3b1_branch2c = self.res3b1_branch2c(res3b1_branch2b_relu)
    bn3b1_branch2c  = self.bn3b1_branch2c(res3b1_branch2c)
    res3b1          = res3a_relu + bn3b1_branch2c
    res3b1_relu     = F.relu(res3b1)
    res3b2_branch2a = self.res3b2_branch2a(res3b1_relu)
    bn3b2_branch2a  = self.bn3b2_branch2a(res3b2_branch2a)
    res3b2_branch2a_relu = F.relu(bn3b2_branch2a)
    res3b2_branch2b_pad = F.pad(res3b2_branch2a_relu, (1, 1, 1, 1))
    res3b2_branch2b = self.res3b2_branch2b(res3b2_branch2b_pad)
    bn3b2_branch2b  = self.bn3b2_branch2b(res3b2_branch2b)
    res3b2_branch2b_relu = F.relu(bn3b2_branch2b)
    res3b2_branch2c = self.res3b2_branch2c(res3b2_branch2b_relu)
    bn3b2_branch2c  = self.bn3b2_branch2c(res3b2_branch2c)
    res3b2          = res3b1_relu + bn3b2_branch2c
    res3b2_relu     = F.relu(res3b2)
    res3b3_branch2a = self.res3b3_branch2a(res3b2_relu)
    bn3b3_branch2a  = self.bn3b3_branch2a(res3b3_branch2a)
    res3b3_branch2a_relu = F.relu(bn3b3_branch2a)
    res3b3_branch2b_pad = F.pad(res3b3_branch2a_relu, (1, 1, 1, 1))
    res3b3_branch2b = self.res3b3_branch2b(res3b3_branch2b_pad)
    bn3b3_branch2b  = self.bn3b3_branch2b(res3b3_branch2b)
    res3b3_branch2b_relu = F.relu(bn3b3_branch2b)
    res3b3_branch2c = self.res3b3_branch2c(res3b3_branch2b_relu)
    bn3b3_branch2c  = self.bn3b3_branch2c(res3b3_branch2c)
    res3b3          = res3b2_relu + bn3b3_branch2c
    res3b3_relu     = F.relu(res3b3)
    res4a_branch1   = self.res4a_branch1(res3b3_relu)
    res4a_branch2a  = self.res4a_branch2a(res3b3_relu)
    bn4a_branch1    = self.bn4a_branch1(res4a_branch1)
    bn4a_branch2a   = self.bn4a_branch2a(res4a_branch2a)
    res4a_branch2a_relu = F.relu(bn4a_branch2a)
    res4a_branch2b_pad = F.pad(res4a_branch2a_relu, (1, 1, 1, 1))
    res4a_branch2b  = self.res4a_branch2b(res4a_branch2b_pad)
    bn4a_branch2b   = self.bn4a_branch2b(res4a_branch2b)
    res4a_branch2b_relu = F.relu(bn4a_branch2b)
    res4a_branch2c  = self.res4a_branch2c(res4a_branch2b_relu)
    bn4a_branch2c   = self.bn4a_branch2c(res4a_branch2c)
    res4a           = bn4a_branch1 + bn4a_branch2c
    res4a_relu      = F.relu(res4a)
    res4b1_branch2a = self.res4b1_branch2a(res4a_relu)
    bn4b1_branch2a  = self.bn4b1_branch2a(res4b1_branch2a)
    res4b1_branch2a_relu = F.relu(bn4b1_branch2a)
    res4b1_branch2b_pad = F.pad(res4b1_branch2a_relu, (1, 1, 1, 1))
    res4b1_branch2b = self.res4b1_branch2b(res4b1_branch2b_pad)
    bn4b1_branch2b  = self.bn4b1_branch2b(res4b1_branch2b)
    res4b1_branch2b_relu = F.relu(bn4b1_branch2b)
    res4b1_branch2c = self.res4b1_branch2c(res4b1_branch2b_relu)
    bn4b1_branch2c  = self.bn4b1_branch2c(res4b1_branch2c)
    res4b1          = res4a_relu + bn4b1_branch2c
    res4b1_relu     = F.relu(res4b1)
    res4b2_branch2a = self.res4b2_branch2a(res4b1_relu)
    bn4b2_branch2a  = self.bn4b2_branch2a(res4b2_branch2a)
    res4b2_branch2a_relu = F.relu(bn4b2_branch2a)
    res4b2_branch2b_pad = F.pad(res4b2_branch2a_relu, (1, 1, 1, 1))
    res4b2_branch2b = self.res4b2_branch2b(res4b2_branch2b_pad)
    bn4b2_branch2b  = self.bn4b2_branch2b(res4b2_branch2b)
    res4b2_branch2b_relu = F.relu(bn4b2_branch2b)
    res4b2_branch2c = self.res4b2_branch2c(res4b2_branch2b_relu)
    bn4b2_branch2c  = self.bn4b2_branch2c(res4b2_branch2c)
    res4b2          = res4b1_relu + bn4b2_branch2c
    res4b2_relu     = F.relu(res4b2)
    res4b3_branch2a = self.res4b3_branch2a(res4b2_relu)
    bn4b3_branch2a  = self.bn4b3_branch2a(res4b3_branch2a)
    res4b3_branch2a_relu = F.relu(bn4b3_branch2a)
    res4b3_branch2b_pad = F.pad(res4b3_branch2a_relu, (1, 1, 1, 1))
    res4b3_branch2b = self.res4b3_branch2b(res4b3_branch2b_pad)
    bn4b3_branch2b  = self.bn4b3_branch2b(res4b3_branch2b)
    res4b3_branch2b_relu = F.relu(bn4b3_branch2b)
    res4b3_branch2c = self.res4b3_branch2c(res4b3_branch2b_relu)
    bn4b3_branch2c  = self.bn4b3_branch2c(res4b3_branch2c)
    res4b3          = res4b2_relu + bn4b3_branch2c
    res4b3_relu     = F.relu(res4b3)
    res4b4_branch2a = self.res4b4_branch2a(res4b3_relu)
    bn4b4_branch2a  = self.bn4b4_branch2a(res4b4_branch2a)
    res4b4_branch2a_relu = F.relu(bn4b4_branch2a)
    res4b4_branch2b_pad = F.pad(res4b4_branch2a_relu, (1, 1, 1, 1))
    res4b4_branch2b = self.res4b4_branch2b(res4b4_branch2b_pad)
    bn4b4_branch2b  = self.bn4b4_branch2b(res4b4_branch2b)
    res4b4_branch2b_relu = F.relu(bn4b4_branch2b)
    res4b4_branch2c = self.res4b4_branch2c(res4b4_branch2b_relu)
    bn4b4_branch2c  = self.bn4b4_branch2c(res4b4_branch2c)
    res4b4          = res4b3_relu + bn4b4_branch2c
    res4b4_relu     = F.relu(res4b4)
    res4b5_branch2a = self.res4b5_branch2a(res4b4_relu)
    bn4b5_branch2a  = self.bn4b5_branch2a(res4b5_branch2a)
    res4b5_branch2a_relu = F.relu(bn4b5_branch2a)
    res4b5_branch2b_pad = F.pad(res4b5_branch2a_relu, (1, 1, 1, 1))
    res4b5_branch2b = self.res4b5_branch2b(res4b5_branch2b_pad)
    bn4b5_branch2b  = self.bn4b5_branch2b(res4b5_branch2b)
    res4b5_branch2b_relu = F.relu(bn4b5_branch2b)
    res4b5_branch2c = self.res4b5_branch2c(res4b5_branch2b_relu)
    bn4b5_branch2c  = self.bn4b5_branch2c(res4b5_branch2c)
    res4b5          = res4b4_relu + bn4b5_branch2c
    res4b5_relu     = F.relu(res4b5)
    res4b6_branch2a = self.res4b6_branch2a(res4b5_relu)
    bn4b6_branch2a  = self.bn4b6_branch2a(res4b6_branch2a)
    res4b6_branch2a_relu = F.relu(bn4b6_branch2a)
    res4b6_branch2b_pad = F.pad(res4b6_branch2a_relu, (1, 1, 1, 1))
    res4b6_branch2b = self.res4b6_branch2b(res4b6_branch2b_pad)
    bn4b6_branch2b  = self.bn4b6_branch2b(res4b6_branch2b)
    res4b6_branch2b_relu = F.relu(bn4b6_branch2b)
    res4b6_branch2c = self.res4b6_branch2c(res4b6_branch2b_relu)
    bn4b6_branch2c  = self.bn4b6_branch2c(res4b6_branch2c)
    res4b6          = res4b5_relu + bn4b6_branch2c
    res4b6_relu     = F.relu(res4b6)
    res4b7_branch2a = self.res4b7_branch2a(res4b6_relu)
    bn4b7_branch2a  = self.bn4b7_branch2a(res4b7_branch2a)
    res4b7_branch2a_relu = F.relu(bn4b7_branch2a)
    res4b7_branch2b_pad = F.pad(res4b7_branch2a_relu, (1, 1, 1, 1))
    res4b7_branch2b = self.res4b7_branch2b(res4b7_branch2b_pad)
    bn4b7_branch2b  = self.bn4b7_branch2b(res4b7_branch2b)
    res4b7_branch2b_relu = F.relu(bn4b7_branch2b)
    res4b7_branch2c = self.res4b7_branch2c(res4b7_branch2b_relu)
    bn4b7_branch2c  = self.bn4b7_branch2c(res4b7_branch2c)
    res4b7          = res4b6_relu + bn4b7_branch2c
    res4b7_relu     = F.relu(res4b7)
    res4b8_branch2a = self.res4b8_branch2a(res4b7_relu)
    bn4b8_branch2a  = self.bn4b8_branch2a(res4b8_branch2a)
    res4b8_branch2a_relu = F.relu(bn4b8_branch2a)
    res4b8_branch2b_pad = F.pad(res4b8_branch2a_relu, (1, 1, 1, 1))
    res4b8_branch2b = self.res4b8_branch2b(res4b8_branch2b_pad)
    bn4b8_branch2b  = self.bn4b8_branch2b(res4b8_branch2b)
    res4b8_branch2b_relu = F.relu(bn4b8_branch2b)
    res4b8_branch2c = self.res4b8_branch2c(res4b8_branch2b_relu)
    bn4b8_branch2c  = self.bn4b8_branch2c(res4b8_branch2c)
    res4b8          = res4b7_relu + bn4b8_branch2c
    res4b8_relu     = F.relu(res4b8)
    res4b9_branch2a = self.res4b9_branch2a(res4b8_relu)
    bn4b9_branch2a  = self.bn4b9_branch2a(res4b9_branch2a)
    res4b9_branch2a_relu = F.relu(bn4b9_branch2a)
    res4b9_branch2b_pad = F.pad(res4b9_branch2a_relu, (1, 1, 1, 1))
    res4b9_branch2b = self.res4b9_branch2b(res4b9_branch2b_pad)
    bn4b9_branch2b  = self.bn4b9_branch2b(res4b9_branch2b)
    res4b9_branch2b_relu = F.relu(bn4b9_branch2b)
    res4b9_branch2c = self.res4b9_branch2c(res4b9_branch2b_relu)
    bn4b9_branch2c  = self.bn4b9_branch2c(res4b9_branch2c)
    res4b9          = res4b8_relu + bn4b9_branch2c
    res4b9_relu     = F.relu(res4b9)
    res4b10_branch2a = self.res4b10_branch2a(res4b9_relu)
    bn4b10_branch2a = self.bn4b10_branch2a(res4b10_branch2a)
    res4b10_branch2a_relu = F.relu(bn4b10_branch2a)
    res4b10_branch2b_pad = F.pad(res4b10_branch2a_relu, (1, 1, 1, 1))
    res4b10_branch2b = self.res4b10_branch2b(res4b10_branch2b_pad)
    bn4b10_branch2b = self.bn4b10_branch2b(res4b10_branch2b)
    res4b10_branch2b_relu = F.relu(bn4b10_branch2b)
    res4b10_branch2c = self.res4b10_branch2c(res4b10_branch2b_relu)
    bn4b10_branch2c = self.bn4b10_branch2c(res4b10_branch2c)
    res4b10         = res4b9_relu + bn4b10_branch2c
    res4b10_relu    = F.relu(res4b10)
    res4b11_branch2a = self.res4b11_branch2a(res4b10_relu)
    bn4b11_branch2a = self.bn4b11_branch2a(res4b11_branch2a)
    res4b11_branch2a_relu = F.relu(bn4b11_branch2a)
    res4b11_branch2b_pad = F.pad(res4b11_branch2a_relu, (1, 1, 1, 1))
    res4b11_branch2b = self.res4b11_branch2b(res4b11_branch2b_pad)
    bn4b11_branch2b = self.bn4b11_branch2b(res4b11_branch2b)
    res4b11_branch2b_relu = F.relu(bn4b11_branch2b)
    res4b11_branch2c = self.res4b11_branch2c(res4b11_branch2b_relu)
    bn4b11_branch2c = self.bn4b11_branch2c(res4b11_branch2c)
    res4b11         = res4b10_relu + bn4b11_branch2c
    res4b11_relu    = F.relu(res4b11)
    res4b12_branch2a = self.res4b12_branch2a(res4b11_relu)
    bn4b12_branch2a = self.bn4b12_branch2a(res4b12_branch2a)
    res4b12_branch2a_relu = F.relu(bn4b12_branch2a)
    res4b12_branch2b_pad = F.pad(res4b12_branch2a_relu, (1, 1, 1, 1))
    res4b12_branch2b = self.res4b12_branch2b(res4b12_branch2b_pad)
    bn4b12_branch2b = self.bn4b12_branch2b(res4b12_branch2b)
    res4b12_branch2b_relu = F.relu(bn4b12_branch2b)
    res4b12_branch2c = self.res4b12_branch2c(res4b12_branch2b_relu)
    bn4b12_branch2c = self.bn4b12_branch2c(res4b12_branch2c)
    res4b12         = res4b11_relu + bn4b12_branch2c
    res4b12_relu    = F.relu(res4b12)
    res4b13_branch2a = self.res4b13_branch2a(res4b12_relu)
    bn4b13_branch2a = self.bn4b13_branch2a(res4b13_branch2a)
    res4b13_branch2a_relu = F.relu(bn4b13_branch2a)
    res4b13_branch2b_pad = F.pad(res4b13_branch2a_relu, (1, 1, 1, 1))
    res4b13_branch2b = self.res4b13_branch2b(res4b13_branch2b_pad)
    bn4b13_branch2b = self.bn4b13_branch2b(res4b13_branch2b)
    res4b13_branch2b_relu = F.relu(bn4b13_branch2b)
    res4b13_branch2c = self.res4b13_branch2c(res4b13_branch2b_relu)
    bn4b13_branch2c = self.bn4b13_branch2c(res4b13_branch2c)
    res4b13         = res4b12_relu + bn4b13_branch2c
    res4b13_relu    = F.relu(res4b13)
    res4b14_branch2a = self.res4b14_branch2a(res4b13_relu)
    bn4b14_branch2a = self.bn4b14_branch2a(res4b14_branch2a)
    res4b14_branch2a_relu = F.relu(bn4b14_branch2a)
    res4b14_branch2b_pad = F.pad(res4b14_branch2a_relu, (1, 1, 1, 1))
    res4b14_branch2b = self.res4b14_branch2b(res4b14_branch2b_pad)
    bn4b14_branch2b = self.bn4b14_branch2b(res4b14_branch2b)
    res4b14_branch2b_relu = F.relu(bn4b14_branch2b)
    res4b14_branch2c = self.res4b14_branch2c(res4b14_branch2b_relu)
    bn4b14_branch2c = self.bn4b14_branch2c(res4b14_branch2c)
    res4b14         = res4b13_relu + bn4b14_branch2c
    res4b14_relu    = F.relu(res4b14)
    res4b15_branch2a = self.res4b15_branch2a(res4b14_relu)
    bn4b15_branch2a = self.bn4b15_branch2a(res4b15_branch2a)
    res4b15_branch2a_relu = F.relu(bn4b15_branch2a)
    res4b15_branch2b_pad = F.pad(res4b15_branch2a_relu, (1, 1, 1, 1))
    res4b15_branch2b = self.res4b15_branch2b(res4b15_branch2b_pad)
    bn4b15_branch2b = self.bn4b15_branch2b(res4b15_branch2b)
    res4b15_branch2b_relu = F.relu(bn4b15_branch2b)
    res4b15_branch2c = self.res4b15_branch2c(res4b15_branch2b_relu)
    bn4b15_branch2c = self.bn4b15_branch2c(res4b15_branch2c)
    res4b15         = res4b14_relu + bn4b15_branch2c
    res4b15_relu    = F.relu(res4b15)
    res4b16_branch2a = self.res4b16_branch2a(res4b15_relu)
    bn4b16_branch2a = self.bn4b16_branch2a(res4b16_branch2a)
    res4b16_branch2a_relu = F.relu(bn4b16_branch2a)
    res4b16_branch2b_pad = F.pad(res4b16_branch2a_relu, (1, 1, 1, 1))
    res4b16_branch2b = self.res4b16_branch2b(res4b16_branch2b_pad)
    bn4b16_branch2b = self.bn4b16_branch2b(res4b16_branch2b)
    res4b16_branch2b_relu = F.relu(bn4b16_branch2b)
    res4b16_branch2c = self.res4b16_branch2c(res4b16_branch2b_relu)
    bn4b16_branch2c = self.bn4b16_branch2c(res4b16_branch2c)
    res4b16         = res4b15_relu + bn4b16_branch2c
    res4b16_relu    = F.relu(res4b16)
    res4b17_branch2a = self.res4b17_branch2a(res4b16_relu)
    bn4b17_branch2a = self.bn4b17_branch2a(res4b17_branch2a)
    res4b17_branch2a_relu = F.relu(bn4b17_branch2a)
    res4b17_branch2b_pad = F.pad(res4b17_branch2a_relu, (1, 1, 1, 1))
    res4b17_branch2b = self.res4b17_branch2b(res4b17_branch2b_pad)
    bn4b17_branch2b = self.bn4b17_branch2b(res4b17_branch2b)
    res4b17_branch2b_relu = F.relu(bn4b17_branch2b)
    res4b17_branch2c = self.res4b17_branch2c(res4b17_branch2b_relu)
    bn4b17_branch2c = self.bn4b17_branch2c(res4b17_branch2c)
    res4b17         = res4b16_relu + bn4b17_branch2c
    res4b17_relu    = F.relu(res4b17)
    res4b18_branch2a = self.res4b18_branch2a(res4b17_relu)
    bn4b18_branch2a = self.bn4b18_branch2a(res4b18_branch2a)
    res4b18_branch2a_relu = F.relu(bn4b18_branch2a)
    res4b18_branch2b_pad = F.pad(res4b18_branch2a_relu, (1, 1, 1, 1))
    res4b18_branch2b = self.res4b18_branch2b(res4b18_branch2b_pad)
    bn4b18_branch2b = self.bn4b18_branch2b(res4b18_branch2b)
    res4b18_branch2b_relu = F.relu(bn4b18_branch2b)
    res4b18_branch2c = self.res4b18_branch2c(res4b18_branch2b_relu)
    bn4b18_branch2c = self.bn4b18_branch2c(res4b18_branch2c)
    res4b18         = res4b17_relu + bn4b18_branch2c
    res4b18_relu    = F.relu(res4b18)
    res4b19_branch2a = self.res4b19_branch2a(res4b18_relu)
    bn4b19_branch2a = self.bn4b19_branch2a(res4b19_branch2a)
    res4b19_branch2a_relu = F.relu(bn4b19_branch2a)
    res4b19_branch2b_pad = F.pad(res4b19_branch2a_relu, (1, 1, 1, 1))
    res4b19_branch2b = self.res4b19_branch2b(res4b19_branch2b_pad)
    bn4b19_branch2b = self.bn4b19_branch2b(res4b19_branch2b)
    res4b19_branch2b_relu = F.relu(bn4b19_branch2b)
    res4b19_branch2c = self.res4b19_branch2c(res4b19_branch2b_relu)
    bn4b19_branch2c = self.bn4b19_branch2c(res4b19_branch2c)
    res4b19         = res4b18_relu + bn4b19_branch2c
    res4b19_relu    = F.relu(res4b19)
    res4b20_branch2a = self.res4b20_branch2a(res4b19_relu)
    bn4b20_branch2a = self.bn4b20_branch2a(res4b20_branch2a)
    res4b20_branch2a_relu = F.relu(bn4b20_branch2a)
    res4b20_branch2b_pad = F.pad(res4b20_branch2a_relu, (1, 1, 1, 1))
    res4b20_branch2b = self.res4b20_branch2b(res4b20_branch2b_pad)
    bn4b20_branch2b = self.bn4b20_branch2b(res4b20_branch2b)
    res4b20_branch2b_relu = F.relu(bn4b20_branch2b)
    res4b20_branch2c = self.res4b20_branch2c(res4b20_branch2b_relu)
    bn4b20_branch2c = self.bn4b20_branch2c(res4b20_branch2c)
    res4b20         = res4b19_relu + bn4b20_branch2c
    res4b20_relu    = F.relu(res4b20)
    res4b21_branch2a = self.res4b21_branch2a(res4b20_relu)
    bn4b21_branch2a = self.bn4b21_branch2a(res4b21_branch2a)
    res4b21_branch2a_relu = F.relu(bn4b21_branch2a)
    res4b21_branch2b_pad = F.pad(res4b21_branch2a_relu, (1, 1, 1, 1))
    res4b21_branch2b = self.res4b21_branch2b(res4b21_branch2b_pad)
    bn4b21_branch2b = self.bn4b21_branch2b(res4b21_branch2b)
    res4b21_branch2b_relu = F.relu(bn4b21_branch2b)
    res4b21_branch2c = self.res4b21_branch2c(res4b21_branch2b_relu)
    bn4b21_branch2c = self.bn4b21_branch2c(res4b21_branch2c)
    res4b21         = res4b20_relu + bn4b21_branch2c
    res4b21_relu    = F.relu(res4b21)
    res4b22_branch2a = self.res4b22_branch2a(res4b21_relu)
    bn4b22_branch2a = self.bn4b22_branch2a(res4b22_branch2a)
    res4b22_branch2a_relu = F.relu(bn4b22_branch2a)
    res4b22_branch2b_pad = F.pad(res4b22_branch2a_relu, (1, 1, 1, 1))
    res4b22_branch2b = self.res4b22_branch2b(res4b22_branch2b_pad)
    bn4b22_branch2b = self.bn4b22_branch2b(res4b22_branch2b)
    res4b22_branch2b_relu = F.relu(bn4b22_branch2b)
    res4b22_branch2c = self.res4b22_branch2c(res4b22_branch2b_relu)
    bn4b22_branch2c = self.bn4b22_branch2c(res4b22_branch2c)
    res4b22         = res4b21_relu + bn4b22_branch2c
    res4b22_relu    = F.relu(res4b22)
    return res4b22_relu</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pycollector.model.face.faster_rcnn.FasterRCNN"><code class="flex name class">
<span>class <span class="ident">FasterRCNN</span></span>
<span>(</span><span>device)</span>
</code></dt>
<dd>
<div class="desc"><p>PyTorch-1.3 model conversion of ResNet-101_faster_rcnn_ohem_iter_20000.caffemodel, leveraging MMDNN conversion tools</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/pycollector/model/face/faster_rcnn.py#L778-L1028" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class FasterRCNN(nn.Module):
    &#34;&#34;&#34;PyTorch-1.3 model conversion of ResNet-101_faster_rcnn_ohem_iter_20000.caffemodel, leveraging MMDNN conversion tools&#34;&#34;&#34;
    def __init__(self, device):
        super(FasterRCNN, self).__init__()

        self.device = device
        torch.device(device)

        self.top = TopLayers()
        self.top.eval()
        self.top = self.top.to(device)

        self.bottom = BottomLayers()
        self.bottom.eval()
        self.bottom = self.bottom.to(device)

        self.rpn = RpnLayers()
        self.rpn.eval()
        self.rpn = self.rpn.to(device)

        # Proposal layer:  manually imported from janus/src/rpn
        self._feat_stride = 16
        #self._anchors = rpn.generate_anchors.generate_anchors(scales=np.array( (8,16,32) ))    
        self._anchors = np.array([[ -84.,  -40.,   99.,   55.],
                                  [-176.,  -88.,  191.,  103.],
                                  [-360., -184.,  375.,  199.],
                                  [ -56.,  -56.,   71.,   71.],
                                  [-120., -120.,  135.,  135.],
                                  [-248., -248.,  263.,  263.],
                                  [ -36.,  -80.,   51.,   95.],
                                  [ -80., -168.,   95.,  183.],
                                  [-168., -344.,  183.,  359.]])
        self._num_anchors = self._anchors.shape[0]
        
    def __call__(self, im, im_info):
        # im is a tensor, N x 3 x H x W; im_info is another,
        # N x 3 (H, W, scale)
        with torch.no_grad():
            im = im.to(self.device)
            
            res4b22 = self.bottom(im)
            
            (rpn_cls_score, rpn_bbox_pred) = self.rpn(res4b22)
            
            (N,C,W,H) = rpn_cls_score.shape
            rpn_cls_score_reshape = torch.reshape(rpn_cls_score, (N, 2, -1, H))
            del rpn_cls_score
            rpn_cls_prob = torch.nn.functional.softmax(rpn_cls_score_reshape, dim=1)  # FIXME: is this dim right?
            rpn_cls_prob_reshape = torch.reshape(rpn_cls_prob, (N, 18, -1, H))
            del rpn_cls_prob
            
            # TODO: Make this handle multiple images, instead of horrible flaming death.
            rois = self._proposal_layer(rpn_cls_prob_reshape.cpu(), rpn_bbox_pred.cpu(), im_info)
            del rpn_bbox_pred
            # import pdb; pdb.set_trace()
            rois_gpu = rois.to(self.device)
            roi_pool5 = torchvision.ops.roi_pool(res4b22, rois_gpu, (14,14), 0.0625)
            del res4b22
            del rois_gpu
            (bbox_pred_1, cls_prob, cls_score) = self.top(roi_pool5)
            rois_cpu = rois.cpu()
            del rois
            bbox_pred_1_cpu = bbox_pred_1.cpu()
            del bbox_pred_1
            cls_prob_cpu = cls_prob.cpu()
            del cls_prob
            cls_score_cpu = cls_score.cpu()
            del cls_score
            del im
        return (rois_cpu, bbox_pred_1_cpu, cls_prob_cpu, cls_score_cpu)

    def _proposal_layer(self, rpn_cls_prob_reshape, rpn_bbox_pred, im_info):
        &#34;&#34;&#34;rpn.proposal_layer&#34;&#34;&#34;
        # &#39;Only single item batches are supported&#39;
        assert(rpn_cls_prob_reshape.shape[0] == 1) 

        # TODO: Defaults from caffe: make me configurable 
        #   {&#39;PROPOSAL_METHOD&#39;: &#39;selective_search&#39;, &#39;SVM&#39;: False, &#39;NMS&#39;: 0.3, &#39;RPN_NMS_THRESH&#39;: 0.7, &#39;SCALES&#39;: [800], 
        #   &#39;RPN_POST_NMS_TOP_N&#39;: 300, &#39;HAS_RPN&#39;: False, &#39;RPN_PRE_NMS_TOP_N&#39;: 6000, &#39;BBOX_REG&#39;: True, &#39;RPN_MIN_SIZE&#39;: 3, &#39;MAX_SIZE&#39;: 1333}
        cfg_key = &#39;TEST&#39;      # either &#39;TRAIN&#39; or &#39;TEST&#39;
        pre_nms_topN = 6000   # RPN_PRE_NMS_TOP_N
        post_nms_topN = 300   # RPN_POST_NMS_TOP_N
        nms_thresh= 0.7       # RPN_NMS_THRESH
        min_size = 3          # RPN_MIN_SIZE                

        # the first set of _num_anchors channels are bg probs
        # the second set are the fg probs, which we want
        scores = rpn_cls_prob_reshape.detach().numpy()[:, self._num_anchors:, :, :]
        bbox_deltas = rpn_bbox_pred.detach().numpy()
        (im_height, im_width, im_scale) = im_info[0]  # H, W, scale

        # 1. Generate proposals from bbox deltas and shifted anchors
        height, width = scores.shape[-2:]

        # Enumerate all shifts
        shift_x = np.arange(0, width) * self._feat_stride
        shift_y = np.arange(0, height) * self._feat_stride
        shift_x, shift_y = np.meshgrid(shift_x, shift_y)
        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),
                            shift_x.ravel(), shift_y.ravel())).transpose()

        # Enumerate all shifted anchors:
        #
        # add A anchors (1, A, 4) to
        # cell K shifts (K, 1, 4) to get
        # shift anchors (K, A, 4)
        # reshape to (K*A, 4) shifted anchors
        A = self._num_anchors
        K = shifts.shape[0]
        anchors = self._anchors.reshape((1, A, 4)) + \
                  shifts.reshape((1, K, 4)).transpose((1, 0, 2))
        anchors = anchors.reshape((K * A, 4))

        # Transpose and reshape predicted bbox transformations to get them
        # into the same order as the anchors:
        #
        # bbox deltas will be (1, 4 * A, H, W) format
        # transpose to (1, H, W, 4 * A)
        # reshape to (1 * H * W * A, 4) where rows are ordered by (h, w, a)
        # in slowest to fastest order
        bbox_deltas = bbox_deltas.transpose((0, 2, 3, 1)).reshape((-1, 4))

        # Same story for the scores:
        #
        # scores are (1, A, H, W) format
        # transpose to (1, H, W, A)
        # reshape to (1 * H * W * A, 1) where rows are ordered by (h, w, a)
        scores = scores.transpose((0, 2, 3, 1)).reshape((-1, 1))

        # Convert anchors into proposals via bbox transformations
        proposals = self._bbox_transform_inv(anchors, bbox_deltas)

        # 2. clip predicted boxes to image
        proposals = self._clip_boxes(proposals, (im_height.item(), im_width.item()))

        # 3. remove predicted boxes with either height or width &lt; threshold
        # (NOTE: convert min_size to input image scale stored in im_info[2])
        keep = self._filter_boxes(proposals, min_size * im_scale.item())
        proposals = proposals[keep, :]
        scores = scores[keep]

        # 4. sort all (proposal, score) pairs by score from highest to lowest
        # 5. take top pre_nms_topN (e.g. 6000)
        order = scores.ravel().argsort()[::-1]
        if pre_nms_topN &gt; 0:
            order = order[:pre_nms_topN]
        proposals = proposals[order, :]
        scores = scores[order]

        # 6. apply nms (e.g. threshold = 0.7)
        # 7. take after_nms_topN (e.g. 300)
        # 8. return the top proposals (-&gt; RoIs top)
        keep = self._nms(np.hstack((proposals, scores)), nms_thresh)
        if post_nms_topN &gt; 0:
            keep = keep[:post_nms_topN]
        proposals = proposals[keep, :]
        scores = scores[keep]

        # Output rois blob
        # Our RPN implementation only supports a single input image, so all
        # batch inds are 0
        batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)
        blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))
        return torch.tensor(blob)


    def _bbox_transform_inv(self, boxes, deltas):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/fast_rcnn.bbox_transform.bbox_transform_inv&#34;&#34;&#34;
        if boxes.shape[0] == 0:
            return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)

        boxes = boxes.astype(deltas.dtype, copy=False)

        widths = boxes[:, 2] - boxes[:, 0] + 1.0
        heights = boxes[:, 3] - boxes[:, 1] + 1.0
        ctr_x = boxes[:, 0] + 0.5 * widths
        ctr_y = boxes[:, 1] + 0.5 * heights
        
        dx = deltas[:, 0::4]
        dy = deltas[:, 1::4]
        dw = deltas[:, 2::4]
        dh = deltas[:, 3::4]
        
        pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]
        pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]
        pred_w = np.exp(dw) * widths[:, np.newaxis]
        pred_h = np.exp(dh) * heights[:, np.newaxis]

        pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)
        # x1
        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w
        # y1
        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h
        # x2
        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w
        # y2
        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h
        
        return pred_boxes

    def _clip_boxes(self, boxes, im_shape):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/fast_rcnn.bbox_transform.clip_boxes&#34;&#34;&#34;
        # x1 &gt;= 0
        boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)
        # y1 &gt;= 0
        boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)
        # x2 &lt; im_shape[1]
        boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)
        # y2 &lt; im_shape[0]
        boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)
        return boxes


    def _filter_boxes(self, boxes, min_size):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/rpn.proposal_layer._filter_boxes&#34;&#34;&#34;
        ws = boxes[:, 2] - boxes[:, 0] + 1
        hs = boxes[:, 3] - boxes[:, 1] + 1
        keep = np.where((ws &gt;= min_size) &amp; (hs &gt;= min_size))[0]
        return keep


    def _nms(self, dets, thresh):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/nms/py_cpu_nms.py&#34;&#34;&#34;
        &#34;&#34;&#34;FIXME: GPU acceleration needed?&#34;&#34;&#34;
        x1 = dets[:, 0]
        y1 = dets[:, 1]
        x2 = dets[:, 2]
        y2 = dets[:, 3]
        scores = dets[:, 4]
        
        areas = (x2 - x1 + 1) * (y2 - y1 + 1)
        order = scores.argsort()[::-1]
        
        keep = []
        while order.size &gt; 0:
            i = order[0]
            keep.append(i)
            xx1 = np.maximum(x1[i], x1[order[1:]])
            yy1 = np.maximum(y1[i], y1[order[1:]])
            xx2 = np.minimum(x2[i], x2[order[1:]])
            yy2 = np.minimum(y2[i], y2[order[1:]])
            
            w = np.maximum(0.0, xx2 - xx1 + 1)
            h = np.maximum(0.0, yy2 - yy1 + 1)
            inter = w * h
            ovr = inter / (areas[i] + areas[order[1:]] - inter)

            inds = np.where(ovr &lt;= thresh)[0]
            order = order[inds + 1]
        
        return keep</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pycollector.model.face.faster_rcnn.FasterRCNN.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pycollector.model.face.faster_rcnn.FasterRCNN.training"><code class="name">var <span class="ident">training</span> :bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pycollector.model.face.faster_rcnn.FasterRCNN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *input:Any) >NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/torch/nn/modules/module.py#L190-L201" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def _forward_unimplemented(self, *input: Any) -&gt; None:
    r&#34;&#34;&#34;Defines the computation performed at every call.

    Should be overridden by all subclasses.

    .. note::
        Although the recipe for forward pass needs to be defined within
        this function, one should call the :class:`Module` instance afterwards
        instead of this since the former takes care of running the
        registered hooks while the latter silently ignores them.
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN"><code class="flex name class">
<span>class <span class="ident">FasterRCNN_MMDNN</span></span>
<span>(</span><span>model_dir, device)</span>
</code></dt>
<dd>
<div class="desc"><p>PyTorch-1.3 model conversion of ResNet-101_faster_rcnn_ohem_iter_20000.caffemodel, leveraging MMDNN conversion tools</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/pycollector/model/face/faster_rcnn.py#L1033-L1290" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class FasterRCNN_MMDNN(nn.Module):
    &#34;&#34;&#34;PyTorch-1.3 model conversion of ResNet-101_faster_rcnn_ohem_iter_20000.caffemodel, leveraging MMDNN conversion tools&#34;&#34;&#34;
    def __init__(self, model_dir, device):
        super(FasterRCNN_MMDNN, self).__init__()

        self.device = device
        torch.device(device)
        # Converted using convert_caffe_to_pytorch.convert_top()
        dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), &#39;..&#39;, &#39;models&#39;, &#39;detection&#39;)
        MainModel = imp.load_source(&#39;MainModel&#39;, os.path.join(dir, &#34;top_layers.py&#34;))
        self.top = torch.load(os.path.join(model_dir, &#34;top_layers.pth&#34;), map_location=device)
        self.top.eval()

        # Converted using convert_caffe_to_pytorch.convert_bottom()
        MainModel = imp.load_source(&#39;MainModel&#39;, os.path.join(dir, &#34;bottom_layers.py&#34;))
        self.bottom = torch.load(os.path.join(model_dir, &#34;bottom_layers.pth&#34;), map_location=device)
        self.bottom.eval()
        self.bottom = self.bottom.to(device)

        # Converted using convert_caffe_to_pytorch.convert_rpn()
        MainModel = imp.load_source(&#39;MainModel&#39;, os.path.join(dir, &#34;rpn_layers.py&#34;))
        self.rpn = torch.load(os.path.join(model_dir, &#34;rpn_layers.pth&#34;), map_location=device)
        self.rpn.eval()
        self.rpn = self.rpn.to(device)


        # Proposal layer:  manually imported from janus/src/rpn
        self._feat_stride = 16
        #self._anchors = rpn.generate_anchors.generate_anchors(scales=np.array( (8,16,32) ))    
        self._anchors = np.array([[ -84.,  -40.,   99.,   55.],
                                  [-176.,  -88.,  191.,  103.],
                                  [-360., -184.,  375.,  199.],
                                  [ -56.,  -56.,   71.,   71.],
                                  [-120., -120.,  135.,  135.],
                                  [-248., -248.,  263.,  263.],
                                  [ -36.,  -80.,   51.,   95.],
                                  [ -80., -168.,   95.,  183.],
                                  [-168., -344.,  183.,  359.]])
        self._num_anchors = self._anchors.shape[0]
        
    def __call__(self, im, im_info):
        # im is a tensor, N x 3 x H x W; im_info is another,
        # N x 3 (H, W, scale)
        # import pdb; pdb.set_trace()
        with torch.no_grad():
            im = im.to(self.device)
            
            res4b22 = self.bottom(im)
            
            (rpn_cls_score, rpn_bbox_pred) = self.rpn(res4b22)
            
            (N,C,W,H) = rpn_cls_score.shape
            rpn_cls_score_reshape = torch.reshape(rpn_cls_score, (N, 2, -1, H))
            del rpn_cls_score
            rpn_cls_prob = torch.nn.functional.softmax(rpn_cls_score_reshape, dim=1)  # FIXME: is this dim right?
            rpn_cls_prob_reshape = torch.reshape(rpn_cls_prob, (N, 18, -1, H))
            del rpn_cls_prob
            
            # TODO: Make this handle multiple images, instead of horrible flaming death.
            rois = self._proposal_layer(rpn_cls_prob_reshape.cpu(), rpn_bbox_pred.cpu(), im_info)
            del rpn_bbox_pred
            # import pdb; pdb.set_trace()
            rois_gpu = rois.to(self.device)
            roi_pool5 = torchvision.ops.roi_pool(res4b22, rois_gpu, (14,14), 0.0625)
            del res4b22
            del rois_gpu
            (bbox_pred_1, cls_prob, cls_score) = self.top(roi_pool5)
            rois_cpu = rois.cpu()
            del rois
            bbox_pred_1_cpu = bbox_pred_1.cpu()
            del bbox_pred_1
            cls_prob_cpu = cls_prob.cpu()
            del cls_prob
            cls_score_cpu = cls_score.cpu()   
            del cls_score
            del im
        return (rois_cpu, bbox_pred_1_cpu, cls_prob_cpu, cls_score_cpu)

    def _proposal_layer(self, rpn_cls_prob_reshape, rpn_bbox_pred, im_info):
        &#34;&#34;&#34;rpn.proposal_layer&#34;&#34;&#34;
        # &#39;Only single item batches are supported&#39;
        assert(rpn_cls_prob_reshape.shape[0] == 1) 

        # TODO: Defaults from caffe: make me configurable 
        #   {&#39;PROPOSAL_METHOD&#39;: &#39;selective_search&#39;, &#39;SVM&#39;: False, &#39;NMS&#39;: 0.3, &#39;RPN_NMS_THRESH&#39;: 0.7, &#39;SCALES&#39;: [800], 
        #   &#39;RPN_POST_NMS_TOP_N&#39;: 300, &#39;HAS_RPN&#39;: False, &#39;RPN_PRE_NMS_TOP_N&#39;: 6000, &#39;BBOX_REG&#39;: True, &#39;RPN_MIN_SIZE&#39;: 3, &#39;MAX_SIZE&#39;: 1333}
        cfg_key = &#39;TEST&#39;      # either &#39;TRAIN&#39; or &#39;TEST&#39;
        pre_nms_topN = 6000   # RPN_PRE_NMS_TOP_N
        post_nms_topN = 300   # RPN_POST_NMS_TOP_N
        nms_thresh= 0.7       # RPN_NMS_THRESH
        min_size = 3          # RPN_MIN_SIZE                

        # the first set of _num_anchors channels are bg probs
        # the second set are the fg probs, which we want
        scores = rpn_cls_prob_reshape.detach().numpy()[:, self._num_anchors:, :, :]
        bbox_deltas = rpn_bbox_pred.detach().numpy()
        (im_height, im_width, im_scale) = im_info[0]  # H, W, scale

        # 1. Generate proposals from bbox deltas and shifted anchors
        height, width = scores.shape[-2:]

        # Enumerate all shifts
        shift_x = np.arange(0, width) * self._feat_stride
        shift_y = np.arange(0, height) * self._feat_stride
        shift_x, shift_y = np.meshgrid(shift_x, shift_y)
        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),
                            shift_x.ravel(), shift_y.ravel())).transpose()

        # Enumerate all shifted anchors:
        #
        # add A anchors (1, A, 4) to
        # cell K shifts (K, 1, 4) to get
        # shift anchors (K, A, 4)
        # reshape to (K*A, 4) shifted anchors
        A = self._num_anchors
        K = shifts.shape[0]
        anchors = self._anchors.reshape((1, A, 4)) + \
                  shifts.reshape((1, K, 4)).transpose((1, 0, 2))
        anchors = anchors.reshape((K * A, 4))

        # Transpose and reshape predicted bbox transformations to get them
        # into the same order as the anchors:
        #
        # bbox deltas will be (1, 4 * A, H, W) format
        # transpose to (1, H, W, 4 * A)
        # reshape to (1 * H * W * A, 4) where rows are ordered by (h, w, a)
        # in slowest to fastest order
        bbox_deltas = bbox_deltas.transpose((0, 2, 3, 1)).reshape((-1, 4))

        # Same story for the scores:
        #
        # scores are (1, A, H, W) format
        # transpose to (1, H, W, A)
        # reshape to (1 * H * W * A, 1) where rows are ordered by (h, w, a)
        scores = scores.transpose((0, 2, 3, 1)).reshape((-1, 1))

        # Convert anchors into proposals via bbox transformations
        proposals = self._bbox_transform_inv(anchors, bbox_deltas)

        # 2. clip predicted boxes to image
        proposals = self._clip_boxes(proposals, (im_height.item(), im_width.item()))

        # 3. remove predicted boxes with either height or width &lt; threshold
        # (NOTE: convert min_size to input image scale stored in im_info[2])
        keep = self._filter_boxes(proposals, min_size * im_scale.item())
        proposals = proposals[keep, :]
        scores = scores[keep]

        # 4. sort all (proposal, score) pairs by score from highest to lowest
        # 5. take top pre_nms_topN (e.g. 6000)
        order = scores.ravel().argsort()[::-1]
        if pre_nms_topN &gt; 0:
            order = order[:pre_nms_topN]
        proposals = proposals[order, :]
        scores = scores[order]

        # 6. apply nms (e.g. threshold = 0.7)
        # 7. take after_nms_topN (e.g. 300)
        # 8. return the top proposals (-&gt; RoIs top)
        keep = self._nms(np.hstack((proposals, scores)), nms_thresh)
        if post_nms_topN &gt; 0:
            keep = keep[:post_nms_topN]
        proposals = proposals[keep, :]
        scores = scores[keep]

        # Output rois blob
        # Our RPN implementation only supports a single input image, so all
        # batch inds are 0
        batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)
        blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))
        return torch.tensor(blob)


    def _bbox_transform_inv(self, boxes, deltas):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/fast_rcnn.bbox_transform.bbox_transform_inv&#34;&#34;&#34;
        if boxes.shape[0] == 0:
            return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)

        boxes = boxes.astype(deltas.dtype, copy=False)

        widths = boxes[:, 2] - boxes[:, 0] + 1.0
        heights = boxes[:, 3] - boxes[:, 1] + 1.0
        ctr_x = boxes[:, 0] + 0.5 * widths
        ctr_y = boxes[:, 1] + 0.5 * heights
        
        dx = deltas[:, 0::4]
        dy = deltas[:, 1::4]
        dw = deltas[:, 2::4]
        dh = deltas[:, 3::4]
        
        pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]
        pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]
        pred_w = np.exp(dw) * widths[:, np.newaxis]
        pred_h = np.exp(dh) * heights[:, np.newaxis]

        pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)
        # x1
        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w
        # y1
        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h
        # x2
        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w
        # y2
        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h
        
        return pred_boxes

    def _clip_boxes(self, boxes, im_shape):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/fast_rcnn.bbox_transform.clip_boxes&#34;&#34;&#34;
        # x1 &gt;= 0
        boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)
        # y1 &gt;= 0
        boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)
        # x2 &lt; im_shape[1]
        boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)
        # y2 &lt; im_shape[0]
        boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)
        return boxes


    def _filter_boxes(self, boxes, min_size):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/rpn.proposal_layer._filter_boxes&#34;&#34;&#34;
        ws = boxes[:, 2] - boxes[:, 0] + 1
        hs = boxes[:, 3] - boxes[:, 1] + 1
        keep = np.where((ws &gt;= min_size) &amp; (hs &gt;= min_size))[0]
        return keep


    def _nms(self, dets, thresh):
        &#34;&#34;&#34;Cloned from janus-tne/src/python/nms/py_cpu_nms.py&#34;&#34;&#34;
        &#34;&#34;&#34;FIXME: GPU acceleration needed?&#34;&#34;&#34;
        x1 = dets[:, 0]
        y1 = dets[:, 1]
        x2 = dets[:, 2]
        y2 = dets[:, 3]
        scores = dets[:, 4]
        
        areas = (x2 - x1 + 1) * (y2 - y1 + 1)
        order = scores.argsort()[::-1]
        
        keep = []
        while order.size &gt; 0:
            i = order[0]
            keep.append(i)
            xx1 = np.maximum(x1[i], x1[order[1:]])
            yy1 = np.maximum(y1[i], y1[order[1:]])
            xx2 = np.minimum(x2[i], x2[order[1:]])
            yy2 = np.minimum(y2[i], y2[order[1:]])
            
            w = np.maximum(0.0, xx2 - xx1 + 1)
            h = np.maximum(0.0, yy2 - yy1 + 1)
            inter = w * h
            ovr = inter / (areas[i] + areas[order[1:]] - inter)

            inds = np.where(ovr &lt;= thresh)[0]
            order = order[inds + 1]
        
        return keep</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.training"><code class="name">var <span class="ident">training</span> :bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *input:Any) >NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/torch/nn/modules/module.py#L190-L201" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def _forward_unimplemented(self, *input: Any) -&gt; None:
    r&#34;&#34;&#34;Defines the computation performed at every call.

    Should be overridden by all subclasses.

    .. note::
        Although the recipe for forward pass needs to be defined within
        this function, one should call the :class:`Module` instance afterwards
        instead of this since the former takes care of running the
        registered hooks while the latter silently ignores them.
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pycollector.model.face.faster_rcnn.RpnLayers"><code class="flex name class">
<span>class <span class="ident">RpnLayers</span></span>
<span>(</span><span>weight_file=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/pycollector/model/face/faster_rcnn.py#L29-L58" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class RpnLayers(nn.Module):
    def __init__(self, weight_file=None):
        super(RpnLayers, self).__init__()
        #global __rpn_layers_weights_dict  # Loaded by MMDNN
        #__rpn_layers_weights_dict = load_weights(weight_file)

        self.rpn_conv_3x3 = self.__conv(2, name=&#39;rpn_conv/3x3&#39;, in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)
        self.rpn_cls_score = self.__conv(2, name=&#39;rpn_cls_score&#39;, in_channels=512, out_channels=18, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)
        self.rpn_bbox_pred = self.__conv(2, name=&#39;rpn_bbox_pred&#39;, in_channels=512, out_channels=36, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)

    def forward(self, x):
        rpn_conv_3x3_pad = F.pad(x, (1, 1, 1, 1))
        rpn_conv_3x3    = self.rpn_conv_3x3(rpn_conv_3x3_pad)
        rpn_relu_3x3    = F.relu(rpn_conv_3x3)
        rpn_cls_score   = self.rpn_cls_score(rpn_relu_3x3)
        rpn_bbox_pred   = self.rpn_bbox_pred(rpn_relu_3x3)
        return rpn_cls_score, rpn_bbox_pred


    @staticmethod
    def __conv(dim, name, **kwargs):
        if   dim == 1:  layer = nn.Conv1d(**kwargs)
        elif dim == 2:  layer = nn.Conv2d(**kwargs)
        elif dim == 3:  layer = nn.Conv3d(**kwargs)
        else:           raise NotImplementedError()

        #layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__rpn_layers_weights_dict[name][&#39;weights&#39;]))
        #if &#39;bias&#39; in __rpn_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__rpn_layers_weights_dict[name][&#39;bias&#39;]))
        return layer</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pycollector.model.face.faster_rcnn.RpnLayers.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pycollector.model.face.faster_rcnn.RpnLayers.training"><code class="name">var <span class="ident">training</span> :bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pycollector.model.face.faster_rcnn.RpnLayers.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) >Callable[...,Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/pycollector/model/face/faster_rcnn.py#L39-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def forward(self, x):
    rpn_conv_3x3_pad = F.pad(x, (1, 1, 1, 1))
    rpn_conv_3x3    = self.rpn_conv_3x3(rpn_conv_3x3_pad)
    rpn_relu_3x3    = F.relu(rpn_conv_3x3)
    rpn_cls_score   = self.rpn_cls_score(rpn_relu_3x3)
    rpn_bbox_pred   = self.rpn_bbox_pred(rpn_relu_3x3)
    return rpn_cls_score, rpn_bbox_pred</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pycollector.model.face.faster_rcnn.TopLayers"><code class="flex name class">
<span>class <span class="ident">TopLayers</span></span>
<span>(</span><span>weight_file=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/pycollector/model/face/faster_rcnn.py#L641-L774" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class TopLayers(nn.Module):
    def __init__(self, weight_file=None):
        super(TopLayers, self).__init__()
        #global __top_layers_weights_dict  # Loaded by MMDNN
        #__top_layers_weights_dict = load_weights(weight_file)

        self.res5a_branch1 = self.__conv(2, name=&#39;res5a_branch1&#39;, in_channels=1024, out_channels=2048, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.res5a_branch2a = self.__conv(2, name=&#39;res5a_branch2a&#39;, in_channels=1024, out_channels=512, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.bn5a_branch1 = self.__batch_normalization(2, &#39;bn5a_branch1&#39;, num_features=2048, eps=9.99999974738e-06, momentum=0.0)
        self.bn5a_branch2a = self.__batch_normalization(2, &#39;bn5a_branch2a&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5a_branch2b = self.__conv(2, name=&#39;res5a_branch2b&#39;, in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn5a_branch2b = self.__batch_normalization(2, &#39;bn5a_branch2b&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5a_branch2c = self.__conv(2, name=&#39;res5a_branch2c&#39;, in_channels=512, out_channels=2048, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn5a_branch2c = self.__batch_normalization(2, &#39;bn5a_branch2c&#39;, num_features=2048, eps=9.99999974738e-06, momentum=0.0)
        self.res5b_branch2a = self.__conv(2, name=&#39;res5b_branch2a&#39;, in_channels=2048, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn5b_branch2a = self.__batch_normalization(2, &#39;bn5b_branch2a&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5b_branch2b = self.__conv(2, name=&#39;res5b_branch2b&#39;, in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn5b_branch2b = self.__batch_normalization(2, &#39;bn5b_branch2b&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5b_branch2c = self.__conv(2, name=&#39;res5b_branch2c&#39;, in_channels=512, out_channels=2048, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn5b_branch2c = self.__batch_normalization(2, &#39;bn5b_branch2c&#39;, num_features=2048, eps=9.99999974738e-06, momentum=0.0)
        self.res5c_branch2a = self.__conv(2, name=&#39;res5c_branch2a&#39;, in_channels=2048, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn5c_branch2a = self.__batch_normalization(2, &#39;bn5c_branch2a&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5c_branch2b = self.__conv(2, name=&#39;res5c_branch2b&#39;, in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.bn5c_branch2b = self.__batch_normalization(2, &#39;bn5c_branch2b&#39;, num_features=512, eps=9.99999974738e-06, momentum=0.0)
        self.res5c_branch2c = self.__conv(2, name=&#39;res5c_branch2c&#39;, in_channels=512, out_channels=2048, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.bn5c_branch2c = self.__batch_normalization(2, &#39;bn5c_branch2c&#39;, num_features=2048, eps=9.99999974738e-06, momentum=0.0)
        self.bbox_pred_1 = self.__dense(name = &#39;bbox_pred_1&#39;, in_features = 2048, out_features = 8, bias = True)
        self.cls_score_1 = self.__dense(name = &#39;cls_score_1&#39;, in_features = 2048, out_features = 2, bias = True)

    def forward(self, x):
        res5a_branch1   = self.res5a_branch1(x)
        res5a_branch2a  = self.res5a_branch2a(x)
        bn5a_branch1    = self.bn5a_branch1(res5a_branch1)
        bn5a_branch2a   = self.bn5a_branch2a(res5a_branch2a)
        res5a_branch2a_relu = F.relu(bn5a_branch2a)

        # Fix MMDNN dilated convolution bug
        #res5a_branch2b_pad = F.pad(res5a_branch2a_relu, (0, 0, 0, 0))
        #res5a_branch2b  = self.res5a_branch2b(res5a_branch2b_pad)
        # Fix broken dilated convolutions on MMDNN conversion, and roll in padding
        res5a_branch2b = F.conv2d(res5a_branch2a_relu, weight=self.res5a_branch2b.weight, bias=self.res5a_branch2b.bias, 
                                  stride=self.res5a_branch2b.stride, padding=(2,2), dilation=2, groups=self.res5a_branch2b.groups)

        bn5a_branch2b   = self.bn5a_branch2b(res5a_branch2b)
        res5a_branch2b_relu = F.relu(bn5a_branch2b)
        res5a_branch2c  = self.res5a_branch2c(res5a_branch2b_relu)
        bn5a_branch2c   = self.bn5a_branch2c(res5a_branch2c)
        res5a           = bn5a_branch1 + bn5a_branch2c
        res5a_relu      = F.relu(res5a)
        res5b_branch2a  = self.res5b_branch2a(res5a_relu)
        bn5b_branch2a   = self.bn5b_branch2a(res5b_branch2a)
        res5b_branch2a_relu = F.relu(bn5b_branch2a)

        # Fix MMDNN dilated convolution bug
        #res5b_branch2b_pad = F.pad(res5b_branch2a_relu, (0, 0, 0, 0))
        #res5b_branch2b  = self.res5b_branch2b(res5b_branch2b_pad)
        res5b_branch2b = F.conv2d(res5b_branch2a_relu, weight=self.res5b_branch2b.weight, bias=self.res5b_branch2b.bias, 
                                  stride=self.res5b_branch2b.stride, padding=(2,2), dilation=2, groups=self.res5b_branch2b.groups)

        bn5b_branch2b   = self.bn5b_branch2b(res5b_branch2b)
        res5b_branch2b_relu = F.relu(bn5b_branch2b)
        res5b_branch2c  = self.res5b_branch2c(res5b_branch2b_relu)
        bn5b_branch2c   = self.bn5b_branch2c(res5b_branch2c)
        res5b           = res5a_relu + bn5b_branch2c
        res5b_relu      = F.relu(res5b)
        res5c_branch2a  = self.res5c_branch2a(res5b_relu)
        bn5c_branch2a   = self.bn5c_branch2a(res5c_branch2a)
        res5c_branch2a_relu = F.relu(bn5c_branch2a)

        # Fix MMDNN dilated convolution bug
        #res5c_branch2b_pad = F.pad(res5c_branch2a_relu, (1, 1, 1, 1))
        #res5c_branch2b  = self.res5c_branch2b(res5c_branch2b_pad)
        res5c_branch2b = F.conv2d(res5c_branch2a_relu, weight=self.res5c_branch2b.weight, bias=self.res5c_branch2b.bias, 
                                  stride=self.res5c_branch2b.stride, padding=(2,2), dilation=2, groups=self.res5c_branch2b.groups)
        bn5c_branch2b   = self.bn5c_branch2b(res5c_branch2b)
        res5c_branch2b_relu = F.relu(bn5c_branch2b)
        res5c_branch2c  = self.res5c_branch2c(res5c_branch2b_relu)
        bn5c_branch2c   = self.bn5c_branch2c(res5c_branch2c)
        res5c           = res5b_relu + bn5c_branch2c
        res5c_relu      = F.relu(res5c)
        pool5           = F.avg_pool2d(res5c_relu, kernel_size=(7, 7), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)
        bbox_pred_0     = pool5.view(pool5.size(0), -1)
        cls_score_0     = pool5.view(pool5.size(0), -1)
        bbox_pred_1     = self.bbox_pred_1(bbox_pred_0)
        cls_score_1     = self.cls_score_1(cls_score_0)
        # import pdb; pdb.set_trace()
        cls_prob        = F.softmax(cls_score_1, dim=1)
        # Returning pre-softmax score to be consistent with Caffe implementation
        return bbox_pred_1, cls_prob, cls_score_1

    @staticmethod
    def __batch_normalization(dim, name, **kwargs):
        if   dim == 0 or dim == 1:  layer = nn.BatchNorm1d(**kwargs)
        elif dim == 2:  layer = nn.BatchNorm2d(**kwargs)
        elif dim == 3:  layer = nn.BatchNorm3d(**kwargs)
        else:           raise NotImplementedError()

        #if &#39;scale&#39; in __top_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;scale&#39;]))
        #else:
        #    layer.weight.data.fill_(1)

        #if &#39;bias&#39; in __top_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;bias&#39;]))
        #else:
        #    layer.bias.data.fill_(0)

        #layer.state_dict()[&#39;running_mean&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;mean&#39;]))
        #layer.state_dict()[&#39;running_var&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;var&#39;]))
        return layer

    @staticmethod
    def __conv(dim, name, **kwargs):
        if   dim == 1:  
            layer = nn.Conv1d(**kwargs)
        elif dim == 2:  
            layer = nn.Conv2d(**kwargs)
        elif dim == 3:  
            layer = nn.Conv3d(**kwargs)
        else:           
            raise NotImplementedError()

        #layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;weights&#39;]))
        #if &#39;bias&#39; in __top_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;bias&#39;]))
        return layer

    @staticmethod
    def __dense(name, **kwargs):
        layer = nn.Linear(**kwargs)
        #layer.state_dict()[&#39;weight&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;weights&#39;]))
        #if &#39;bias&#39; in __top_layers_weights_dict[name]:
        #    layer.state_dict()[&#39;bias&#39;].copy_(torch.from_numpy(__top_layers_weights_dict[name][&#39;bias&#39;]))
        return layer</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="pycollector.model.face.faster_rcnn.TopLayers.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="pycollector.model.face.faster_rcnn.TopLayers.training"><code class="name">var <span class="ident">training</span> :bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pycollector.model.face.faster_rcnn.TopLayers.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) >Callable[...,Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/visym/collector/blob/60e2cc017171b77a037dfbee460fee83eb001c8a/pycollector/model/face/faster_rcnn.py#L670-L729" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def forward(self, x):
    res5a_branch1   = self.res5a_branch1(x)
    res5a_branch2a  = self.res5a_branch2a(x)
    bn5a_branch1    = self.bn5a_branch1(res5a_branch1)
    bn5a_branch2a   = self.bn5a_branch2a(res5a_branch2a)
    res5a_branch2a_relu = F.relu(bn5a_branch2a)

    # Fix MMDNN dilated convolution bug
    #res5a_branch2b_pad = F.pad(res5a_branch2a_relu, (0, 0, 0, 0))
    #res5a_branch2b  = self.res5a_branch2b(res5a_branch2b_pad)
    # Fix broken dilated convolutions on MMDNN conversion, and roll in padding
    res5a_branch2b = F.conv2d(res5a_branch2a_relu, weight=self.res5a_branch2b.weight, bias=self.res5a_branch2b.bias, 
                              stride=self.res5a_branch2b.stride, padding=(2,2), dilation=2, groups=self.res5a_branch2b.groups)

    bn5a_branch2b   = self.bn5a_branch2b(res5a_branch2b)
    res5a_branch2b_relu = F.relu(bn5a_branch2b)
    res5a_branch2c  = self.res5a_branch2c(res5a_branch2b_relu)
    bn5a_branch2c   = self.bn5a_branch2c(res5a_branch2c)
    res5a           = bn5a_branch1 + bn5a_branch2c
    res5a_relu      = F.relu(res5a)
    res5b_branch2a  = self.res5b_branch2a(res5a_relu)
    bn5b_branch2a   = self.bn5b_branch2a(res5b_branch2a)
    res5b_branch2a_relu = F.relu(bn5b_branch2a)

    # Fix MMDNN dilated convolution bug
    #res5b_branch2b_pad = F.pad(res5b_branch2a_relu, (0, 0, 0, 0))
    #res5b_branch2b  = self.res5b_branch2b(res5b_branch2b_pad)
    res5b_branch2b = F.conv2d(res5b_branch2a_relu, weight=self.res5b_branch2b.weight, bias=self.res5b_branch2b.bias, 
                              stride=self.res5b_branch2b.stride, padding=(2,2), dilation=2, groups=self.res5b_branch2b.groups)

    bn5b_branch2b   = self.bn5b_branch2b(res5b_branch2b)
    res5b_branch2b_relu = F.relu(bn5b_branch2b)
    res5b_branch2c  = self.res5b_branch2c(res5b_branch2b_relu)
    bn5b_branch2c   = self.bn5b_branch2c(res5b_branch2c)
    res5b           = res5a_relu + bn5b_branch2c
    res5b_relu      = F.relu(res5b)
    res5c_branch2a  = self.res5c_branch2a(res5b_relu)
    bn5c_branch2a   = self.bn5c_branch2a(res5c_branch2a)
    res5c_branch2a_relu = F.relu(bn5c_branch2a)

    # Fix MMDNN dilated convolution bug
    #res5c_branch2b_pad = F.pad(res5c_branch2a_relu, (1, 1, 1, 1))
    #res5c_branch2b  = self.res5c_branch2b(res5c_branch2b_pad)
    res5c_branch2b = F.conv2d(res5c_branch2a_relu, weight=self.res5c_branch2b.weight, bias=self.res5c_branch2b.bias, 
                              stride=self.res5c_branch2b.stride, padding=(2,2), dilation=2, groups=self.res5c_branch2b.groups)
    bn5c_branch2b   = self.bn5c_branch2b(res5c_branch2b)
    res5c_branch2b_relu = F.relu(bn5c_branch2b)
    res5c_branch2c  = self.res5c_branch2c(res5c_branch2b_relu)
    bn5c_branch2c   = self.bn5c_branch2c(res5c_branch2c)
    res5c           = res5b_relu + bn5c_branch2c
    res5c_relu      = F.relu(res5c)
    pool5           = F.avg_pool2d(res5c_relu, kernel_size=(7, 7), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)
    bbox_pred_0     = pool5.view(pool5.size(0), -1)
    cls_score_0     = pool5.view(pool5.size(0), -1)
    bbox_pred_1     = self.bbox_pred_1(bbox_pred_0)
    cls_score_1     = self.cls_score_1(cls_score_0)
    # import pdb; pdb.set_trace()
    cls_prob        = F.softmax(cls_score_1, dim=1)
    # Returning pre-softmax score to be consistent with Caffe implementation
    return bbox_pred_1, cls_prob, cls_score_1</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="Visym Collector" href="https://github.com/visym/collector/">
<img src="https://www.visym.com/labs/images/visym_logo_black_notext.png" alt="" width="150"> <p> </p>
</a>
</header>
<form>
<input id="lunr-search" name="q" placeholder=" Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pycollector.model.face" href="index.html">pycollector.model.face</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pycollector.model.face.faster_rcnn.conversion" href="#pycollector.model.face.faster_rcnn.conversion">conversion</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pycollector.model.face.faster_rcnn.BottomLayers" href="#pycollector.model.face.faster_rcnn.BottomLayers">BottomLayers</a></code></h4>
<ul class="">
<li><code><a title="pycollector.model.face.faster_rcnn.BottomLayers.dump_patches" href="#pycollector.model.face.faster_rcnn.BottomLayers.dump_patches">dump_patches</a></code></li>
<li><code><a title="pycollector.model.face.faster_rcnn.BottomLayers.forward" href="#pycollector.model.face.faster_rcnn.BottomLayers.forward">forward</a></code></li>
<li><code><a title="pycollector.model.face.faster_rcnn.BottomLayers.training" href="#pycollector.model.face.faster_rcnn.BottomLayers.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pycollector.model.face.faster_rcnn.FasterRCNN" href="#pycollector.model.face.faster_rcnn.FasterRCNN">FasterRCNN</a></code></h4>
<ul class="">
<li><code><a title="pycollector.model.face.faster_rcnn.FasterRCNN.dump_patches" href="#pycollector.model.face.faster_rcnn.FasterRCNN.dump_patches">dump_patches</a></code></li>
<li><code><a title="pycollector.model.face.faster_rcnn.FasterRCNN.forward" href="#pycollector.model.face.faster_rcnn.FasterRCNN.forward">forward</a></code></li>
<li><code><a title="pycollector.model.face.faster_rcnn.FasterRCNN.training" href="#pycollector.model.face.faster_rcnn.FasterRCNN.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN" href="#pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN">FasterRCNN_MMDNN</a></code></h4>
<ul class="">
<li><code><a title="pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.dump_patches" href="#pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.dump_patches">dump_patches</a></code></li>
<li><code><a title="pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.forward" href="#pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.forward">forward</a></code></li>
<li><code><a title="pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.training" href="#pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pycollector.model.face.faster_rcnn.RpnLayers" href="#pycollector.model.face.faster_rcnn.RpnLayers">RpnLayers</a></code></h4>
<ul class="">
<li><code><a title="pycollector.model.face.faster_rcnn.RpnLayers.dump_patches" href="#pycollector.model.face.faster_rcnn.RpnLayers.dump_patches">dump_patches</a></code></li>
<li><code><a title="pycollector.model.face.faster_rcnn.RpnLayers.forward" href="#pycollector.model.face.faster_rcnn.RpnLayers.forward">forward</a></code></li>
<li><code><a title="pycollector.model.face.faster_rcnn.RpnLayers.training" href="#pycollector.model.face.faster_rcnn.RpnLayers.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pycollector.model.face.faster_rcnn.TopLayers" href="#pycollector.model.face.faster_rcnn.TopLayers">TopLayers</a></code></h4>
<ul class="">
<li><code><a title="pycollector.model.face.faster_rcnn.TopLayers.dump_patches" href="#pycollector.model.face.faster_rcnn.TopLayers.dump_patches">dump_patches</a></code></li>
<li><code><a title="pycollector.model.face.faster_rcnn.TopLayers.forward" href="#pycollector.model.face.faster_rcnn.TopLayers.forward">forward</a></code></li>
<li><code><a title="pycollector.model.face.faster_rcnn.TopLayers.training" href="#pycollector.model.face.faster_rcnn.TopLayers.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>