URLS=[
"pycollector/index.html",
"pycollector/recognition.html",
"pycollector/user.html",
"pycollector/detection.html",
"pycollector/backend.html",
"pycollector/version.html",
"pycollector/util.html",
"pycollector/globals.html",
"pycollector/dataset.html",
"pycollector/label.html",
"pycollector/model/index.html",
"pycollector/model/pyvideoresearch/index.html",
"pycollector/model/pyvideoresearch/bases/index.html",
"pycollector/model/pyvideoresearch/bases/resnet50_3d.html",
"pycollector/model/pyvideoresearch/bases/resnet101_3d.html",
"pycollector/model/pyvideoresearch/bases/base.html",
"pycollector/model/face/index.html",
"pycollector/model/face/recognition.html",
"pycollector/model/face/detection.html",
"pycollector/model/face/faster_rcnn.html",
"pycollector/model/ResNets_3D_PyTorch/index.html",
"pycollector/model/ResNets_3D_PyTorch/resnet.html",
"pycollector/model/yolov5/index.html",
"pycollector/model/yolov5/utils/index.html",
"pycollector/model/yolov5/utils/metrics.html",
"pycollector/model/yolov5/utils/activations.html",
"pycollector/model/yolov5/utils/loss.html",
"pycollector/model/yolov5/utils/google_utils.html",
"pycollector/model/yolov5/utils/autoanchor.html",
"pycollector/model/yolov5/utils/torch_utils.html",
"pycollector/model/yolov5/utils/general.html",
"pycollector/model/yolov5/models/index.html",
"pycollector/model/yolov5/models/experimental.html",
"pycollector/model/yolov5/models/export.html",
"pycollector/model/yolov5/models/common.html",
"pycollector/model/yolov5/models/yolo.html",
"pycollector/model/yolov3/index.html",
"pycollector/model/yolov3/utils/index.html",
"pycollector/model/yolov3/utils/utils.html",
"pycollector/model/yolov3/utils/parse_config.html",
"pycollector/model/yolov3/network.html",
"pycollector/video.html",
"pycollector/project.html"
];
INDEX=[
{
"ref":"pycollector",
"url":0,
"doc":""
},
{
"ref":"pycollector.recognition",
"url":1,
"doc":""
},
{
"ref":"pycollector.recognition.ActivityRecognition",
"url":1,
"doc":""
},
{
"ref":"pycollector.recognition.ActivityRecognition.class_to_index",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityRecognition.index_to_class",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityRecognition.classlist",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityRecognition.num_classes",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityRecognition.fromindex",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityRecognition.label_confidence",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityRecognition.activity",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityRecognition.top1",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityRecognition.topk",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityRecognition.temporal_support",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityRecognition.totensor",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityRecognition.binary_vector",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.PIP_250k",
"url":1,
"doc":"Activity recognition using people in public - 250k stabilized"
},
{
"ref":"pycollector.recognition.PIP_250k.dump_patches",
"url":1,
"doc":""
},
{
"ref":"pycollector.recognition.PIP_250k.training",
"url":1,
"doc":""
},
{
"ref":"pycollector.recognition.PIP_250k.category",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.PIP_250k.category_confidence",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.PIP_250k.topk",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.PIP_250k.topk_probability",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.PIP_250k.forward",
"url":1,
"doc":"Same as :meth: torch.nn.Module.forward() . Args:  args: Whatever you decide to pass into the forward method.  kwargs: Keyword arguments are also possible. Return: Your model's output",
"func":1
},
{
"ref":"pycollector.recognition.PIP_250k.configure_optimizers",
"url":1,
"doc":"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. -  Single optimizer . -  List or Tuple of optimizers. -  Two lists - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple lr_dict). -  Dictionary , with an  \"optimizer\" key, and (optionally) a  \"lr_scheduler\" key whose value is a single LR scheduler or lr_dict. -  Tuple of dictionaries as described above, with an optional  \"frequency\" key. -  None - Fit will run without any optimizer. Note: The lr_dict is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below.  code-block python lr_dict = { 'scheduler': lr_scheduler,  The LR scheduler instance (required)  The unit of the scheduler's step size, could also be 'step' 'interval': 'epoch', 'frequency': 1,  The frequency of the scheduler 'monitor': 'val_loss',  Metric for  ReduceLROnPlateau to monitor 'strict': True,  Whether to crash the training if  monitor is not found 'name': None,  Custom name for  LearningRateMonitor to use } Only the  \"scheduler\" key is required, the rest will be set to the defaults above. Note: The  frequency value specified in a dict along with the  optimizer key is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: In the former case, all optimizers will operate on the given batch in each optimization step. In the latter, only one optimizer will operate on the given batch at every step. This is different from the  frequency value specified in the lr_dict mentioned below.  code-block python def configure_optimizers(self): optimizer_one = torch.optim.SGD(self.model.parameters(), lr=0.01) optimizer_two = torch.optim.SGD(self.model.parameters(), lr=0.01) return [ {'optimizer': optimizer_one, 'frequency': 5}, {'optimizer': optimizer_two, 'frequency': 10}, ] In this example, the first optimizer will be used for the first 5 steps, the second optimizer for the next 10 steps and that cycle will continue. If an LR scheduler is specified for an optimizer using the  lr_scheduler key in the above dict, the scheduler will only be updated when its optimizer is being used. Examples  most cases def configure_optimizers(self): return Adam(self.parameters(), lr=1e-3)  multiple optimizer case (e.g.: GAN) def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) return gen_opt, dis_opt  example with learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) dis_sch = CosineAnnealing(dis_opt, T_max=10) return [gen_opt, dis_opt], [dis_sch]  example with step-based learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) gen_sch = {'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step'}  called after each training step dis_sch = CosineAnnealing(dis_opt, T_max=10)  called every epoch return [gen_opt, dis_opt], [gen_sch, dis_sch]  example with optimizer frequencies  see training procedure in  Improved Training of Wasserstein GANs , Algorithm 1  https: arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note: Some things to know: - Lightning calls  .backward() and  .step() on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision ( precision=16 ), Lightning will automatically handle the optimizers. - If you use multiple optimizers, :meth: training_step will have an additional  optimizer_idx parameter. - If you use :class: torch.optim.LBFGS , Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default  .step() schedule, override the :meth: optimizer_step hook.",
"func":1
},
{
"ref":"pycollector.recognition.PIP_250k.training_step",
"url":1,
"doc":"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): Integer displaying index of this batch optimizer_idx (int): When using multiple optimizers, this argument will also be present. hiddens(:class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Return: Any of. - :class: ~torch.Tensor - The loss tensor -  dict - A dictionary. Can include any keys, but must include the key  'loss' -  None - Training will skip to the next batch Note: Returning  None is currently not supported for multi-GPU or TPU, or with 16-bit precision enabled. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional  optimizer_idx parameter.  code-block python  Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx  0:  do training_step with encoder if optimizer_idx  1:  do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step.  code-block python  Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens):  hiddens are the hidden states from the previous truncated backprop step  . out, hiddens = self.lstm(data, hiddens)  . return {'loss': loss, 'hiddens': hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.",
"func":1
},
{
"ref":"pycollector.recognition.PIP_250k.validation_step",
"url":1,
"doc":"Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val dataloaders used) Return: Any of. - Any object or value -  None - Validation will skip to the next batch  code-block python  pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined('validation_step_end'): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs)  code-block python  if you have one val dataloader: def validation_step(self, batch, batch_idx)  if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples  CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch  implement your own out = self(x) loss = self.loss(out, y)  log 6 example images  or generated text . or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0)  calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y  labels_hat).item() / (len(y)  1.0)  log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument.  code-block python  CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx):  dataloader_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.",
"func":1
},
{
"ref":"pycollector.recognition.PIP_250k.validation_epoch_end",
"url":1,
"doc":"Called at the end of the validation epoch with the outputs of all validation steps.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: outputs: List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note: If you didn't define a :meth: validation_step , this won't be called. Examples: With a single dataloader:  code-block python def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs:  do something With multiple dataloaders,  outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each validation step for that dataloader.  code-block python def validation_epoch_end(self, outputs): for dataloader_output_result in outputs: dataloader_outs = dataloader_output_result.dataloader_i_outputs self.log('final_metric', final_value)",
"func":1
},
{
"ref":"pycollector.recognition.PIP_250k.from_checkpoint",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.PIP_250k.totensor",
"url":1,
"doc":"Return captured lambda function if v=None, else return tensor",
"func":1
},
{
"ref":"pycollector.recognition.PIP_370k",
"url":1,
"doc":"Activity recognition using people in public - 250k stabilized"
},
{
"ref":"pycollector.recognition.PIP_370k.dump_patches",
"url":1,
"doc":""
},
{
"ref":"pycollector.recognition.PIP_370k.training",
"url":1,
"doc":""
},
{
"ref":"pycollector.recognition.PIP_370k.forward",
"url":1,
"doc":"Same as :meth: torch.nn.Module.forward() . Args:  args: Whatever you decide to pass into the forward method.  kwargs: Keyword arguments are also possible. Return: Your model's output",
"func":1
},
{
"ref":"pycollector.recognition.PIP_370k.totensor",
"url":1,
"doc":"Return captured lambda function if v=None, else return tensor",
"func":1
},
{
"ref":"pycollector.recognition.PIP_370k.configure_optimizers",
"url":1,
"doc":"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. -  Single optimizer . -  List or Tuple of optimizers. -  Two lists - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple lr_dict). -  Dictionary , with an  \"optimizer\" key, and (optionally) a  \"lr_scheduler\" key whose value is a single LR scheduler or lr_dict. -  Tuple of dictionaries as described above, with an optional  \"frequency\" key. -  None - Fit will run without any optimizer. Note: The lr_dict is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below.  code-block python lr_dict = { 'scheduler': lr_scheduler,  The LR scheduler instance (required)  The unit of the scheduler's step size, could also be 'step' 'interval': 'epoch', 'frequency': 1,  The frequency of the scheduler 'monitor': 'val_loss',  Metric for  ReduceLROnPlateau to monitor 'strict': True,  Whether to crash the training if  monitor is not found 'name': None,  Custom name for  LearningRateMonitor to use } Only the  \"scheduler\" key is required, the rest will be set to the defaults above. Note: The  frequency value specified in a dict along with the  optimizer key is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: In the former case, all optimizers will operate on the given batch in each optimization step. In the latter, only one optimizer will operate on the given batch at every step. This is different from the  frequency value specified in the lr_dict mentioned below.  code-block python def configure_optimizers(self): optimizer_one = torch.optim.SGD(self.model.parameters(), lr=0.01) optimizer_two = torch.optim.SGD(self.model.parameters(), lr=0.01) return [ {'optimizer': optimizer_one, 'frequency': 5}, {'optimizer': optimizer_two, 'frequency': 10}, ] In this example, the first optimizer will be used for the first 5 steps, the second optimizer for the next 10 steps and that cycle will continue. If an LR scheduler is specified for an optimizer using the  lr_scheduler key in the above dict, the scheduler will only be updated when its optimizer is being used. Examples  most cases def configure_optimizers(self): return Adam(self.parameters(), lr=1e-3)  multiple optimizer case (e.g.: GAN) def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) return gen_opt, dis_opt  example with learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) dis_sch = CosineAnnealing(dis_opt, T_max=10) return [gen_opt, dis_opt], [dis_sch]  example with step-based learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) gen_sch = {'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step'}  called after each training step dis_sch = CosineAnnealing(dis_opt, T_max=10)  called every epoch return [gen_opt, dis_opt], [gen_sch, dis_sch]  example with optimizer frequencies  see training procedure in  Improved Training of Wasserstein GANs , Algorithm 1  https: arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note: Some things to know: - Lightning calls  .backward() and  .step() on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision ( precision=16 ), Lightning will automatically handle the optimizers. - If you use multiple optimizers, :meth: training_step will have an additional  optimizer_idx parameter. - If you use :class: torch.optim.LBFGS , Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default  .step() schedule, override the :meth: optimizer_step hook.",
"func":1
},
{
"ref":"pycollector.recognition.PIP_370k.training_step",
"url":1,
"doc":"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): Integer displaying index of this batch optimizer_idx (int): When using multiple optimizers, this argument will also be present. hiddens(:class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Return: Any of. - :class: ~torch.Tensor - The loss tensor -  dict - A dictionary. Can include any keys, but must include the key  'loss' -  None - Training will skip to the next batch Note: Returning  None is currently not supported for multi-GPU or TPU, or with 16-bit precision enabled. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional  optimizer_idx parameter.  code-block python  Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx  0:  do training_step with encoder if optimizer_idx  1:  do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step.  code-block python  Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens):  hiddens are the hidden states from the previous truncated backprop step  . out, hiddens = self.lstm(data, hiddens)  . return {'loss': loss, 'hiddens': hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.",
"func":1
},
{
"ref":"pycollector.recognition.PIP_370k.validation_step",
"url":1,
"doc":"Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val dataloaders used) Return: Any of. - Any object or value -  None - Validation will skip to the next batch  code-block python  pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined('validation_step_end'): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs)  code-block python  if you have one val dataloader: def validation_step(self, batch, batch_idx)  if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples  CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch  implement your own out = self(x) loss = self.loss(out, y)  log 6 example images  or generated text . or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0)  calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y  labels_hat).item() / (len(y)  1.0)  log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument.  code-block python  CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx):  dataloader_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.",
"func":1
},
{
"ref":"pycollector.recognition.PIP_370k.validation_epoch_end",
"url":1,
"doc":"Called at the end of the validation epoch with the outputs of all validation steps.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: outputs: List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note: If you didn't define a :meth: validation_step , this won't be called. Examples: With a single dataloader:  code-block python def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs:  do something With multiple dataloaders,  outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each validation step for that dataloader.  code-block python def validation_epoch_end(self, outputs): for dataloader_output_result in outputs: dataloader_outs = dataloader_output_result.dataloader_i_outputs self.log('final_metric', final_value)",
"func":1
},
{
"ref":"pycollector.recognition.ActivityTracker",
"url":1,
"doc":"Activity recognition using people in public - 250k stabilized"
},
{
"ref":"pycollector.recognition.ActivityTracker.dump_patches",
"url":1,
"doc":""
},
{
"ref":"pycollector.recognition.ActivityTracker.training",
"url":1,
"doc":""
},
{
"ref":"pycollector.recognition.ActivityTracker.temporal_stride",
"url":1,
"doc":"",
"func":1
},
{
"ref":"pycollector.recognition.ActivityTracker.forward",
"url":1,
"doc":"Overload forward for multi-gpu batch. Don't use torch DataParallel!",
"func":1
},
{
"ref":"pycollector.recognition.ActivityTracker.lrt",
"url":1,
"doc":"top-k with likelihood ratio test with background null hypothesis",
"func":1
},
{
"ref":"pycollector.recognition.ActivityTracker.totensor",
"url":1,
"doc":"Return captured lambda function if v=None, else return tensor",
"func":1
},
{
"ref":"pycollector.recognition.ActivityTracker.configure_optimizers",
"url":1,
"doc":"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. -  Single optimizer . -  List or Tuple of optimizers. -  Two lists - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple lr_dict). -  Dictionary , with an  \"optimizer\" key, and (optionally) a  \"lr_scheduler\" key whose value is a single LR scheduler or lr_dict. -  Tuple of dictionaries as described above, with an optional  \"frequency\" key. -  None - Fit will run without any optimizer. Note: The lr_dict is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below.  code-block python lr_dict = { 'scheduler': lr_scheduler,  The LR scheduler instance (required)  The unit of the scheduler's step size, could also be 'step' 'interval': 'epoch', 'frequency': 1,  The frequency of the scheduler 'monitor': 'val_loss',  Metric for  ReduceLROnPlateau to monitor 'strict': True,  Whether to crash the training if  monitor is not found 'name': None,  Custom name for  LearningRateMonitor to use } Only the  \"scheduler\" key is required, the rest will be set to the defaults above. Note: The  frequency value specified in a dict along with the  optimizer key is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: In the former case, all optimizers will operate on the given batch in each optimization step. In the latter, only one optimizer will operate on the given batch at every step. This is different from the  frequency value specified in the lr_dict mentioned below.  code-block python def configure_optimizers(self): optimizer_one = torch.optim.SGD(self.model.parameters(), lr=0.01) optimizer_two = torch.optim.SGD(self.model.parameters(), lr=0.01) return [ {'optimizer': optimizer_one, 'frequency': 5}, {'optimizer': optimizer_two, 'frequency': 10}, ] In this example, the first optimizer will be used for the first 5 steps, the second optimizer for the next 10 steps and that cycle will continue. If an LR scheduler is specified for an optimizer using the  lr_scheduler key in the above dict, the scheduler will only be updated when its optimizer is being used. Examples  most cases def configure_optimizers(self): return Adam(self.parameters(), lr=1e-3)  multiple optimizer case (e.g.: GAN) def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) return gen_opt, dis_opt  example with learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) dis_sch = CosineAnnealing(dis_opt, T_max=10) return [gen_opt, dis_opt], [dis_sch]  example with step-based learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) gen_sch = {'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step'}  called after each training step dis_sch = CosineAnnealing(dis_opt, T_max=10)  called every epoch return [gen_opt, dis_opt], [gen_sch, dis_sch]  example with optimizer frequencies  see training procedure in  Improved Training of Wasserstein GANs , Algorithm 1  https: arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_dis.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note: Some things to know: - Lightning calls  .backward() and  .step() on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision ( precision=16 ), Lightning will automatically handle the optimizers. - If you use multiple optimizers, :meth: training_step will have an additional  optimizer_idx parameter. - If you use :class: torch.optim.LBFGS , Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default  .step() schedule, override the :meth: optimizer_step hook.",
"func":1
},
{
"ref":"pycollector.recognition.ActivityTracker.training_step",
"url":1,
"doc":"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): Integer displaying index of this batch optimizer_idx (int): When using multiple optimizers, this argument will also be present. hiddens(:class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Return: Any of. - :class: ~torch.Tensor - The loss tensor -  dict - A dictionary. Can include any keys, but must include the key  'loss' -  None - Training will skip to the next batch Note: Returning  None is currently not supported for multi-GPU or TPU, or with 16-bit precision enabled. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional  optimizer_idx parameter.  code-block python  Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx  0:  do training_step with encoder if optimizer_idx  1:  do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step.  code-block python  Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens):  hiddens are the hidden states from the previous truncated backprop step  . out, hiddens = self.lstm(data, hiddens)  . return {'loss': loss, 'hiddens': hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.",
"func":1
},
{
"ref":"pycollector.recognition.ActivityTracker.validation_step",
"url":1,
"doc":"Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor ,  .) | [:class: ~torch.Tensor ,  .]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val dataloaders used) Return: Any of. - Any object or value -  None - Validation will skip to the next batch  code-block python  pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined('validation_step_end'): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs)  code-block python  if you have one val dataloader: def validation_step(self, batch, batch_idx)  if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples  CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch  implement your own out = self(x) loss = self.loss(out, y)  log 6 example images  or generated text . or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0)  calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y  labels_hat).item() / (len(y)  1.0)  log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument.  code-block python  CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx):  dataloader_idx tells you which dataset this is. Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.",
"func":1
},
{
"ref":"pycollector.recognition.ActivityTracker.validation_epoch_end",
"url":1,
"doc":"Called at the end of the validation epoch with the outputs of all validation steps.  code-block python  the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: outputs: List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: None Note: If you didn't define a :meth: validation_step , this won't be called. Examples: With a single dataloader:  code-block python def validation_epoch_end(self, val_step_outputs): for out in val_step_outputs:  do something With multiple dataloaders,  outputs will be a list of lists. The outer list contains one entry per dataloader, while the inner list contains the individual outputs of each validation step for that dataloader.  code-block python def validation_epoch_end(self, outputs): for dataloader_output_result in outputs: dataloader_outs = dataloader_output_result.dataloader_i_outputs self.log('final_metric', final_value)",
"func":1
},
{
"ref":"pycollector.user",
"url":2,
"doc":""
},
{
"ref":"pycollector.user.User",
"url":2,
"doc":"User class for collector's user management Args: username ([str]): username of pycollector. password ([str], optional): password for the user. Defaults to None."
},
{
"ref":"pycollector.user.User.refresh",
"url":2,
"doc":"",
"func":1
},
{
"ref":"pycollector.user.User.login",
"url":2,
"doc":"[summary] Args: username ([str]): username of pycollector. password ([str], optional): password for the user. Defaults to None.",
"func":1
},
{
"ref":"pycollector.user.User.get_ssm_param",
"url":2,
"doc":"[summary]",
"func":1
},
{
"ref":"pycollector.user.User.is_token_expired",
"url":2,
"doc":"[summary] Returns: [type]: [description]",
"func":1
},
{
"ref":"pycollector.user.User.token_expired_by",
"url":2,
"doc":"",
"func":1
},
{
"ref":"pycollector.user.User.is_authenticated",
"url":2,
"doc":"",
"func":1
},
{
"ref":"pycollector.user.User.add_user_to_group",
"url":2,
"doc":"Check if the current user is already in the pycollector user group, if not add the user to group Returns: [type]: [description]",
"func":1
},
{
"ref":"pycollector.user.User.username",
"url":2,
"doc":""
},
{
"ref":"pycollector.user.User.cognito_username",
"url":2,
"doc":""
},
{
"ref":"pycollector.user.User.lambda_client",
"url":2,
"doc":""
},
{
"ref":"pycollector.detection",
"url":3,
"doc":""
},
{
"ref":"pycollector.detection.TorchNet",
"url":3,
"doc":""
},
{
"ref":"pycollector.detection.TorchNet.gpu",
"url":3,
"doc":"",
"func":1
},
{
"ref":"pycollector.detection.TorchNet.cpu",
"url":3,
"doc":"",
"func":1
},
{
"ref":"pycollector.detection.TorchNet.iscpu",
"url":3,
"doc":"",
"func":1
},
{
"ref":"pycollector.detection.TorchNet.isgpu",
"url":3,
"doc":"",
"func":1
},
{
"ref":"pycollector.detection.TorchNet.batchsize",
"url":3,
"doc":"",
"func":1
},
{
"ref":"pycollector.detection.FaceDetector",
"url":3,
"doc":"Faster R-CNN based face detector"
},
{
"ref":"pycollector.detection.Yolov5",
"url":3,
"doc":"Yolov5 based object detector >>> d = pycollector.detection.Detector() >>> d(vipy.image.vehicles( .show()"
},
{
"ref":"pycollector.detection.Yolov5.classlist",
"url":3,
"doc":"",
"func":1
},
{
"ref":"pycollector.detection.Yolov3",
"url":3,
"doc":"Yolov3 based object detector >>> d = pycollector.detection.Detector() >>> d(vipy.image.vehicles( .show()"
},
{
"ref":"pycollector.detection.Yolov3.classlist",
"url":3,
"doc":"",
"func":1
},
{
"ref":"pycollector.detection.ObjectDetector",
"url":3,
"doc":"Default object detector"
},
{
"ref":"pycollector.detection.MultiscaleObjectDetector",
"url":3,
"doc":"Given a list of images, break each one into a set of overlapping tiles, and ObjectDetector() on each, then recombining detections"
},
{
"ref":"pycollector.detection.VideoDetector",
"url":3,
"doc":"Iterate ObjectDetector() over each frame of video, yielding the detected frame"
},
{
"ref":"pycollector.detection.MultiscaleVideoDetector",
"url":3,
"doc":"Given a list of images, break each one into a set of overlapping tiles, and ObjectDetector() on each, then recombining detections"
},
{
"ref":"pycollector.detection.VideoTracker",
"url":3,
"doc":"Default object detector"
},
{
"ref":"pycollector.detection.VideoTracker.track",
"url":3,
"doc":"Batch tracking",
"func":1
},
{
"ref":"pycollector.detection.MultiscaleVideoTracker",
"url":3,
"doc":"MultiscaleVideoTracker() class"
},
{
"ref":"pycollector.detection.MultiscaleVideoTracker.stream",
"url":3,
"doc":"",
"func":1
},
{
"ref":"pycollector.detection.MultiscaleVideoTracker.track",
"url":3,
"doc":"Batch tracking",
"func":1
},
{
"ref":"pycollector.detection.Proposal",
"url":3,
"doc":"Default object detector"
},
{
"ref":"pycollector.detection.VideoProposal",
"url":3,
"doc":"pycollector.detection.VideoProposal() class. Track-based object proposals in video."
},
{
"ref":"pycollector.detection.VideoProposal.allowable_objects",
"url":3,
"doc":"",
"func":1
},
{
"ref":"pycollector.detection.VideoProposal.isallowable",
"url":3,
"doc":"",
"func":1
},
{
"ref":"pycollector.detection.VideoProposalRefinement",
"url":3,
"doc":"pycollector.detection.VideoProposalRefinement() class. Track-based object proposal refinement of a weakly supervised loose object box from a human annotator."
},
{
"ref":"pycollector.detection.ActorAssociation",
"url":3,
"doc":"pycollector.detection.VideoAssociation() class Select the best object track of the target class associated with the primary actor class by gated spatial IOU and distance. Add the best object track to the scene and associate with all activities performed by the primary actor."
},
{
"ref":"pycollector.detection.ActorAssociation.isallowable",
"url":3,
"doc":"",
"func":1
},
{
"ref":"pycollector.detection.ActorAssociation.track",
"url":3,
"doc":"Batch tracking",
"func":1
},
{
"ref":"pycollector.backend",
"url":4,
"doc":""
},
{
"ref":"pycollector.backend.API",
"url":4,
"doc":"class for pycollector API Args: object ([type]): [description] Args: username ([type], optional): [description]. Defaults to None. password ([type], optional): [description]. Defaults to None."
},
{
"ref":"pycollector.backend.API.get_project",
"url":4,
"doc":"[summary] Args: -",
"func":1
},
{
"ref":"pycollector.backend.API.videos",
"url":4,
"doc":"[summary] Args: -",
"func":1
},
{
"ref":"pycollector.backend.API.lastvideo",
"url":4,
"doc":"",
"func":1
},
{
"ref":"pycollector.backend.API.new_collection",
"url":4,
"doc":"",
"func":1
},
{
"ref":"pycollector.backend.API.list_collections",
"url":4,
"doc":"",
"func":1
},
{
"ref":"pycollector.backend.API.delete_collection",
"url":4,
"doc":"",
"func":1
},
{
"ref":"pycollector.backend.API.login",
"url":2,
"doc":"[summary] Args: username ([str]): username of pycollector. password ([str], optional): password for the user. Defaults to None.",
"func":1
},
{
"ref":"pycollector.backend.API.get_ssm_param",
"url":2,
"doc":"[summary]",
"func":1
},
{
"ref":"pycollector.backend.API.is_token_expired",
"url":2,
"doc":"[summary] Returns: [type]: [description]",
"func":1
},
{
"ref":"pycollector.backend.API.add_user_to_group",
"url":2,
"doc":"Check if the current user is already in the pycollector user group, if not add the user to group Returns: [type]: [description]",
"func":1
},
{
"ref":"pycollector.version",
"url":5,
"doc":""
},
{
"ref":"pycollector.version.num",
"url":5,
"doc":"",
"func":1
},
{
"ref":"pycollector.version.at_least_version",
"url":5,
"doc":"Is versionstring='X.Y.Z' at least the current version?",
"func":1
},
{
"ref":"pycollector.version.is_at_least",
"url":5,
"doc":"Synonym for at_least_version",
"func":1
},
{
"ref":"pycollector.version.is_exactly",
"url":5,
"doc":"",
"func":1
},
{
"ref":"pycollector.util",
"url":6,
"doc":""
},
{
"ref":"pycollector.util.mergedict",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.lowerif",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.isday",
"url":6,
"doc":"Is the yyyymmdd formatted as 'YYYY-MM-DD' such as '2020-03-18'",
"func":1
},
{
"ref":"pycollector.util.isdate",
"url":6,
"doc":"Alias for isday",
"func":1
},
{
"ref":"pycollector.util.fromdate",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.ismonday",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.is_more_recent_than",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.yyyymmdd_to_date",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.is_email_address",
"url":6,
"doc":"Quick and dirty, will throw an error on blank characters if strict=True",
"func":1
},
{
"ref":"pycollector.util.allmondays",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.timestamp_YYYYMMDD_HHMMSS",
"url":6,
"doc":"Datetime stamp in eastern timezone with second resolution",
"func":1
},
{
"ref":"pycollector.util.istimestamp_YYYYMMDD_HHMMSS",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.fromtimestamp_YYYYMMDD_HHMMSS",
"url":6,
"doc":"Assumed eastern timezone",
"func":1
},
{
"ref":"pycollector.util.timestamp",
"url":6,
"doc":"Datetime stamp in eastern timezone with microsecond resolution",
"func":1
},
{
"ref":"pycollector.util.istimestamp",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.fromtimestamp",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.clockstamp",
"url":6,
"doc":"Datetime stamp in eastern timezone with second resolution",
"func":1
},
{
"ref":"pycollector.util.datestamp",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.today",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.fromclockstamp",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.allmondays_since",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.lastmonday",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.lastweek_monday",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.nextsunday",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.util.nextday",
"url":6,
"doc":"",
"func":1
},
{
"ref":"pycollector.globals",
"url":7,
"doc":""
},
{
"ref":"pycollector.globals.cachedir",
"url":7,
"doc":"Set the location to save videos and JSON files when downloaded. This will default to the system temp directory if not set. -This can be set by default by creating the environment variable VIPY_CACHE='/path/to/newdir'",
"func":1
},
{
"ref":"pycollector.globals.logging",
"url":7,
"doc":"Single entry point for enabling/disabling logging vs. printing All vipy functions overload \"from vipy.globals import print\" for simplified readability of code. This global function redirects print or warn to using the standard logging module. If format is provided, this will create a basicConfig handler, but this should be configured by the end-user.",
"func":1
},
{
"ref":"pycollector.globals.warn",
"url":7,
"doc":"",
"func":1
},
{
"ref":"pycollector.globals.print",
"url":7,
"doc":"Main entry point for all print statements in the pycollector package. All pycollector code calls this to print helpful messages. -All print() statements in pycollector are overloaded to call pycollector.globals.print() so that it can be redirected to logging as needed -Printing can be disabled by calling pycollector.globals.silent() -Printing can be redirected to standard python logging by calling pycollector.globals.logging(True)",
"func":1
},
{
"ref":"pycollector.globals.verbose",
"url":7,
"doc":"",
"func":1
},
{
"ref":"pycollector.globals.silent",
"url":7,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset",
"url":8,
"doc":""
},
{
"ref":"pycollector.dataset.disjoint_activities",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.asmeva",
"url":8,
"doc":"Convert a list of collector.dataset.Video() to MEVA annotation style",
"func":1
},
{
"ref":"pycollector.dataset.TorchDataset",
"url":8,
"doc":"Converter from a pycollector dataset to a torch dataset"
},
{
"ref":"pycollector.dataset.TorchTensordir",
"url":8,
"doc":"A torch dataset stored as a directory of .pkl.bz2 files each containing a list of [(tensor, str=json.dumps(label ,  .] tuples used for data augmented training. This is useful to use the default Dataset loaders in Torch.  note Use python random() and not numpy random"
},
{
"ref":"pycollector.dataset.TorchTensordir.filter",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset",
"url":8,
"doc":"pycollector.dataset.Dataset() class This class is designed to be used with vipy.batch.Batch() for massively parallel operations"
},
{
"ref":"pycollector.dataset.Dataset.id",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.list",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.tolist",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.flatten",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.istype",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.isvipy",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.is_vipy_video",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.is_vipy_scene",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.clone",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.archive",
"url":8,
"doc":"Create a archive file for this dataset. This will be archived as: /path/to/tarfile.{tar.gz|.tgz|.bz2} tarfilename tarfilename.{json|pkl} mediadir/ video.mp4 extras1.ext extras2.ext Inputs: - tarfile: /path/to/tarfilename.tar.gz - delprefix: the absolute file path contained in the media filenames to be removed. If a video has a delprefix='/a/b' then videos with path /a/b/c/d.mp4' -> 'c/d.mp4', and {JSON|PKL} will be saved with relative paths to mediadir - mediadir: the subdirectory name of the media to be contained in the archive. Usually \"videos\". - extrafiles: list of tuples [(abspath, filename_in_archive), .] Example: - Input files contain /path/to/oldvideos/category/video.mp4 - Output will contain relative paths videos/category/video.mp4 >>> d.archive('out.tar.gz', delprefix='/path/to/oldvideos', mediadir='videos')",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.save",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.classlist",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.classes",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.categories",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.num_classes",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.class_to_index",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.index_to_class",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.label_to_index",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.powerset",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.powerset_to_index",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.dedupe",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.countby",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.union",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.difference",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.has",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.replace",
"url":8,
"doc":"Replace elements in self with other with equality detemrined by the key lambda function",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.merge",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.augment",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.filter",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.valid",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.takefilter",
"url":8,
"doc":"Apply the lambda function f and return n elements in a list where the filter returns true Args: f: [lambda] If f(x) returns true, then keep n: [int >= 0] The number of elements to take Returns: [n=0] Returns empty list [n=1] Returns singleton element [n>1] Returns list of elements of at most n such that each element f(x) is True",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.to_jsondir",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.takelist",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.take",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.take_per_category",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.split",
"url":8,
"doc":"Split the dataset by category by fraction so that video IDs are never in the same set",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.tocsv",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.map",
"url":8,
"doc":"Distributed map. To perform this in parallel across four processes: >>> with vipy.globals.parallel(4): >>> self.map(lambda v:  .)",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.localmap",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.flatmap",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.count",
"url":8,
"doc":"Counts for each label. Args: f: [lambda] if provided, count the number of elements that return true. This is the same as len(self.filter(f without modifying the dataset. Returns: A dictionary of counts per category [if f is None] A length of elements that satisfy f(v) = True [if f is not None]",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.frequency",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.percentage",
"url":8,
"doc":"Fraction of dataset for each label",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.multilabel_inverse_frequency_weight",
"url":8,
"doc":"Return an inverse frequency weight for multilabel activities, where label counts are the fractional label likelihood within a clip",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.inverse_frequency_weight",
"url":8,
"doc":"Return inverse frequency weight for categories in dataset. Useful for unbalanced class weighting during training",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.collectors",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.os",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.device",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.duration_in_frames",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.duration_in_seconds",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.framerate",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.density",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.stats",
"url":8,
"doc":"Analyze the dataset to return helpful statistics and plots",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.to_torch",
"url":8,
"doc":"Return a torch dataset that will apply the lambda function f_video_to_tensor to each element in the dataset on demand",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.to_torch_tensordir",
"url":8,
"doc":"Return a TorchTensordir dataset that will load a pkl.bz2 file that contains one of n_augmentations (tensor, label) pairs. This is useful for fast loading of datasets that contain many videos.",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.annotate",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.tohtml",
"url":8,
"doc":"Generate a standalone HTML file containing quicklooks for each annotated activity in dataset, along with some helpful provenance information for where the annotation came from",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.video_montage",
"url":8,
"doc":"30x50 activity montage, each 64x64 elements. Args: outfile: [str] The name of the outfile for the video. Must have a valid video extension. gridrows: [int, None] The number of rows to include in the montage. If None, infer from other args gridcols: [int] The number of columns in the montage mindim: [int] The square size of each video in the montage bycategory: [bool] Make the video such that each row is a category category: [str, list] Make the video so that every element is of category. May be a list of more than one categories annotate: [bool] If true, include boxes and captions for objects and activities trackcrop: [bool] If true, center the video elements on the tracks with dilation factor 1.5 transpose: [bool] If true, organize categories columnwise, but still return a montage of size (gridrows, gridcols) max_duration: [float] If not None, then set a maximum duration in seconds for elements in the video. If None, then the max duration is the duration of the longest element. Returns: A clone of the dataset containing the selected videos for the montage, ordered rowwise in the montage  notes - If a category does not contain the required number of elements for bycategory, it is removed prior to visualization - Elements are looped if they exit prior to the end of the longest video (or max_duration)",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.boundingbox_refinement",
"url":8,
"doc":"Must be connected to dask scheduler such that each machine has GPU resources",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.stabilize",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.track",
"url":8,
"doc":"",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.zip",
"url":8,
"doc":"Zip two datasets. Equivalent to zip(self, other). >>> for (d1,d2) in D1.zip(D2, sortkey=lambda v: v.instanceid( : >>> pass >>> for (d1, d2) in zip(D1, D2): >>> pass Args: other: [ pycollector.dataset.Dataset ] sortkey: [lambda] sort both datasets using the provided sortkey lambda. Returns: Generator for the tuple sequence ( (self[0], other[0]), (self[1], other[1]),  . )",
"func":1
},
{
"ref":"pycollector.dataset.Dataset.sort",
"url":8,
"doc":"Sort the dataset in-place using the sortkey lambda function",
"func":1
},
{
"ref":"pycollector.label",
"url":9,
"doc":""
},
{
"ref":"pycollector.label.Label",
"url":9,
"doc":""
},
{
"ref":"pycollector.label.piplabel_to_mevalabel",
"url":9,
"doc":"",
"func":1
},
{
"ref":"pycollector.label.mevalabel_to_index",
"url":9,
"doc":"",
"func":1
},
{
"ref":"pycollector.label.piplabel_to_index",
"url":9,
"doc":"",
"func":1
},
{
"ref":"pycollector.label.pip_250k_powerset",
"url":9,
"doc":"",
"func":1
},
{
"ref":"pycollector.model",
"url":10,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch",
"url":11,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases",
"url":12,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d",
"url":13,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.Bottleneck3D",
"url":13,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.Bottleneck3D.dump_patches",
"url":13,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.Bottleneck3D.training",
"url":13,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.Bottleneck3D.expansion",
"url":13,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.Bottleneck3D.forward",
"url":13,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.ResNet3D",
"url":13,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.ResNet3D.dump_patches",
"url":13,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.ResNet3D.training",
"url":13,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.ResNet3D.forward",
"url":13,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.ResNet3D.load_2d",
"url":13,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.ResNet3D.replace_logits",
"url":13,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.ResNet503D",
"url":13,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet50_3d.ResNet503D.get",
"url":13,
"doc":"Call this function to get the model Returns a child of nn.Module",
"func":1
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet101_3d",
"url":14,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet101_3d.ResNet1013D",
"url":14,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.resnet101_3d.ResNet1013D.get",
"url":14,
"doc":"Call this function to get the model Returns a child of nn.Module",
"func":1
},
{
"ref":"pycollector.model.pyvideoresearch.bases.base",
"url":15,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.base.Base",
"url":15,
"doc":""
},
{
"ref":"pycollector.model.pyvideoresearch.bases.base.Base.get",
"url":15,
"doc":"Call this function to get the model Returns a child of nn.Module",
"func":1
},
{
"ref":"pycollector.model.face",
"url":16,
"doc":""
},
{
"ref":"pycollector.model.face.recognition",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.convert_resnet101v4_image",
"url":17,
"doc":"Convert an RGB byte image to a FloatTensor suitable for processing with the network. This function assumes the image has already been resized, cropped, jittered, etc.",
"func":1
},
{
"ref":"pycollector.model.face.recognition.unconvert_resnet101v4_image",
"url":17,
"doc":"Convert a FloatTensor to an RGB byte Image",
"func":1
},
{
"ref":"pycollector.model.face.recognition.conv3x3",
"url":17,
"doc":"3x3 convolution with padding",
"func":1
},
{
"ref":"pycollector.model.face.recognition.BasicBlock",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.face.recognition.BasicBlock.dump_patches",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.BasicBlock.training",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.BasicBlock.expansion",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.BasicBlock.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.face.recognition.Bottleneck",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.face.recognition.Bottleneck.dump_patches",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.Bottleneck.training",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.Bottleneck.expansion",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.Bottleneck.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.face.recognition.ConcatChannels",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.face.recognition.ConcatChannels.dump_patches",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.ConcatChannels.training",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.ConcatChannels.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.face.recognition.Multiply",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.face.recognition.Multiply.dump_patches",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.Multiply.training",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.Multiply.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.face.recognition.ResNet",
"url":17,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.face.recognition.ResNet.dump_patches",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.ResNet.training",
"url":17,
"doc":""
},
{
"ref":"pycollector.model.face.recognition.ResNet.forward",
"url":17,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.face.recognition.resnet101v6",
"url":17,
"doc":"Construct resnet-101v6 model",
"func":1
},
{
"ref":"pycollector.model.face.detection",
"url":18,
"doc":""
},
{
"ref":"pycollector.model.face.detection.log_info",
"url":18,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.face.detection.FaceRCNN",
"url":18,
"doc":"Wrapper for PyTorch RCNN detector"
},
{
"ref":"pycollector.model.face.detection.FaceRCNN.dets_to_scene",
"url":18,
"doc":"Convert detections returned from this object to a vipy.image.Scene object",
"func":1
},
{
"ref":"pycollector.model.face.detection.FaceRCNN.detect",
"url":18,
"doc":"Run detection on a numpy image, with specified padding and min size",
"func":1
},
{
"ref":"pycollector.model.face.detection.FaceRCNN.select_from_rotated",
"url":18,
"doc":"Given that we tried rotating the image, select the best rotation to use",
"func":1
},
{
"ref":"pycollector.model.face.detection.FaceRCNN.im_detect",
"url":18,
"doc":"Detect object classes in an image given object proposals. Arguments: net (pytorch): Fast R-CNN network to use im (ndarray): color image to test (in BGR order, as (H, W, C) boxes (ndarray): R x 4 array of object proposals or None (for RPN) Returns: scores (ndarray): R x K array of object class scores (K includes background as object category 0) boxes (ndarray): R x (4 K) array of predicted bounding boxes",
"func":1
},
{
"ref":"pycollector.model.face.detection.FaceRCNN.bbox_transform",
"url":18,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.face.detection.FaceRCNN.bbox_transform_inv",
"url":18,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.face.detection.FaceRCNN.clip_boxes",
"url":18,
"doc":"Clip boxes to image boundaries.",
"func":1
},
{
"ref":"pycollector.model.face.faster_rcnn",
"url":19,
"doc":""
},
{
"ref":"pycollector.model.face.faster_rcnn.RpnLayers",
"url":19,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.face.faster_rcnn.RpnLayers.dump_patches",
"url":19,
"doc":""
},
{
"ref":"pycollector.model.face.faster_rcnn.RpnLayers.training",
"url":19,
"doc":""
},
{
"ref":"pycollector.model.face.faster_rcnn.RpnLayers.forward",
"url":19,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.face.faster_rcnn.BottomLayers",
"url":19,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.face.faster_rcnn.BottomLayers.dump_patches",
"url":19,
"doc":""
},
{
"ref":"pycollector.model.face.faster_rcnn.BottomLayers.training",
"url":19,
"doc":""
},
{
"ref":"pycollector.model.face.faster_rcnn.BottomLayers.forward",
"url":19,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.face.faster_rcnn.TopLayers",
"url":19,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.face.faster_rcnn.TopLayers.dump_patches",
"url":19,
"doc":""
},
{
"ref":"pycollector.model.face.faster_rcnn.TopLayers.training",
"url":19,
"doc":""
},
{
"ref":"pycollector.model.face.faster_rcnn.TopLayers.forward",
"url":19,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.face.faster_rcnn.FasterRCNN",
"url":19,
"doc":"PyTorch-1.3 model conversion of ResNet-101_faster_rcnn_ohem_iter_20000.caffemodel, leveraging MMDNN conversion tools Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.face.faster_rcnn.FasterRCNN.dump_patches",
"url":19,
"doc":""
},
{
"ref":"pycollector.model.face.faster_rcnn.FasterRCNN.training",
"url":19,
"doc":""
},
{
"ref":"pycollector.model.face.faster_rcnn.FasterRCNN.forward",
"url":19,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN",
"url":19,
"doc":"PyTorch-1.3 model conversion of ResNet-101_faster_rcnn_ohem_iter_20000.caffemodel, leveraging MMDNN conversion tools Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.dump_patches",
"url":19,
"doc":""
},
{
"ref":"pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.training",
"url":19,
"doc":""
},
{
"ref":"pycollector.model.face.faster_rcnn.FasterRCNN_MMDNN.forward",
"url":19,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.face.faster_rcnn.conversion",
"url":19,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch",
"url":20,
"doc":""
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet",
"url":21,
"doc":""
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.get_inplanes",
"url":21,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.conv3x3x3",
"url":21,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.conv1x1x1",
"url":21,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.BasicBlock",
"url":21,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.BasicBlock.dump_patches",
"url":21,
"doc":""
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.BasicBlock.training",
"url":21,
"doc":""
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.BasicBlock.expansion",
"url":21,
"doc":""
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.BasicBlock.forward",
"url":21,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.Bottleneck",
"url":21,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.Bottleneck.dump_patches",
"url":21,
"doc":""
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.Bottleneck.training",
"url":21,
"doc":""
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.Bottleneck.expansion",
"url":21,
"doc":""
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.Bottleneck.forward",
"url":21,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.ResNet",
"url":21,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.ResNet.dump_patches",
"url":21,
"doc":""
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.ResNet.training",
"url":21,
"doc":""
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.ResNet.forward",
"url":21,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.ResNets_3D_PyTorch.resnet.generate_model",
"url":21,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5",
"url":22,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils",
"url":23,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.metrics",
"url":24,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.metrics.fitness",
"url":24,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.metrics.ap_per_class",
"url":24,
"doc":"Compute the average precision, given the recall and precision curves. Source: https: github.com/rafaelpadilla/Object-Detection-Metrics.  Arguments tp: True positives (nparray, nx1 or nx10). conf: Objectness value from 0-1 (nparray). pred_cls: Predicted object classes (nparray). target_cls: True object classes (nparray). plot: Plot precision-recall curve at mAP@0.5 save_dir: Plot save directory  Returns The average precision as computed in py-faster-rcnn.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.metrics.compute_ap",
"url":24,
"doc":"Compute the average precision, given the recall and precision curves. Source: https: github.com/rbgirshick/py-faster-rcnn.  Arguments recall: The recall curve (list). precision: The precision curve (list).  Returns The average precision as computed in py-faster-rcnn.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.metrics.ConfusionMatrix",
"url":24,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.metrics.ConfusionMatrix.process_batch",
"url":24,
"doc":"Return intersection-over-union (Jaccard index) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format. Arguments: detections (Array[N, 6]), x1, y1, x2, y2, conf, class labels (Array[M, 5]), class, x1, y1, x2, y2 Returns: None, updates confusion matrix accordingly",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.metrics.ConfusionMatrix.matrix",
"url":24,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.metrics.ConfusionMatrix.plot",
"url":24,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.metrics.ConfusionMatrix.print",
"url":24,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.metrics.plot_pr_curve",
"url":24,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.activations",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.Swish",
"url":25,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.utils.activations.Swish.dump_patches",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.Swish.training",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.Swish.forward",
"url":25,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.activations.Hardswish",
"url":25,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.utils.activations.Hardswish.dump_patches",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.Hardswish.training",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.Hardswish.forward",
"url":25,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.activations.MemoryEfficientSwish",
"url":25,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.utils.activations.MemoryEfficientSwish.dump_patches",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.MemoryEfficientSwish.training",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.MemoryEfficientSwish.F",
"url":25,
"doc":"Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use this class: https: pytorch.org/docs/stable/notes/extending.html extending-torch-autograd Every operation performed on :class: Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies ( input  >> class Exp(Function): >>> >>> @staticmethod >>> def forward(ctx, i): >>> result = i.exp() >>> ctx.save_for_backward(result) >>> return result >>> >>> @staticmethod >>> def backward(ctx, grad_output): >>> result, = ctx.saved_tensors >>> return grad_output  result >>> >>>  Use it by calling the apply method: >>> output = Exp.apply(input)"
},
{
"ref":"pycollector.model.yolov5.utils.activations.MemoryEfficientSwish.forward",
"url":25,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.activations.Mish",
"url":25,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.utils.activations.Mish.dump_patches",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.Mish.training",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.Mish.forward",
"url":25,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.activations.MemoryEfficientMish",
"url":25,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.utils.activations.MemoryEfficientMish.dump_patches",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.MemoryEfficientMish.training",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.MemoryEfficientMish.F",
"url":25,
"doc":"Records operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use this class: https: pytorch.org/docs/stable/notes/extending.html extending-torch-autograd Every operation performed on :class: Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies ( input  >> class Exp(Function): >>> >>> @staticmethod >>> def forward(ctx, i): >>> result = i.exp() >>> ctx.save_for_backward(result) >>> return result >>> >>> @staticmethod >>> def backward(ctx, grad_output): >>> result, = ctx.saved_tensors >>> return grad_output  result >>> >>>  Use it by calling the apply method: >>> output = Exp.apply(input)"
},
{
"ref":"pycollector.model.yolov5.utils.activations.MemoryEfficientMish.forward",
"url":25,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.activations.FReLU",
"url":25,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.utils.activations.FReLU.dump_patches",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.FReLU.training",
"url":25,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.activations.FReLU.forward",
"url":25,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.loss",
"url":26,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.loss.smooth_BCE",
"url":26,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.loss.BCEBlurWithLogitsLoss",
"url":26,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.utils.loss.BCEBlurWithLogitsLoss.dump_patches",
"url":26,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.loss.BCEBlurWithLogitsLoss.training",
"url":26,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.loss.BCEBlurWithLogitsLoss.forward",
"url":26,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.loss.FocalLoss",
"url":26,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.utils.loss.FocalLoss.dump_patches",
"url":26,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.loss.FocalLoss.training",
"url":26,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.loss.FocalLoss.forward",
"url":26,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.loss.QFocalLoss",
"url":26,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.utils.loss.QFocalLoss.dump_patches",
"url":26,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.loss.QFocalLoss.training",
"url":26,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.loss.QFocalLoss.forward",
"url":26,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.loss.compute_loss",
"url":26,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.loss.build_targets",
"url":26,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.google_utils",
"url":27,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.google_utils.gsutil_getsize",
"url":27,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.google_utils.attempt_download",
"url":27,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.google_utils.gdrive_download",
"url":27,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.google_utils.get_token",
"url":27,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.autoanchor",
"url":28,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.autoanchor.check_anchor_order",
"url":28,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.autoanchor.check_anchors",
"url":28,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.autoanchor.kmean_anchors",
"url":28,
"doc":"Creates kmeans-evolved anchors from training dataset Arguments: path: path to dataset  .yaml, or a loaded dataset n: number of anchors img_size: image size used for training thr: anchor-label wh ratio threshold hyperparameter hyp['anchor_t'] used for training, default=4.0 gen: generations to evolve anchors using genetic algorithm verbose: print all results Return: k: kmeans evolved anchors Usage: from utils.autoanchor import  ; _ = kmean_anchors()",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils",
"url":29,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.torch_distributed_zero_first",
"url":29,
"doc":"Decorator to make all processes in distributed training wait for each local_master to do something.",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.init_torch_seeds",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.select_device",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.time_synchronized",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.is_parallel",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.intersect_dicts",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.initialize_weights",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.find_modules",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.sparsity",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.prune",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.fuse_conv_and_bn",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.model_info",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.load_classifier",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.scale_img",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.copy_attr",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.ModelEMA",
"url":29,
"doc":"Model Exponential Moving Average from https: github.com/rwightman/pytorch-image-models Keep a moving average of everything in the model state_dict (parameters and buffers). This is intended to allow functionality like https: www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage A smoothed version of the weights is necessary for some training schemes to perform well. This class is sensitive where it is initialized in the sequence of model init, GPU assignment and distributed training wrappers."
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.ModelEMA.update",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.torch_utils.ModelEMA.update_attr",
"url":29,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general",
"url":30,
"doc":""
},
{
"ref":"pycollector.model.yolov5.utils.general.set_logging",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.init_seeds",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.get_latest_run",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.check_git_status",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.check_img_size",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.check_file",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.check_dataset",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.make_divisible",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.labels_to_class_weights",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.labels_to_image_weights",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.coco80_to_coco91_class",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.xyxy2xywh",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.xywh2xyxy",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.scale_coords",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.clip_coords",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.bbox_iou",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.box_iou",
"url":30,
"doc":"Return intersection-over-union (Jaccard index) of boxes. Both sets of boxes are expected to be in (x1, y1, x2, y2) format. Arguments: box1 (Tensor[N, 4]) box2 (Tensor[M, 4]) Returns: iou (Tensor[N, M]): the NxM matrix containing the pairwise IoU values for every element in boxes1 and boxes2",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.wh_iou",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.non_max_suppression",
"url":30,
"doc":"Performs Non-Maximum Suppression (NMS) on inference results Returns: detections with shape: nx6 (x1, y1, x2, y2, conf, cls)",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.strip_optimizer",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.print_mutation",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.apply_classifier",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.utils.general.increment_path",
"url":30,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models",
"url":31,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.CrossConv",
"url":32,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.experimental.CrossConv.dump_patches",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.CrossConv.training",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.CrossConv.forward",
"url":32,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.experimental.C3",
"url":32,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.experimental.C3.dump_patches",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.C3.training",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.C3.forward",
"url":32,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.experimental.Sum",
"url":32,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.experimental.Sum.dump_patches",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.Sum.training",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.Sum.forward",
"url":32,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.experimental.GhostConv",
"url":32,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.experimental.GhostConv.dump_patches",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.GhostConv.training",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.GhostConv.forward",
"url":32,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.experimental.GhostBottleneck",
"url":32,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.experimental.GhostBottleneck.dump_patches",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.GhostBottleneck.training",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.GhostBottleneck.forward",
"url":32,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.experimental.MixConv2d",
"url":32,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.experimental.MixConv2d.dump_patches",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.MixConv2d.training",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.MixConv2d.forward",
"url":32,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.experimental.Ensemble",
"url":32,
"doc":"Holds submodules in a list. :class: ~torch.nn.ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all :class: ~torch.nn.Module methods. Args: modules (iterable, optional): an iterable of modules to add Example class MyModule(nn.Module): def __init__(self): super(MyModule, self).__init__() self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)]) def forward(self, x):  ModuleList can act as an iterable, or be indexed using ints for i, l in enumerate(self.linears): x = self.linears[i  2](x) + l(x) return x Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.experimental.Ensemble.dump_patches",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.Ensemble.training",
"url":32,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.experimental.Ensemble.forward",
"url":32,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.experimental.attempt_load",
"url":32,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.export",
"url":33,
"doc":"Exports a YOLOv5  .pt model to ONNX and TorchScript formats Usage: $ export PYTHONPATH=\"$PWD\"  python models/export.py  weights ./weights/yolov5s.pt  img 640  batch 1"
},
{
"ref":"pycollector.model.yolov5.models.common",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.autopad",
"url":34,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.DWConv",
"url":34,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Conv",
"url":34,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.common.Conv.dump_patches",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Conv.training",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Conv.forward",
"url":34,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Conv.fuseforward",
"url":34,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Bottleneck",
"url":34,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.common.Bottleneck.dump_patches",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Bottleneck.training",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Bottleneck.forward",
"url":34,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.BottleneckCSP",
"url":34,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.common.BottleneckCSP.dump_patches",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.BottleneckCSP.training",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.BottleneckCSP.forward",
"url":34,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.SPP",
"url":34,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.common.SPP.dump_patches",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.SPP.training",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.SPP.forward",
"url":34,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Focus",
"url":34,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.common.Focus.dump_patches",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Focus.training",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Focus.forward",
"url":34,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Concat",
"url":34,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.common.Concat.dump_patches",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Concat.training",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Concat.forward",
"url":34,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.NMS",
"url":34,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.common.NMS.dump_patches",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.NMS.training",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.NMS.conf",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.NMS.iou",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.NMS.classes",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.NMS.forward",
"url":34,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Detections",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Detections.display",
"url":34,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Detections.print",
"url":34,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Detections.show",
"url":34,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Detections.save",
"url":34,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Detections.tolist",
"url":34,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Flatten",
"url":34,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.common.Flatten.dump_patches",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Flatten.training",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Flatten.forward",
"url":34,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.common.Classify",
"url":34,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.common.Classify.dump_patches",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Classify.training",
"url":34,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.common.Classify.forward",
"url":34,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.yolo",
"url":35,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.yolo.Detect",
"url":35,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.yolo.Detect.dump_patches",
"url":35,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.yolo.Detect.training",
"url":35,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.yolo.Detect.stride",
"url":35,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.yolo.Detect.export",
"url":35,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.yolo.Detect.forward",
"url":35,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.yolo.Model",
"url":35,
"doc":"Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x return F.relu(self.conv2(x Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth: to , etc. :ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov5.models.yolo.Model.dump_patches",
"url":35,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.yolo.Model.training",
"url":35,
"doc":""
},
{
"ref":"pycollector.model.yolov5.models.yolo.Model.forward",
"url":35,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.yolo.Model.forward_once",
"url":35,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.yolo.Model.fuse",
"url":35,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.yolo.Model.nms",
"url":35,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.yolo.Model.info",
"url":35,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov5.models.yolo.parse_model",
"url":35,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov3",
"url":36,
"doc":""
},
{
"ref":"pycollector.model.yolov3.utils",
"url":37,
"doc":""
},
{
"ref":"pycollector.model.yolov3.utils.utils",
"url":38,
"doc":""
},
{
"ref":"pycollector.model.yolov3.utils.utils.to_cpu",
"url":38,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.utils.load_classes",
"url":38,
"doc":"Loads class labels at 'path'",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.utils.weights_init_normal",
"url":38,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.utils.rescale_boxes",
"url":38,
"doc":"Rescales bounding boxes to the original shape",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.utils.xywh2xyxy",
"url":38,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.utils.ap_per_class",
"url":38,
"doc":"Compute the average precision, given the recall and precision curves. Source: https: github.com/rafaelpadilla/Object-Detection-Metrics.  Arguments tp: True positives (list). conf: Objectness value from 0-1 (list). pred_cls: Predicted object classes (list). target_cls: True object classes (list).  Returns The average precision as computed in py-faster-rcnn.",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.utils.compute_ap",
"url":38,
"doc":"Compute the average precision, given the recall and precision curves. Code originally from https: github.com/rbgirshick/py-faster-rcnn.  Arguments recall: The recall curve (list). precision: The precision curve (list).  Returns The average precision as computed in py-faster-rcnn.",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.utils.get_batch_statistics",
"url":38,
"doc":"Compute true positives, predicted scores and predicted labels per sample",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.utils.bbox_wh_iou",
"url":38,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.utils.bbox_iou",
"url":38,
"doc":"Returns the IoU of two bounding boxes",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.utils.non_max_suppression",
"url":38,
"doc":"Removes detections with lower object confidence score than 'conf_thres' and performs Non-Maximum Suppression to further filter detections. Returns detections with shape: (x1, y1, x2, y2, object_conf, class_score, class_pred)",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.utils.build_targets",
"url":38,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.parse_config",
"url":39,
"doc":""
},
{
"ref":"pycollector.model.yolov3.utils.parse_config.parse_model_config",
"url":39,
"doc":"Parses the yolo-v3 layer configuration file and returns module definitions",
"func":1
},
{
"ref":"pycollector.model.yolov3.utils.parse_config.parse_data_config",
"url":39,
"doc":"Parses the data configuration file",
"func":1
},
{
"ref":"pycollector.model.yolov3.network",
"url":40,
"doc":""
},
{
"ref":"pycollector.model.yolov3.network.create_modules",
"url":40,
"doc":"Constructs module list of layer blocks from module configuration in module_defs",
"func":1
},
{
"ref":"pycollector.model.yolov3.network.Upsample",
"url":40,
"doc":"nn.Upsample is deprecated Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov3.network.Upsample.dump_patches",
"url":40,
"doc":""
},
{
"ref":"pycollector.model.yolov3.network.Upsample.training",
"url":40,
"doc":""
},
{
"ref":"pycollector.model.yolov3.network.Upsample.forward",
"url":40,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov3.network.EmptyLayer",
"url":40,
"doc":"Placeholder for 'route' and 'shortcut' layers Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov3.network.EmptyLayer.dump_patches",
"url":40,
"doc":""
},
{
"ref":"pycollector.model.yolov3.network.EmptyLayer.training",
"url":40,
"doc":""
},
{
"ref":"pycollector.model.yolov3.network.EmptyLayer.forward",
"url":40,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov3.network.YOLOLayer",
"url":40,
"doc":"Detection layer Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov3.network.YOLOLayer.dump_patches",
"url":40,
"doc":""
},
{
"ref":"pycollector.model.yolov3.network.YOLOLayer.training",
"url":40,
"doc":""
},
{
"ref":"pycollector.model.yolov3.network.YOLOLayer.compute_grid_offsets",
"url":40,
"doc":"",
"func":1
},
{
"ref":"pycollector.model.yolov3.network.YOLOLayer.forward",
"url":40,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov3.network.Darknet",
"url":40,
"doc":"YOLOv3 object detection model Initializes internal Module state, shared by both nn.Module and ScriptModule."
},
{
"ref":"pycollector.model.yolov3.network.Darknet.dump_patches",
"url":40,
"doc":""
},
{
"ref":"pycollector.model.yolov3.network.Darknet.training",
"url":40,
"doc":""
},
{
"ref":"pycollector.model.yolov3.network.Darknet.forward",
"url":40,
"doc":"Defines the computation performed at every call. Should be overridden by all subclasses.  note Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.",
"func":1
},
{
"ref":"pycollector.model.yolov3.network.Darknet.load_darknet_weights",
"url":40,
"doc":"Parses and loads the weights stored in 'weights_path'",
"func":1
},
{
"ref":"pycollector.model.yolov3.network.Darknet.save_darknet_weights",
"url":40,
"doc":"@:param path - path of the new weights file @:param cutoff - save layers between 0 and cutoff (cutoff = -1 -> all are saved)",
"func":1
},
{
"ref":"pycollector.video",
"url":41,
"doc":""
},
{
"ref":"pycollector.video.Video",
"url":41,
"doc":"pycollector.video.Video class"
},
{
"ref":"pycollector.video.Video.cast",
"url":41,
"doc":"Cast a conformal vipy object to this class. This is useful for downcast and upcast conversion of video objects.",
"func":1
},
{
"ref":"pycollector.video.Video.from_json",
"url":41,
"doc":"Restore an object serialized with self.json() Usage: >>> vs = vipy.video.Scene.from_json(v.json( ",
"func":1
},
{
"ref":"pycollector.video.Video.json",
"url":41,
"doc":"Return JSON encoded string of this object. This may fail if attributes contain non-json encodeable object",
"func":1
},
{
"ref":"pycollector.video.Video.isedited",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.editedat",
"url":41,
"doc":"Android appends an '_ ' timestamp as milliseconds since epoch (POSIX timestamp), iOS will replace the first '_datetimestr' with a new datetimest",
"func":1
},
{
"ref":"pycollector.video.Video.edited",
"url":41,
"doc":"Return the datetime representation of the editedat() string",
"func":1
},
{
"ref":"pycollector.video.Video.variant",
"url":41,
"doc":"Category variant",
"func":1
},
{
"ref":"pycollector.video.Video.geolocation",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.fetch",
"url":41,
"doc":"Download JSON and MP4 if not already downloaded",
"func":1
},
{
"ref":"pycollector.video.Video.fetchvideo",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.fetchjson",
"url":41,
"doc":"Download JSON if not already downloaded",
"func":1
},
{
"ref":"pycollector.video.Video.is_json_loaded",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.hasjson",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.hasMP4",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.activity_categories",
"url":41,
"doc":"Return a set of unique activity categories in the video, not including object categories",
"func":1
},
{
"ref":"pycollector.video.Video.quicklooks",
"url":41,
"doc":"Return a vipy.image.Image object containing a montage quicklook for each of the activities in this video. Usage: >>> filenames = [im.saveas('/path/to/quicklook.jpg') for im in self.quicklooks()]",
"func":1
},
{
"ref":"pycollector.video.Video.trim",
"url":41,
"doc":"Temporally clip the video so that the video start is the beginning of the first activity, and the end of the video is the end of the last activity. Optionally add a temporal pad of padframes before and after the clip",
"func":1
},
{
"ref":"pycollector.video.Video.timestamp",
"url":41,
"doc":"Return collected_date from json as a datetime object, WARNING: older veresion of the app do not include timezone info in this string, so this datetime is not offset aware",
"func":1
},
{
"ref":"pycollector.video.Video.uploaded",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.metadata",
"url":41,
"doc":"Return a dictionary of metadata about this video. This is an alias for the 'attributes' dictionary.",
"func":1
},
{
"ref":"pycollector.video.Video.videoid",
"url":41,
"doc":"Return a unique video identifier for this video, as specified in the 'video_id' attribute, or by SHA1 hash of the  vipy.video.Video.filename and  vipy.video.Video.url . Args: newid: [str] If not None, then update the video_id as newid. Returns: The video ID if newid=None else self  note - If the video filename changes (e.g. from transformation), and video_id is not set in self.attributes, then the video ID will change. - If a video does not have a filename or URL or a video ID in the attributes, then this will return None - To preserve a video ID independent of transformations, set self.setattribute('video_id', ${MY_ID}), or pass in newid",
"func":1
},
{
"ref":"pycollector.video.Video.collectorid",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.subjectid",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.collectionid",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.collection_name",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.collection",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.duration",
"url":41,
"doc":"Video length in seconds",
"func":1
},
{
"ref":"pycollector.video.Video.quickshow",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.downcast",
"url":41,
"doc":"Convert from pycollector.video to vipy.video.Scene by downcasting class",
"func":1
},
{
"ref":"pycollector.video.Video.upcast",
"url":41,
"doc":"Convert from pycollector.video to pycollector.admin.video by upcasting class, available to admins only",
"func":1
},
{
"ref":"pycollector.video.Video.project",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.program",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.video.Video.object_detection",
"url":41,
"doc":"Run an object detector on a given frame of video. It is more efficient to construct an ObjectDetector() object once and reuse it.",
"func":1
},
{
"ref":"pycollector.video.Video.face_detection",
"url":41,
"doc":"Run face detection on a given frame of video. It is more efficient to construct a FaceDetector() object once and reuse it.",
"func":1
},
{
"ref":"pycollector.video.Video.faces",
"url":41,
"doc":"Alias for face_detection",
"func":1
},
{
"ref":"pycollector.video.last",
"url":41,
"doc":"",
"func":1
},
{
"ref":"pycollector.project",
"url":42,
"doc":""
},
{
"ref":"pycollector.project.Project",
"url":42,
"doc":"collector.project.Project class Projects() are sets of CollectionInstances() and Instances() in a program. Args: username ([str]): username of pycollector. password ([str], optional): password for the user. Defaults to None."
},
{
"ref":"pycollector.project.Project.videos",
"url":42,
"doc":"",
"func":1
},
{
"ref":"pycollector.project.Project.last",
"url":42,
"doc":"",
"func":1
},
{
"ref":"pycollector.project.Project.login",
"url":2,
"doc":"[summary] Args: username ([str]): username of pycollector. password ([str], optional): password for the user. Defaults to None.",
"func":1
},
{
"ref":"pycollector.project.Project.get_ssm_param",
"url":2,
"doc":"[summary]",
"func":1
},
{
"ref":"pycollector.project.Project.is_token_expired",
"url":2,
"doc":"[summary] Returns: [type]: [description]",
"func":1
},
{
"ref":"pycollector.project.Project.add_user_to_group",
"url":2,
"doc":"Check if the current user is already in the pycollector user group, if not add the user to group Returns: [type]: [description]",
"func":1
}
]